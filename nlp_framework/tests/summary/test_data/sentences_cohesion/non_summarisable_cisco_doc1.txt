

Table of Contents

	Table of Contents
	UCS Test Drive for Technical Decision Makers
	Course Introduction
	Overview
	Course Goal and Objectives
	Course Flow
	Additional References


	Lesson 1: Challenges in the Data Center Today
	Overview
	Changes Being Driven in the Data Center
	Evolution of Server Scalability
	Cisco UCS Business Benefits
	Summary


	Lesson 2: Cisco Unified Computing System B-Series Blade Server Hardware Components
	Overview
	Cisco UCS Overview
	Data Center Bridging
	Cisco UCS 5100 Series Blade Server Chassis
	Cisco UCS B-Series Components
	Cisco UCS B-Series Blade Servers
	Cisco UCS Extended Memory Architecture
	Cisco UCS C-Series
	Power Requirements
	Determining Supported Configurations
	Summary


	Lesson 3: Cisco UCS Management Framework
	Overview
	Cisco UCS Management Architecture
	Third Party Management and Orchestration Tools
	Cisco UCS Service Profiles
	Service Profile Templates
	Managing Cisco UCS Resources
	Summary


	Lesson 4: Cisco UCS Core Network Connectivity
	Overview
	Cisco UCS Core Network Connectivity
	Fabric Interconnect: End Host Mode
	EHM with L2 Disjointed Networks
	Cisco UCS SAN Connectivity
	Fabric Interconnect―SAN End Host Mode
	Summary


	Lesson 5: UCS Port Configuration
	Overview
	UCS 6100 and 6200 Port Configuration
	Pin Groups
	UCS Port Channels
	Summary


	Lesson 6: Creating Service Profiles
	Overview
	Simple versus Expert Service Profile Wizards
	Simple Service Profile Wizard
	Service Profile Expert Wizard
	Summary


	Lesson 7: Managing Service Profiles
	Overview
	Associating and Disassociating a Service Profile to a Server Blade
	Management IP Address, KVM, and Virtual Media
	Summary


	Lesson 8: Use Cases
	Overview
	UCS Application Solutions
	Converged Infrastructure
	Cisco IAC
	Summary


	Lesson 9: Intel Reliability, Performance, and Security Optimizations
	Overview
	Intel Processor Overview
	RAS Features
	Intel Performance Optimizations
	Intel Security Features
	Cisco UCS BIOS Best Practice Settings for Desktop Virtualization
	Summary













Course Introduction

Overview

The Cisco Unified Computing System family of products is a next-generation computing solution based on industry-standard technologies and innovative management concepts. Cisco UCS redefines the way data centers will use and deploy compute resources, while reducing management overhead and increasing efficiency. This 2-day course provides you with a detailed architectural overview of how Cisco UCS can be deployed to increase compute density, reduce the cabling, power and cooling burdens, and accelerate server provisioning in both virtualized and nonvirtualized environments. The course includes hands-on labs to demonstrate the deployment and management of compute resources in a Cisco UCS environment.



Learner Skills and Knowledge



	Understanding of server system design and architecture

	Familiarity with Ethernet networking

	Familiarity with Storage Area Networks

	Understanding of data center architecture







The figure lists the skills and knowledge that you should possess to benefit fully from the course. 



Course Introduction

Overview

The Cisco Unified Computing System family of products is a next-generation computing solution based on industry-standard technologies and innovative management concepts. Cisco UCS redefines the way data centers will use and deploy compute resources, while reducing management overhead and increasing efficiency. This 2-day course provides you with a detailed architectural overview of how Cisco UCS can be deployed to increase compute density, reduce the cabling, power and cooling burdens, and accelerate server provisioning in both virtualized and nonvirtualized environments. The course includes hands-on labs to demonstrate the deployment and management of compute resources in a Cisco UCS environment.



Learner Skills and Knowledge



	Understanding of server system design and architecture

	Familiarity with Ethernet networking

	Familiarity with Storage Area Networks

	Understanding of data center architecture







The figure lists the skills and knowledge that you should possess to benefit fully from the course. 



Course Goal and Objectives


This topic describes the course goal and objectives.



Course Goal
















“Understand the benefits and components of a Cisco Unified Computing System.”






Upon completing this course, you will be able to meet these objectives:

	Discuss challenges in the Data Center today

	Describe the Cisco UCS B-Series Blade Server hardware components

	Describe Cisco UCS Management framework

	Provide a detailed description of Cisco UCS core network connectivity

	Describe Cisco UCS port configuration

	Describe how to create Service Profiles

	Describe how to manage Service Profiles

	Explain use cases that demonstrate how Cisco UCS can benefit application deployments and provide business value

	Explain the value of the Intel chipset for providing highly-available, easily-maintained servers




Course Flow


This topic presents the suggested flow of the course materials.



Course Flow




















The schedule reflects the recommended structure for this course. This structure allows enough time for the instructor to present the course information and for you to work through the lab activities. The exact timing of the subject materials and labs depends on the pace of your specific class.



Additional References


This topic presents the Cisco icons and symbols that are used in this course, as well as information on where to find additional technical references.



Cisco Icons and Symbols




















Cisco Glossary of Terms

For additional information on Cisco terminology, refer to the Cisco Internetworking Terms and Acronyms glossary of terms at: http://www.cisco.com/univercd/cc/td/doc/cisintwk/ita/index.html.



Cisco Icons and Symbols (Cont.)




















These are the icons and symbols you will see throughout this course (continued).



Lesson 1
Challenges in the Data Center Today

Overview

This lesson discusses challenges in the Data Center today.


Objectives

Upon completing this lesson, you will be able to discuss challenges in the Data Center today. You will able to meet these objectives:
	Describe the transformations taking place in the Data Center and the emergence of cloud computing

	Describe the evolution of server scalability

	Explain how Cisco UCS provides business benefits in the form of improved management and rapid provisioning






Lesson 1
Challenges in the Data Center Today

Overview

This lesson discusses challenges in the Data Center today.


Objectives

Upon completing this lesson, you will be able to discuss challenges in the Data Center today. You will able to meet these objectives:
	Describe the transformations taking place in the Data Center and the emergence of cloud computing

	Describe the evolution of server scalability

	Explain how Cisco UCS provides business benefits in the form of improved management and rapid provisioning






Changes Being Driven in the Data Center


This topic describes the transformations taking place in the Data Center and the emergence of cloud computing. The benefits provided by the private cloud are discussed, and customer expectations for Cloud computing are explained. The challenges facing IT organizations to deliver on these expectations are also described.



IT is Undergoing a Transformation



	Current IT architecture models are burdened by procurement, management costs, and complexity

	IT is now moving toward a service-based consumption model: the Private Cloud

	This new model requires a new way of thinking:

			The underlying technology

	The way IT is delivered to maximize customer success

























The need for a new IT model has never been more clear, but navigating the path to that model has never been more complicated. The realities of out-dated technologies, rampant incremental approaches, and the absence of a compelling end-state architecture are impeding adoption by the customer.


IT is undergoing a transformation. The “accidental architecture” of IT today increases procurement and management costs, and adds unnecessary complexity, while making it difficult to meet customer service level agreements. These factors make IT less responsive to the business and create the perception of IT as a cost center.


IT is now moving towards a “private cloud” model, which is a new model for delivering IT as a service, whether that service is provided internally (like IT today), externally (in the service provider role), or in combination. This new model requires a new way of thinking about both the underlying technology and the way IT is delivered for customer success.


By harnessing the power of virtualization, private clouds place considerable business benefits within reach. These include:


	Business enablement: Increased business agility and responsiveness to changing priorities; speed of deployment and the ability to address the scale of global operations with business innovation

	Service-based business models: The ability to operate IT as a service

	Facilities optimization: Lower energy usage; better (less) use of datacenter real estate

	IT budget savings: Efficient use of resources through consolidation and simplification

	Reduced complexity: Moving away from fragmented, “accidental architectures” to integrated, optimized technology that lowers risk, increases speed, and produces predictable outcomes

	Flexibility: Ability of IT to gain responsiveness and scalability through federation to cloud service providers while maintaining enterprise-required policy and control



In the 1980s and 1990s, Moore’s Law (1965) was replaced by an unwritten rule that everyone knew but did not lament loudly enough: Enterprise IT doubles in complexity and TCO every five years, and IT gets more pinched by the pressure points. Enterprise IT solutions over the past 30 years have become more costly to analyze and design, procure, customize, integrate, inter-operate, scale, service and maintain. This is due to the inherent complexity of each of the lifecycle stages of the various solutions. Within the last decade, we have seen the rise of diverse inter-networks, variously called 'fabrics,' 'grids,' and, generically, the 'cloud.’ These inter-networks are constructed on commodity hardware, heavily yet selectively service-oriented with a scale of virtualized power never before contemplated, all housed in massive Data Centers on- and off-premises. It has only been in the past several years that the notion of cloud computing (infrastructure, software, or whatever-business-needs as an IT service) has been taken seriously in its own right, championed by pioneers who have proved the model’s viability, even if on a limited basis.


With enterprise-level credibility, enabled by the best players in the IT industry, the next wave of computing will be on terms that make business sense to the business savvy.



The Dilemma for IT Budgets Today
















Source: Forrester Research, Inc., IT Budget Allocations: Planning For 2011. December 3, 2010






Close to ¾ of the time spent by IT personnel in the data center is spent in maintenance tasks, while only a quarter of the time is spent in strategic engagements that help the business move forward. The end result is that businesses are unhappy with the support they are getting from IT when it comes to assisting them with new strategic initiatives.



Server Management: Increasing Complexity




















As computing capacity increases within the data center, so does complexity. Blade servers solve many issues but they also bring an additional point of management into the datacenter per chassis. Several independent systems must be managed, including LAN, SAN, servers, and storage. These separate resources must be managed at each network layer:
	Access

	Aggregation

	Core





Typically, these resources are managed by individual teams and can be monitored by using proprietary system-monitoring tools and alert aggregators. In many cases, customers must use multiple monitoring applications to cover all aspects of the data center.



Server I/O Cabling Proliferation




















Smaller 1U server form factors have resulted in serious cabling problems for data centers, with up to 336 uplink cables per rack creating a cabling nightmare. Proper in-rack cable management is crucial but not always implemented, resulting in higher TCO, higher failure rates, and airflow restrictions that drive up cooling costs and failure rates.


The move to blade servers has helped reduce the cabling clutter, but increasing I/O requirements mean that LAN and SAN uplinks from the rack to the aggregation layer are migrating to 10GE, which has a higher TCO than 1GE. Many data centers have Cat6 uplink cabling installed, which is fine for carrying 1GE traffic, but will result in an unacceptably high bit error rate (BER) when IT upgrades the switches and NICs to 10GE. Thus, an upgrade to 10GE would require either massive replacement of cabling or acceptance of degraded application performance levels.



Data Center Network Proliferation




















A large percent of all servers are also connected to a SAN over Fibre Channel. The use of high-performance computing is also growing, for example, with financial trading applications that demand very low latency. This leads to more complexity with regard to cabling, power, cooling, and management.


The increase in server quantity has resulted in a similar increase in network requirements. As application demand expands and new servers are implemented, the networks must grow to meet this demand. These networks include LAN, SAN, and high-performance computing (HPC). This growth results in increased power and cooling needs, increased cabling costs, and management challenges.


Networking needs vary greatly depending on the operating system and application but can reach as high as eight 1-Gigabit Ethernet ports and four 4-Gb Fibre Channel ports for a virtualization platform. This is up to 12 cables per physical server and can add up to 252 cables for a rack filled to capacity with 2U servers.



Proliferation of Server Virtualization




















After years of slow adoption, server virtualization has proliferated in the enterprise data center. The VM installed base was 2.9M in 2007, only about seven percent of the total DC server market; but this base is anticipated to be over sixty percent within 4 years.


The primary impact on server architecture is that virtualization enables optimized usage of the multi-core CPU architecture, which in turns drives higher memory and I/O requirements.


Virtualization also adds another layer of complexity to both data center architecture and operations. Visibility and control are significant issues in many environments. Server, network, and storage administrators are used, designing and managing their environments for fixed workloads; but workloads in a virtualized environment are mobile. Administrators need to be able to reassert some level of control over where and when applications are deployed in a virtualized infrastructure.


Virtualization has allowed companies to more easily consolidate servers in the data center. Instead of the standard one-to-one server model, many servers, each running in independent virtual machines (VMs), can run on a single physical server.


Virtualization is achieved by creating virtual representations of physical hardware components. There are a number of advantages to using virtualization, including better use of computing resources, greater server densities, and seamless server migrations:


	Virtual machine: A virtualized set of hardware that is able to operate in a similar fashion to a physical server

	Virtual server: A virtual set of hardware along with the operating system, applications, and files that are able to operate comparably to a physical server

	Hypervisor layer: A software layer that abstracts the physical hardware and creates individual virtual hardware for each VM, such as:

			VMware ESX

	Microsoft Hyper-V

	Xen









Workload Portability



Unique server identifiers tied to hardware:


	Unique identifiers can require configuration changes by the SAN and LAN teams after OS migration from a failed server

	Increases recovery times and application downtime




















Servers have unique identifiers that identify themselves on various networks. These identifiers are tied to the hardware. If you change any of these items, the server will potentially lose its ability to access network resources or even to boot an operating system. These are some identifiers:


	World Wide Name (WWN): Hard coded to a host bus adapter (HBA), this identifier is needed for SAN access.

	MAC address: Hard coded to a network interface card (NIC), this identifier is needed for LAN access.

	BIOS: This identifier contains settings that are specific to the server hardware.

	Firmware: This low-level software runs on peripheral devices and adapter cards to enable the operating system to interface with the device.



If a server fails and its operating system and application needs to be migrated to another physical server, the operating system, the application, and the network may require manual configuration changes. These manual configuration changes lead to longer recovery times and increased application downtime.



Evolution of Server Scalability


This topic describes the evolution of server scalability.



Server Evolution: Scalability



	
Monolithic servers:

	
Commodity servers:

	
Blade servers:


	
	Large number of CPUs

	Proprietary platforms and OS

	Multiple apps per server

	High cost

	Large failure domain


	
	Few CPUs

	X86 platform

	Commoditized operating system

	1 application per server

	Underutilized servers

	High power and cooling costs


	
	Multicore CPUs

	X86 platform

	Commoditized operating system

	Virtualization

	Management complexity

	Limited scale



	


















The figure describes the server scalability models from the 1990s through today. Monolithic compute resources typically were scaled by adding more compute power to a small number of servers. The result was higher total cost of ownership (TCO) and large failure domains.


During the early 2000s, the scalability model shifted away from fewer, larger servers to more numerous, smaller servers.


This transition was driven primarily by three factors:


	A shift from centralized IT control of assets to decentralized, departmental control

	The development of clustering technologies that allowed applications to scale more efficiently across multiple servers

	Increasing standardization on the Intel x86 platform to reduce cost of ownership



Since the early 2000s, IT has begun to move away from traditional tower or rack-mounted servers in favor of blade servers. Larger servers required more space and needed to have their own power supply, networking, and SAN cables. For smaller data centers, these infrastructure requirements are not problematic, but in a large data center, these requirements result in increasing complexity and cost of ownership.


By putting the computing resources of a server in a blade form factor within an enclosure, administrators gain these advantages:


	Maximum use of physical space

	Shared power distribution

	Shared networking and storage access

	More efficient power and cooling




Blade Deployments



Benefits:


	Reduction in redundant equipment costs

	Power and cooling savings

	Shared switching could reduce cabling

	Rapid hardware provisioning
















Challenges:


	Increased physical density may create power and cooling challenges

	Increased compute density could increase bandwidth and cabling requirements

	Each chassis and local switching creates additional management points




















The consolidation of discreet servers into blade chassis provides a number of advantages in the data center:


	Reduced physical footprint

	Shared networking and SAN switching

	Reduced cabling

	Rapid provisioning of additional resources



As the physical density of a blade environment increases relative to rack-mounted servers, there may be more power and cooling required in the same physical space. This can cause challenges in powering and cooling the data center.


The relatively high density of compute resources in a blade environment often requires significantly more bandwidth and cabling to fully use the deployment.


Each chassis in a blade environment typically will have a management IP address and require monitoring for system and blade health. Additionally, each local LAN or SAN switch (if installed) adds management overhead and overall network design challenges.



CPU and Memory Scalability
















With a single-core CPU, CPU is usually the bottleneck.















Without virtualization, multicores are often underutilized.















With multicore and virtualization, memory is often the bottleneck.






Today, server processors are multicore, offering multiple processing cores in the same space that was previously used for single-core processors.


In addition, many vendors have created memory controllers that can address large amounts of RAM into the tens of gigabytes. While blade server sizes have remained the same, the processing and memory has increased significantly, allowing blade servers to run processing and memory-intensive applications.



Cisco UCS Business Benefits


This topic describes how Cisco UCS provides business benefits in the form of improved management and rapid provisioning.



Data Center Management Challenges Require a Trade-Off between Quality, Time, and Resources




















The figure helps us to understand the practical need for data center transformation and the move to converged infrastructure and providing IT-as-a-Service. The growth in the amount of equipment and the heterogeneity of it has made the data center a very complex place to manage. From an operational perspective, it can easily take three to four months to deploy a new application because approval processes, contracts, delivery of equipment, installation and deployment all take a long time. Moreover, this deployment requires complex designs, multiple contracts and dedicated specialized personnel to manage the process. In addition, capital and operational budgets are under constant scrutiny; and the IT organization is asked to continue running the IT environment while supporting the growth initiatives of the business with limited resources. Given the heterogeneity of the environment and the siloed nature of the applications and data center structure, there is little operational efficiency. Also, there is a lack of visibility of performance across the data center. Benchmark metrics are often unknown. Running the IT infrastructure has become complex and time-consuming, requiring a long list of tasks that consume the lion’s share of the time of experts, both personnel and administrative staff, from roadmap planning, to change and release management, to assuring compliance and managing security.



Cisco Unified Computing System Ends the Need for Trade-Offs




















The figure lists the advantages of the Cisco Unified Computing System (UCS).



How Cisco UCS Features Reduce CapEx




















Cisco UCS reduces TCO through optimization of the computing platform for virtualization and unified I/O. TCO reduction targets include:


	Up to 10% better processor performance via Cisco Hypervisor Bypass Technology

	Up to 30% fewer components, switches, cabling, and management modules to purchase, manage, power, and cool

	Up to 30% lower memory and software licensing costs via Cisco Extended Memory Technology




How Cisco UCS Features Reduce OpEx




















OpEx savings result from the following:


	Lower facilities costs (power, cooling space)

	Increased operational efficiency (consolidation and profile-based management)

	Reduced downtime (primarily due profile-based management)




Cisco UCS Business Advantage: Easier IT Management





	Company

	Cost and Time Savings 


	MediaPro

	50% faster to deploy and provision compared to traditional servers


	Molina Healthcare

	33% reduction in time to deploy new applications


	Moses Cone

	96 hours saved on server configuration 


	NetApp

	10,000 virtual machines deployed in less than 1 hour


	Nighthawk Radiology

	15 to 20 minutes to provision servers


	Slumberland

	74% reduction in time to provision servers


	Tele Sistemi Ferroviari

	25% savings in new server provisioning costs


	Klinikum Wels-Grieskirchen

	80% reduction in management consoles (6:1) for network, applications, and servers


	NetApp

	99% reduction in management points (204 to 2)








Cisco UCS is a dynamically scalable, smart infrastructure. It is configured through unified, model-based management to simplify and speed the deployment of enterprise-class applications and services running in monolithic, virtualized, and cloud-computing environments. Using the latest Intel Xeon Processor E7 family, the system combines servers with networking and storage access into a single converged system that delivers greater cost efficiency and agility with increased performance, visibility, and control.


Cisco UCS enables IT organizations to be more effective by reducing the time spent on tactical, operational activities. Greater time-on-task efficiency leaves more time to focus on making the businesses successful and more competitive in the marketplace. Cisco UCS does this by bringing together both blade and rack-mount servers in a converged system that is self-aware and self-integrating. Cisco UCS automatically discovers, inventories, and configures components, making their power ready to be harnessed quickly and efficiently.


Greater Time-On-Task Efficiency

Automated configuration can change an IT organization’s approach from reactive to proactive. The result is more time for innovation, less time spent on maintenance, and faster response times. These efficiencies allow IT staff more time to address strategic business initiatives. They also enable better quality of life for IT staff, which means higher morale and better staff retention, both critical elements for long-term efficiency.


Cisco UCS Manager is an embedded, model-based management system that allows IT administrators to set a vast range of server configuration policies, from firmware and BIOS settings to network and storage connectivity. Individual servers can be deployed in less time and with fewer steps than in traditional environments. Automation frees staff from tedious, repetitive, time-consuming chores that are often the source of errors that cause downtime, making the entire data center more cost effective.



Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	Since the early 2000s, IT has begun to move away from traditional tower or rack-mounted servers in favor of blade servers.

	Today, server processors are multi-core, offering multiple processing cores in the same space that was previously used for single-core processors.

	Blade servers solve many issues, but they also bring an additional point of management per chassis into the data center.

	The separate resources must be managed at each network layer: access, aggregation, and core. Typically, these resources are managed by individual teams.

	Virtualization enables optimized usage of the multi-core architecture, which in turn, drives higher memory and I/O requirements but also adds another layer of complexity to both data center architecture and operations.

	Cisco UCS provides easy-to-realize proof points for the reduced OpEx, reduced CapEx, and improvements in IT management that allow for strategic gains in business agility and reduced costs.








Lesson 2
Cisco Unified Computing System B-Series Blade Server Hardware Components

Overview

The Cisco Unified Computing System (Cisco UCS) is a comprehensive compute resource system that integrates servers, networking, and I/O into a single intelligent platform. In this lesson, you will learn about the various individual components of the Cisco UCS 5108 Blade Server Chassis, along with the power supply options. The Cisco UCS 6120XP 20-Port Fabric Interconnect and Cisco UCS 6140XP 40-Port Fabric Interconnect are discussed, as well as the Cisco UCS 6248UP 48-Port Fabric Interconnect. Various blade models and configurations are discussed, as well as the various network connectivity options including the Cisco UCS Virtual Interface Card 1280 Converged Network Adapter.


Objectives

Upon completing this lesson, you will be able to describe the Cisco UCS B-Series Blade Server hardware components. You will able to meet these objectives:


	Describe the logical view of the Cisco UCS architecture from various perspectives

	Describe Data Center Bridging

	Describe the Cisco UCS 5100 Series Blade Server Chassis

	Describe the Cisco UCS B-Series fabric components

	Describe the Cisco UCS B-Series models and options

	Describe the Cisco UCS extended Memory architecture

	Describe the Cisco UCS C-Series models and options

	Describe the power requirements of the Cisco UCS B-Series

	Describe how to determine supported configurations




Lesson 2
Cisco Unified Computing System B-Series Blade Server Hardware Components

Overview

The Cisco Unified Computing System (Cisco UCS) is a comprehensive compute resource system that integrates servers, networking, and I/O into a single intelligent platform. In this lesson, you will learn about the various individual components of the Cisco UCS 5108 Blade Server Chassis, along with the power supply options. The Cisco UCS 6120XP 20-Port Fabric Interconnect and Cisco UCS 6140XP 40-Port Fabric Interconnect are discussed, as well as the Cisco UCS 6248UP 48-Port Fabric Interconnect. Various blade models and configurations are discussed, as well as the various network connectivity options including the Cisco UCS Virtual Interface Card 1280 Converged Network Adapter.


Objectives

Upon completing this lesson, you will be able to describe the Cisco UCS B-Series Blade Server hardware components. You will able to meet these objectives:


	Describe the logical view of the Cisco UCS architecture from various perspectives

	Describe Data Center Bridging

	Describe the Cisco UCS 5100 Series Blade Server Chassis

	Describe the Cisco UCS B-Series fabric components

	Describe the Cisco UCS B-Series models and options

	Describe the Cisco UCS extended Memory architecture

	Describe the Cisco UCS C-Series models and options

	Describe the power requirements of the Cisco UCS B-Series

	Describe how to determine supported configurations




Cisco UCS Overview


This topic describes the logical view of the Cisco UCS architecture from various perspectives.



UCS Building Blocks



	UCS Manager


Embedded: Manages entire system
	













	UCS Fabric Interconnect

6120/40: 20 & 40 Port 10Gb FCoE

6248/96: 48 & 96 Port 10Gb Unified Ports
	













	UCS IO Module

Remote line card
	













	UCS Blade Server Chassis

Flexible bay configurations
	













	UCS Blade or Rack Server

Industry-standard architecture
	













	UCS Virtual Adapters

Choice of multiple adapters
	



















Cisco UCS Blade Server:


	Industry-standard architecture



Cisco UCS Adapters:


	Choice of multiple Converged Network Adapters and Virtual Adapters



The Cisco Unified Computing System is based on a standard set of components that most IT staff are very familiar with. The intelligence for managing the overall system is based on a Pentium-class processor that Cisco has embedded in the fabric interconnect. The Cisco UCS Manager software that manages the entire system communicates with firmware embedded in every device in the system.


Note that there are three adapters. The first one is a standard 10GB Ethernet adapter and the second is a 10GB Fiber over Ethernet adapter. The most important one is the third one, which is referred to as Palo. That adapter supports the virtualization of the network connections, which will be discussed more in depth later in the presentation.



Cisco Data Center Topology with UCS



	Data Center Bridging/FCoE traffic is employed between Cisco UCS fabric interconnects and blade chassis.

	Traffic between the core and fabric interconnect is Ethernet or Fibre Channel.




















Traffic exiting the fabric interconnects are the following:


	Ethernet

	FCoE

	Fibre Channel



FCoE traffic is always used between the blade and the fabric interconnect for FC block-based I/O. FCoE is not yet supported end-to-end in Cisco UCS platforms.



Data Center Bridging


Unified fabric I/O consolidation requires intermediate devices to be aware of the different classes of traffic so that appropriate levels of service can be applied. Data Center Bridging (DCB) incorporates the Ethernet enhancements that are required to support unified fabric I/O consolidation.



SCSI Transport Evolution




















FCoE is an evolution of SCSI. SCSI can be transported over the SCSI bus, Fibre Channel, and FCoE:


	SCSI bus: The SCSI bus is an industry standard for carrying SCSI commands and data between computers and peripheral devices. It is a limited length local peripheral bus, with standardized peripheral controllers across platforms.

	Fibre Channel: SCSI is ported to a network model using Fibre Channel as the transport media. Fibre Channel preserves the investment in SCSI drivers, and provides the ability to share peripherals and support storage consolidation.

	FCoE: FCoE now ports SCSI and Fibre Channel to a network model using Ethernet as the transport media. This provides investment protection to existing Fibre Channel SANs and targets, and supports the same operational model as Fibre Channel. FCoE supports both LAN and SAN network consolidation and makes use of the economic model of Ethernet.




Data Center Bridging Ethernet Enhancements




	DCB is a collection of IEEE-based enhancements to Ethernet
















Ethernet enhancements:


	Priority groups: Virtualize links and allocate resources per traffic class

	Priority flow control (PFC) provides frame loss behavior based on class of service (COS) IEEE 802.1Qbb

	No drop COS provides lossless storage fabric

	Bandwidth Management: Enhances Transmission Selection

	Data Center Bridging Exchange (DCBX) provides peer communication and configuration exchange








The figure summarizes the enhancements to Ethernet that are required to achieve I/O consolidation. The traditional Ethernet specifications have some limitations when used within a data center for I/O consolidation. The main problem is that there is not much to ensure that frames are not lost. Enhanced Ethernet or Data Centre Bridging (DCB) is sometimes called lossless to refer to its error handling and flow control, although it is not technically truly lossless. DCB includes a number of standard IEEE-based enhancements to standard Ethernet, including Priority Groups, Priority Flow Control, and DCBX, which is a protocol that runs between switches and Converged Network Adapters to negotiate capabilities between them.



Fibre Channel over Ethernet (FCoE)



FCoE requires the following:


	10Gb Ethernet

	Jumbo Ethernet frames to encapsulate full FC frame

	Mapping FCID to Ethernet MAC address

	Lossless delivery of FC frames:

			IEEE 802.1Qbb (PFC)

	IEEE 802.3bd (PAUSE)

























In an I/O consolidation environment, separate protocols operate as tunnels on the same physical media. Fiber Channel traffic is tunneled in an Ethernet payload, while TCP/IP traffic exists separately inside Ethernet payloads. The implementation of FCoE uses Classes of Services with 802.1Q trunking to label and define a distinct traffic class and transport for the FCoE frames. In the Cisco UCS configuration, VLANs and COS 802.1p values are defined for the FCoE traffic. These values are distinct from the VLAN and COS values used by other TCP/IP Ethernet streams. This difference allows the traffic management tools in the Cisco UCS Mezzanine cards, IO Modules, and Fabric Interconnects, which are all using IO consolidation, to apply these features that ensure the different traffic streams are given the correct services.


FCoE encapsulates FC frames into Ethernet frames, and adds an FCoE header before the Ethernet header, using a specific Ethertype for FCoE. The receiver of the FCoE frames within the Ethernet frame knows to which service the frames belong.


Fibre Channel over Ethernet (FCoE) is a new protocol, based upon the Fibre Channel layers defined by the ANSI T11 committee, that replaces the lower layers of Fibre Channel with Unified I/O.


Minimum requirements for FCoE are the following:


	Jumbo Frames, so that an entire Fibre Channel frame (length 2180 Bytes) can be carried in the payload of a single Ethernet frame.

	The mapping of FC pWWN addresses to Ethernet MAC addresses.

	An FCoE Initialization Protocol (FIP) that provides login for FC devices across a Unified Fabric.

	Lossless delivery of Fibre Channel frames.

	A minimum 10 Gbps Ethernet platform.



FCoE traffic consists of a Fibre Channel frame encapsulated within an Ethernet frame. The Fibre Channel frame payload may in turn carry SCSI messages and data, or in the future, FICON for mainframe traffic.



Cisco UCS 5100 Series Blade Server Chassis


This topic describes the Cisco UCS 5100 Series Blade Server Chassis.



Cisco UCS 5100 Series Front View




















The power supplies for both the Cisco UCS 6100 and 6200 Series and the Cisco UCS 5108 chassis are accessible from the front. Both half-width and full-width blades are supported in the same chassis. The fan modules for the Cisco UCS 6100 and 6200 Series are accessible from the front, while on the Cisco UCS 5100 Series they are accessible from the rear. Note that the recommended configuration includes two fabric interconnects. For clarity, only one is shown in the figure.



Cisco UCS 5100 Series Rear View: Cabling




















All of the cabling for a Cisco UCS chassis is plugged into the rear. Two Cisco UCS fabric extenders, generally referred to as I/O modules (IOMs), are required for a fully redundant configuration. In a high-availability configuration, the Cisco fabric extender on the left (as seen from the rear) connects to the fabric interconnect on the left. The fabric extender on the right connects to the fabric interconnect on the right.


		
In the diagram are the Cisco UCS 6248XP fabric interconnects with the Cisco UCS 2108XP Fabric Extenders. The cabling shown is just an example; it is not required to connect the IO Modules to the same port-group on the fabric interconnects.





Cisco UCS Cabling Comparison




















This is a picture of the Cisco UCS platform in production. On average, Cisco UCS requires 40% fewer connections than a traditional server architecture, and 66 percent fewer components, while providing better airflow and requiring less energy consumption. Cisco UCS also reduces the management points in a Data Center POD build out, providing for reduced OpEx, simpler integration and automation, and reduced provisioning times.


The connections shown in red are unified IO connections from the 2104 Series IO modules that connect up to the Fabric Interconnects. They are 10G connections that unify 10GE and Fibre Channel on the same cable to reduce the number of connectors and cables within the chassis.



Cisco UCS B-Series Components


This topic describes the Cisco UCS B-Series components.



Cisco UCS 6200 Series



Cisco UCS 6248UP: 32 built-in unified ports (Ethernet or Fibre Channel):


	One expansion module: 16 unified ports or Layer 3 routing (for future use)

	Reduces the latency of previous-generation fabric interconnects by nearly half

	Double fabric capacity of previous generation in a single-rack unit

	12-port licenses preinstalled
















Cisco UCS 6296UP: 48 built-in unified ports (Ethernet or Fibre Channel):


	Three expansion slots: 16 unified ports or Layer 3 routing (for future use)

	18 port licenses preinstalled







The Cisco UCS 6200 Series is available in 48- or 96-port configurations, as follows:


	Cisco UCS 6248UP: The Cisco UCS 6248UP is a 1-RU, 10 Gigabit Ethernet, FCoE, and Fibre Channel switch offering up to 960-Gb/s throughput and up to 48 ports. The switch has 32 1- or 10-Gb/s fixed Ethernet, FCoE, and Fibre Channel ports and 1 expansion slot.

	Cisco UCS 6296UP 96-Port Fabric Interconnect: The Cisco UCS 6296UP is a 2-RU, 10 Gigabit Ethernet, FCoE, and native Fibre Channel switch offering up to 1920-Gb/s throughput and up to 96 ports. The switch has 48 1- or 10-Gb/s fixed Ethernet, FCoE, and Fibre Channel ports and 3 expansion slots.



Performance

	Cisco UCS 6248UP: Layer 2 hardware forwarding at 960 Gb/s or 714.24 mpps

	Cisco UCS 6296UP: Layer 2 hardware forwarding at 1.92 Tb/s or 1428.48 mpps

	MAC address table entries: 32,000

	Low-latency, cut-through design: Provides predictable, consistent traffic latency regardless of packet size, traffic pattern, or enabled features



Layer 2

	Layer 2 interconnect ports and VLAN trunks

	IEEE 802.1Q VLAN encapsulation

	Support for up to 1024 VLANs and virtual SANs (VSANs) per interconnect

	Per-VLAN Rapid Spanning Tree Plus (PVRST+)

	Internet Group Management Protocol (IGMP) versions 1, 2, and 3 snooping

	Cisco EtherChannel technology

	Link Aggregation Control Protocol (LACP): IEEE 802.3ad

	Advanced EtherChannel hashing based on Layer 2, 3, and 4 information

	Jumbo frames on all ports (up to 9216 B)

	Pause frames (IEEE 802.3x)



Layer 3

	Layer 3 ready (with future Layer 3 expansion module and daughter card)



Quality of Service (QoS)

	Layer 2 IEEE 802.1p (class of service [CoS])

	Eight hardware queues per port

	Per-port QoS configuration

	CoS trust

	Per-port virtual output queuing

	CoS-based egress queuing

	Egress strict-priority queuing

	Egress port-based scheduling: weighted round-robin (WRR)



High Availability

	Hot-swappable, field-replaceable power supplies, fan modules, and expansion modules

	1+1 power redundancy

	N+1 fan module redundancy



Management

	Interconnect management uses redundant 10-, 100-, or 1000-Mb/s management or console ports.

	All management is provided through Cisco UCS Manager. Please refer to the Cisco UCS Manager data sheet for more information about management interfaces.



Low-Latency, Lossless 10 Gigabit Ethernet Unified Network Fabric

	Priority flow control (PFC): per-priority pause frame support

	Data Center Bridging Exchange (DCBX) Protocol

	IEEE 802.1Qaz: bandwidth management

	Layer 2 multipathing (future)



Unified Ports

	All ports configurable as 1 or 10 Gigabit Ethernet or 1-, 2-, 4-, and 8-Gb/s Fibre Channel



Industry Standards

	IEEE 802.1p: CoS prioritization

	IEEE 802.1Q: VLAN tagging

	IEEE 802.1s: multiple VLAN instances of Spanning Tree Protocol (STP)

	IEEE 802.1w: rapid reconfiguration of STP

	IEEE 802.3: Ethernet

	IEEE 802.3ad: LACP

	IEEE 802.3ae: 10 Gigabit Ethernet

	SFP+ support

	Remote Monitoring (RMON)




Cisco UCS 6200 Series Expansion Module



	16 unified ports that can be configured as server or border ports

	Ports can be configured as 10 Gigabit Ethernet or 1, 2, 4, or 8 Gb/s Fibre Channel

	Accepts SFP or SFP+, with 8 ports pre-licensed

	Ethernet and Fibre Channel ports must be configured contiguously




















The Cisco UCS 6200 Series expansion module adds 16 more unified ports that can operate in one of the following:


	Ethernet mode (1 or 10 Gb/s)

	FCoE mode

	Fibre Channel mode (1, 2, 4, or 8 Gb/s)



In the future, a Layer 3 expansion module will also be available.



Fabric Interconnect Comparison 



	Product Features and Specifications
	Cisco UCS 6248UP
	Cisco UCS 6296UP

	Switch Fabric Throughput
	960 Gb/s
	1920 Gb/s

	Switch Footprint
	1 RU
	2 RU

	1-Gb/s Port Density 
	48
	96

	10-Gb/s Port Density
	48
	96

	1-, 2-, 4-, 8-Gb/s Fibre Channel Port Density
	48
	96

	Port-to-Port Latency
	2.0 microseconds
	2.0 microseconds

	Number of VLANs
	1024
	1024

	Layer 3 Ready
	Yes
	Yes

	40 Gigabit Ethernet Ready (future)
	Yes
	Yes

	Maximum Virtual Interface Support
	116
	116







The figure provides a configuration matrix comparing the features and capabilities of the Cisco UCS 6100 Series and the Cisco UCS 6200 Series.



Cisco UCS 2104XP IOM



	Four 10 Gigabit Ethernet, FCoE-capable, enhanced SFP+ ports (40-Gb/s bandwidth to the Cisco UCS 6100 Series)

	Eight 10 Gigabit Ethernet ports connected through the midplane




















Fabric Extender

The IOM logically extends the fabric from the fabric interconnect to the blade server, multiplexing and forwarding all traffic using a cut-through architecture over 10-Gb/s unified fabric connections. The IOM manages all switching for blade-to-blade and blade-to-fabric communications, simplifying diagnostics, cabling, and management. No local switching occurs in the IOM. There are four 10-Gb/s ports that can be used to connect the Cisco UCS 5108 chassis to the fabric interconnect switch. The Cisco UCS 5108 chassis contains two IOM slots for increased redundancy and bandwidth, though only one IOM is required for a minimal configuration.


If only one IOM is installed, it must be placed into the left bay as viewed from the rear of the Cisco UCS 5108 chassis. If both are used, they are hot-swappable and fully redundant from an Ethernet perspective. The IOM provides 10 Gb/s per blade server port (and there are two ports per blade server). Each blade server connects to both IOMs in the chassis via the mezzanine card.


The IOM supports increased scalability by multiplexing up to an eight-to-one oversubscribed blade servers-to-ports ratio. This ratio would be the case for a fully populated chassis with half-width blades, using a single 10-Gb/s uplink from the IOM to the fabric interconnect. There are eight internal ports.


CMC

The IOM has a Chassis Management Controller (CMC) that is a key part of the management infrastructure. The CMC collects status data from the IOM using the Intelligent Platform Management Interface (IPMI) protocol over the integrated circuit (I2C) serial bus. The CMC then communicates this information to the management node using the Ethernet server link. The CMC controls the power supply and fan speeds, and it serves as a proxy for the Cisco UCS Manager to the blade servers for certain functions. The CMC also plays a part in high-availability scenarios.



Cisco UCS 2204XP I/O Module



	Four 10 Gigabit Ethernet, FCoE-capable, SFP+ ports; 40Gbps bandwidth to Cisco UCS Fabric Interconnect

	Eight 16 Gigabit Ethernet ports connected through the midplane

	No local switching is performed on the Cisco UCS fabric interconnect




















The Cisco UCS 2204XP Fabric Extender has four 10 Gigabit Ethernet, FCoE-capable, SFP+ ports that connect the blade chassis to the fabric interconnect. Each Cisco UCS 2204XP has sixteen 10 Gigabit Ethernet ports connected through the midplane to each half-width slot in the chassis. Typically configured in pairs for redundancy, two fabric extenders provide up to 80 Gbps of I/O to the chassis.



Cisco UCS 2208XP



	Eight 10 Gigabit Ethernet, FCoE-capable, enhanced SFP+ ports (80-Gb/s bandwidth to the Cisco UCS 6100 Series)

	32 10 Gigabit Ethernet ports connected through the midplane

	Requires UCS VIC 1280 for full server bandwidth




















The Cisco UCS 2208XP Fabric Extender provides eight 10 Gigabit Ethernet ports for uplink to the fabric interconnects. It provides 32 internal ports to the Cisco UCS 5108, with four 10 Gigabit Ethernet ports per blade slot. The Cisco UCS 2208XP is compatible with both the Cisco UCS 6100 Series and the Cisco UCS 6200 Series. The Cisco UCS 2208XP requires the Cisco UCS Manager version 2.0 or later.



Comparing I/O Module Connectivity




















The amount of bandwidth available to each slot (full-width blades would potentially have access to double these bandwidth figures) depends on the IO Module installed. The first generation 2104XP IO Module provides each of the 8 slots in a single chassis with 20 Gbits per second, 10 for the A Fabric and 10 for the B Fabric. All mezzanine cards available can access this bandwidth.


The 2204XP IO Module provides 40 Gbits to each slot, 20 Gbits for the A side and 20 Gbits for the B side, through a 2x10 Gbit Port Channel on each side. The VIC 1280 or VIC 1240 is required to see all 20 Gbits per side. All other cards will see 10 Gbits per side.


The 2208XP IO Module provides 80 Gbits to each slot, 40 Gbits for the A side and 40 Gbits for the B side, through a 4x10 Gbit Port Channel on each side. The VIC 1280 or the VIC 1240 with port extender is required to see all 40 Gbits per side. A VIC 1240 without the port extender would see 20 Gbits per side, and all other cards would see 10 Gbit per side.


The amount of bandwidth available to each blade is limited by the uplinks on the chassis, and the PCIe bus (VIC 1280/1240 is limited to 64 Gbit, PCIe x16).



Cisco UCS B-Series Blade Servers


This topic describes the Cisco UCS B-Series models and options.



Cisco UCS Blade Servers




















Blade Servers come in two physical sizes: Full-width, which takes up two slots in a Cisco UCS 5108 chassis, and half-width, which occupies a single slot.


Half-Width Blade Servers

	Cisco UCS B22 M3 Blade Server:

			Half-width

	Two processor sockets (Xeon E5-2400)

	24 DIMM slots (384 GB max)

	One full sized mezzanine slot

	One LOM mezzanine slot (for Cisco UCS VIC 1240)

	1 x Mezzanine slot (Gen 3) for Cisco UCS VIC 1280 or third-party mezzanine

	Internal USB 2.0 port

	2 x Flexible Flash 16-GB SD Cards






	Cisco UCS B200 M2 Blade Server:

			Half-width

	Two processor sockets (Xeon 5600)

	12 DIMM slots (192 GB max)

	One mezzanine slot






	Cisco UCS B200 M3 Blade Server:

			Half-width

	Two processor sockets (Xeon E5-2600―two CPU configuration only)

	24 DIMM slots (384 GB max)

	One full sized mezzanine slot

	One LAN on motherboard (LOM) mezzanine slot (for Cisco UCS VIC 1240)

	Internal USB 2.0 port

	2 x Flexible Flash 16-GB SD Cards






	Cisco UCS B230 M2 Blade Server:

			Half-width

	Two processor sockets (Xeon E7-2800)

	32 DIMM slots (512 GB)

	One mezzanine slot








Full-Width Blade Servers

	Cisco UCS B250 M2 Extended Memory Blade Server:

			Full-width

	Two processor sockets

	48 DIMM slots (384 GB max)

	Two mezzanine slots

	Two internal drive bays






	Cisco UCS B420 M3 Blade Server:

			Full-width

	Four processor slots

	48 DIMM slots

	Three mezzanine slots (one dedicated connector for Cisco VIC 1240 modular LOM [mLOM])

	Four internal drive bays






	Cisco UCS B440 M2 High-Performance Blade Server:

			Full-width

	Two processor slots

	32 DIMM slots

	Two mezzanine slots

	Four internal drive bays








Only one CPU is required for normal system operation. If only one CPU is installed, it must go into the first socket. The CPUs must be identical on the same blade server, but can be mixed between blade servers in the same chassis. The Cisco Integrated Management Controller (Cisco IMC) is a microcontroller on the motherboard that provides lights-out hardware status and configurability to the system management software.


The following terms are used in the figure:


	Serial Attached Small Computer System Interface (SAS)

	Solid-state disk (SSD)

	Serial Advanced Technology Attachment (SATA)




Cisco UCS Half-Width Blades




















The Cisco UCS B22 M3 Blade Server is a half-width blade that harnesses the power of the latest Intel Xeon processor E5-2400 product family with expandability to 192 GB of RAM (using 16-GB DIMMs), two hot-pluggable drives, two PCI Express (PCIe) mezzanine slots, and up to eight 10-Gb/s throughput connections.


The Cisco UCS B22 M3 provides the following:


	Two multicore Intel Xeon processor E5-2400 product family CPU sockets, for up to 16 processing cores

	12 DIMM slots for industry-standard double-data-rate 3 (DDR3) memory running up to 1600 MHz and up to 192 GB of total memory (using 16-GB DIMMs)

	Two optional, hot-pluggable SAS or SATA hard disk drives (HDDs) or solid-state drives (SSDs)

	Built-in Cisco IMC GUI or CLI interface to monitor the server inventory, health, and system event logs



The Cisco UCS B200 M2 2-Socket Blade Server is a half-width blade that contains two small form-factor (SFF) drive bays, supporting single-attached storage disks or SSDs in a hardware Redundant Array of Independent Disks (RAID) configuration (RAID-0 or RAID-1).


This blade server has a 550W power and cooling budget. The chassis can hold up to eight half-width blade servers. The motherboard contains two CPU sockets and twelve double data rate 3 (DDR3) dual inline memory module (DIMM) memory sockets for a maximum of 192 GB of RAM. There is a single mezzanine adapter slot.


The Cisco UCS B200 M3 is a half-width blade with industry-leading performance and scalability. By leveraging the new Intel E5-2600 processors, the Cisco UCS B200 M3 can support up to 24 DIMM slots in this form factor making it both cost effective and efficient for running critical data center applications.


The figure summarizes some of the useful features that are provided by the Cisco UCS B200 M3, including the following:


	Two hot pluggable single attached drive bays, or SATA hard disk drive, or SSD

	Two flexible flash drives: 16 GB secure digital (SD) cards

	Up to 24 DDR3 DIMMS per server

	Support for modular LOM (Cisco UCS VIC 1240)

	Internal USB 2.0 port



		
UCS B200 M3 supports only a two-CPU configuration.




The Cisco UCS B230 M2 is a half-width blade containing two SFF drive bays that support SSDs in a hardware RAID-0 or RAID-1 configuration. The Cisco UCS 5108 chassis can hold up to eight half-width blade servers. The motherboard contains two CPU sockets and 32 DDR3 DIMM memory sockets. The maximum memory is 512 GB of RAM. There is a single mezzanine adapter slot.



Cisco UCS Full-Width Blades




















The Cisco UCS B250 M2 is a full-width blade with two SFF drive bays. The blade has a 1100-W power and cooling budget. The chassis can hold up to four full-width blades. The motherboard contains 2 CPU sockets and 48 DDR3 DIMM memory sockets, providing four times the memory capacity of the half-width blade.


There are two mezzanine adapter slots, doubling the I/O bandwidth of the Cisco UCS B200 M2.


The Cisco UCS B440 M2 is a full-width blade that provides two SFF drive bays. The blade has a 1100-W power and cooling budget. The Cisco UCS 5108 chassis can hold up to four full-width blades. The motherboard contains 2 CPU sockets and 48 DDR3 DIMM memory sockets, providing four times the memory capacity of the half-width blade. There are two mezzanine adapter slots, doubling the I/O bandwidth of the half-width blades.


Designed for enterprise performance and scalability, the Cisco UCS B420 M3 Blade Server combines the advantages of four-socket computing with the cost-effective Intel Xeon processor E5-4600 product family. The Cisco UCS B420 M3 is a full-width blade. Up to four of these high-density, four-socket blade servers can reside in the 6-RU Cisco UCS 5108.


The Cisco UCS B420 M3 provides the following:


	Up to 4 Intel Xeon processor E5-4600 processor family CPUs, with a maximum of 32 cores per server

	48 DIMM slots for registered error-correcting code DIMMs, with up to 1.5 GB of memory capacity (using 32-GB load-reduced DIMMs [LRDIMMs])

	Three mezzanine connectors enable up to 160-Gb/s bandwidth:

			One dedicated connector for Cisco VIC 1240 mLOM

	Two connectors for Cisco VIC 1280, VIC port expander, or third-party network adapter cards






	Four hot-pluggable drive bays supporting SAS, SATA, and SSD drives

	RAID 0, 1, 5, and 10, with optional 1-GB flash memory-backed write cache




Mezzanine I/O Adapters




















Mezzanine Cards

The types of adapters (or mezzanine cards) available in a Cisco UCS blade server include the following:


	Cisco UCS M81KR VIC or VIC 1280

	Cisco UCS CNA M72KR-E Emulex Converged Network Adapter

	Cisco UCS CNA M72KR-Q QLogic Converged Network Adapter

	Cisco UCS NIC M51KR-B Broadcom BCM57711 Network Adapter and Cisco UCS CNA M61KR-I Intel Converged Network Adapter (10 Gigabit Ethernet network adapters with Internet Small Computer Systems Interface [iSCSI] offload)

	Cisco UCS 82598KR-CI 10 Gigabit Ethernet Adapter (Intel)



Cisco UCS M81KR VIC

The Cisco UCS M81KR VIC supports hardware-based Ethernet vNICs or Fibre Channel virtual host bus adapters (vHBAs), running at 500K I/O operations per second (IOPS) in both initiator and target mode. This card supports hypervisor bypass. The Cisco UCS M81KR VIC is a CNA with dual 10-Gb/s ports and dual Fibre Channel ports to the backplane. The card provides failover between the redundant Ethernet links. No multipathing software is required on the host operating system for this functionality. As of the Cisco UCS Manager version 2.0, the Cisco UCS M81KR VIC can boot from iSCSI.


The number of virtual interfaces that are supported depends on the number of uplinks from the chassis IOM to the fabric interconnect.


Cisco UCS VIC 1280

The Cisco UCS VIC 1280 supports hardware-based Ethernet vNICs and Fibre Channel virtual vHBAs, running at 500K IOPS in both initiator and target mode. This card supports hypervisor bypass. The Cisco UCS VIC 1280 is a CNA with eight 10 Gigabit Ethernet ports to the backplane. The card provides failover between the redundant Ethernet links. No multipathing software is required on the host operating system for this functionality. As of the Cisco UCS Manager version 2.0, the Cisco UCS VIC 1280 can boot from iSCSI.


Cisco UCS VIC 1240

The Cisco UCS VIC 1240 supports hardware-based Ethernet vNICs and Fibre Channel vHBAs. The Cisco UCS VIC 1240 is a CNA with four 10 Gigabit Ethernet ports to the backplane with the Cisco UCS 2208 IOM, and two 10 Gigabit Ethernet ports with the Cisco UCS 2104 IOM. The Cisco UCS VIC 1240 is an LOM card and is only available with the Cisco UCS B200 M3.


Cisco UCS CNA M72KR-E and Cisco UCS CNA M72KR-Q

The Cisco UCS CNA M72KR-E and M72KR-Q are Generation 2 CNAs with two host-side 10 Gigabit Ethernet ports and two Fibre Channel ports to the backplane.


The Qlogic version of the CNA appears to the operating system as two 10-Gigabit native Ethernet NICs and two 8-Gb/s native Fibre Channel interfaces. The Emulex version of the CNA appears to the operating system as 10-Gb/s Fibre Channel.


Cisco UCS NIC M51KR-B and Cisco UCS CNA M61KR-I

The Cisco UCS NIC M51KR-B and CNA M61KR-I are network adapters that provide two 10 Gigabit Ethernet network interfaces with iSCSI offload. The Cisco UCS NIC M51KR-B provides boot from iSCSI capability. The Cisco UCS NIC M51KR-B is a chipset from Broadcom, while the Cisco UCS CNA M61KR-I is a chipset from Intel. As of the Cisco UCS Manager version 1.4, boot from iSCSI occurs in the adapter BIOS instead of a service profile.


Cisco UCS 82598KR-CI

The Cisco UCS 82598KR-CI is made by Intel. It has two 10 Gigabit Ethernet ports to the backplane, which run native Ethernet. The card does not provide failover functionality.


Comparison

The Cisco UCS VIC M81KR and VIC 1280 both provide enhanced virtualization. They support up to 128 vNICs presented to the VMware ESX server, which can then be used as physical NICs for VMs. Also, the personality of a VM can migrate (with VMotion) using the virtual network tag (VNTag) functionality of the Cisco UCS VIC M81KR and Cisco Nexus 5000 Series:


	The Cisco UCS CNA M72KR adapter provides increased scalability. The FCoE protocol encapsulation is offloaded from the host and performed in hardware on the Cisco UCS CNA M72KR.

	The Cisco UCS M51KR-B and M61KR-I provide standard 10 Gigabit Ethernet connectivity to go along with iSCSI offload.




Cisco UCS VIC 1280




















The figure describes the Cisco UCS VIC 1280.



Cisco UCS VIC 1240
















Excellent flexibility and performance:


	256 PCIe devices, vNICs or vHBAs

	40 Gb/s to half-width B200 M3 blade




















There is a Port Expander Card available for the Cisco UCS VIC 1240 that adds dual 2 x 10-Gb/s ports to the mLOM. This port expander essentially opens up four additional lanes, resulting in dual 4 x 10 Gb/s or 80 Gb/s of bandwidth from the server.



Cisco Fabric Extender (FEX) Architecture 



	Nexus 2000 FEX extends the fabric to TOR switches

	UCS 2100/2200 FEX extends the fabric to UCS Blade Chassis

	Adapter FEX extends the fabric into the server

	VM-FEX extends the fabric to the virtual machines

	RedHat KVM environments now supported in Cisco UCS 2.0




















The Cisco Fabric Extender (FEX) Architecture is a solution that provides a unified access-architecture across any computing environment, whether traditional or virtualized. It provides simplified operations and scalability with a single point of management and policy enforcement on the access switch across a large number of ports. The technology behind this solution is based on IEEE 802.1Qbh Bridge Port Extension.


Key components include the following:


	Rack FEX (Nexus 5000 + 2000): Nexus 2000 Series Fabric Extenders (FEX) extends the fabric to top-of-the-rack (TOR) switches.

	Chassis FEX (Cisco UCS 6248 + 2208): Cisco UCS 2100 Series Fabric Extenders extends the fabric to the Cisco UCS Blade Chassis.

	Adapter FEX (M81KR or 1280 VIC): Adapter Fabric Extender extends the fabric into the server

	VM-FEX (M81KR/1280 VIC plus hypervisor integration): Virtual Machine aware Fabric Extender extends the fabric to the virtual machines. VM-FEX allows using the Cisco UCS Virtual Interface Cards (M81KR or 1280 VICs) as remote switch line cards (FEXs) inside of a hypervisor and directly assigning an independent NIC to each and every Virtual Machine. This allows each VM to have its own logical switch port on the upstream Fabric Interconnect, complete with its own configuration, statistics, and so on.



Key Benefits

Simplified operations:


	Implement a single point of management from the access layer parent switches, Nexus 5000, and Nexus 7000 Series Switches

	Consolidate multiple 1 Gigabit Ethernet adapters with a 10 Gigabit Ethernet adapter, thereby reducing cabling, NICs, power consumption, and operating expense

	Partition the 10 Gigabit Ethernet server adapter or switch into multiple connections (vNICs or virtual line cards) to provide dedicated network bandwidth to individual applications or virtual machines



Architectural flexibility:


	Common, scalable, and adaptive architecture across data center racks and points of delivery (PoD) that supports various server options, connectivity options, physical topologies and evolving needs

	Choice of parent switches, adapter vendors; works across Cisco platforms and third-party architectures



Scalability:


Homogeneous (consistent) policies across large number of ports


	More than 1500 1-Gigabit Ethernet ports, more than 1000 10-Gigabit Ethernet physical ports, and even more virtual ports managed from a single point of management

	Scalable Gigabit and 10 Gigabit Ethernet server access with no reliance on Spanning Tree

	Low predictable latency at scale



Optimized for Server Virtualization

Cisco UCS has been optimized to implement VM-FEX technology. This technology provides improved support for server virtualization, including better policy-based configuration and security, conformance with a company's operational model, and accommodation for VMware's VMotion.


Dynamic vNIC Connection Policy

The dynamic vNIC connection policy determines how the connectivity between VMs and dynamic vNICs is configured:


	This policy is required for Cisco UCS instances that include servers with VIC adapters on which you have installed VMs and configured dynamic vNICs.

	Each dynamic vNIC connection policy includes an Ethernet adapter policy and designates the number of vNICs that can be configured for any server associated with a service profile that includes the policy.

	For VM-FEX that has all ports on a blade in standard mode, you need to use the VMware adapter policy.

	For VM-FEX that has at least one port on a blade in high-performance mode, use the VMwarePassThrough adapter policy or create a custom policy. If you need to create a custom policy, the resources provisioned need to equal the resource requirements of the guest OS that needs the most resources and for which you will be using high-performance mode. For KVM, use the system-provided Ethernet adapter policy named Linux.



		
If you migrate a server that is configured with dynamic vNICs using VMotion, the dynamic interface used by the vNICs fails and Cisco UCS Manager notifies you of that failure. When the server comes back up, Cisco UCS Manager assigns new dynamic vNICs to the server. If you are monitoring traffic on the dynamic vNIC, you must reconfigure the monitoring source.





Cisco UCS VIC Adapter Advantages 



Dual Port 10GE card appears as multiple virtual NICs:


	Independent NIC paths for Functions or Tenants:

			Management, vMotion, HA/FT, NAS, Apps

	Provides added protection in case of configuration error

	Added flexibility within virtual hypervisor switching






	Employed with VMDirectPath




















The Cisco UCS VIC M81KR supports multiple Ethernet virtual network interface cards (vNICs) or Fibre Channel Host Bus Adapters (vHBAs), running at 500 KIOPs in both initiator and target mode. This card supports kernel/hypervisor bypass. The Cisco UCS VIC M81KR is a converged network adapter (CNA) with dual 10-GE ports and dual Fibre Channel ports to the backplane. The card provides failover between the redundant Ethernet links. No multipathing software is required on the host operating system for this functionality.



Cisco UCS Extended Memory Architecture


This topic describes the Cisco UCS Extended Memory architecture.



Intel Xeon 5600 (Nehalem) Memory Scalability



	Intel Nehalem Xeon processors support three memory channels of 3 DIMMS each

	Traditional servers limited to nine DIMMS per populated CPU socket




















The Intel Xeon 5600, known as Nehalem, line of processors support three memory channels of three DIMMs each. This limitation is largely due to the electrical characteristics of the memory busses. A side effect of these characteristics is that as the number of DIMMs installed on a bus increase, the speed at which that bus runs may decrease. As such, many models of servers using the Nehalem processor use only two DIMMs per channel, for a total of six DIMMs per processor socket. In a dual-socket system, twelve DIMMs of 8-GB would limit the system to 96 GB, a commonly seen limit in today’s server platforms, and in the B200-M1 Cisco UCS blade as well. To increase the amount of supported memory, additional sockets and processors must be added.



Cisco UCS Extended Memory Technology



	
Cisco UCS with Memory Extension:

		
Classic:


	
	Up to 48 DIMMs

	Max 512GB of Memory

	Cost savings with smaller

	DIMMs


	












	
	18 DIMMs

	Max 384GB

	Higher cost due to fewer DIMM slots






















Cisco UCS introduces the Cisco Extended Memory Technology. With the latest generation of processors from Intel, the memory, and not the CPU, is even more likely than before to be the bottleneck for virtualized workloads, placing a new emphasis on the importance of cost-effective memory scalability. The Cisco UCS C250 M2 server supports 48 DIMM slots that can be configured to deliver up to 384 GB of main memory using 8-GB DIMMs, or up to 192 GB of main memory using lower-cost 4-GB DIMMs. The superior benchmark results demonstrate the importance of cost-effective memory footprints for virtualized environments and show customers how Cisco's patented technology can help increase consolidation ratios and further reduce capital and operating costs while also reducing the number of servers and licenses that need to be managed.


The Intel Nehalem architecture provides each CPU with its own high-speed memory controller, which can access up to three channels of DDR3 DIMMs. Each DIMM can support 2GB, 4GB, 8GB, and now 16GB, of RAM. Each channel can support up to three DIMM banks. The choice is up to the server vendor to provide either two or three banks per channel. When populating more banks, the memory must be run at slower speeds. If a vendor provides three DIMM banks per channel and all three DIMMs are populated, it will operate with a performance penalty for RAM access compared to a system with only one or two DIMMs per channel. The Cisco UCS B200 M1 is an example of a server where only two DIMMs per channel are supported.


Cisco has created a patented memory expansion technology in Cisco UCS that allows multiple DIMMs to be presented to the CPU and BIOS as a single DIMM, and still operate at a reasonable speed. This enables two potential benefits:


	First, this allows a single 2-socket server to physically address up to 384GB of RAM, which is the maximum amount of RAM a Nehalem-based system can use.

	Second, it allows customers to use a larger number of much less expensive, lower-capacity DIMMs to achieve the same RAM footprint as can be found in conventional servers for a fraction of the cost.



Providing this large a memory footprint to a dual-socket server allows customers with memory-bound application loads (such as large-scale VM deployments or large enterprise databases) to provide the applications with all of the memory it needs without resorting to purchasing larger, more expensive, and less advanced four-socket server models with their additional per-CPU software licensing costs.



Extended Memory Cost Savings




















In higher memory configurations, HP needs to use 32 GB DIMMs or LV 16 GB DIMMs.


The figure shows a price point comparison of HP on 09/06/11 and Cisco GPL/1.875. An apples-to-apples comparison of 3-year hardware-only support is given:


	HP DL380 G7, 384 GB configuration is derived from web 32 GB DIMM pricing. Actual 384 GB is not configurable on the HP website.

	256 GB configuration is also constructed with 32 GB DIMMs and is configurable on the website.

	HPDL380 G7, 288 GB configuration, is non-linear because in the on-line configurator, it is possible to make this 288 GB configuration with 2 x (9 x 16GB) LV DIMMs.

	192 GB configuration is also constructed with 16 GB LV DIMMs, which is the principal reason for the difference in list prices.

	All C250 configurations are done with less expensive 8 GB DIMMs.



These comparisons were done to emphasize how expensive a DL380 G7 can be when 32 GB DIMMs are used for any memory configurations because of the low DIMM slot count.



Cisco UCS B-Series Side-by-Side Comparison



	Model
	B200 M1 Blade
	B200 M2 Blade
	B230 M2 Blade
	B250 M1 Extended Memory Blade
	B250 M2 Extended Memory Blade
	B440 M1 High-Performance Blade

	Sockets
	2
	2
	2
	2
	2
	4

	CPUs
	Xeon 5500
	Xeon 5600
	E7-2800
	Xeon 5500
	Xeon 5600
	E7-4800

	Memory
	12 DIMMs 96 GB
	12 DIMMs 192 GB
	32 DIMMs 512 GB
	48 DIMMs; 384 GB
	48 DIMMs; 384 GB
	32 DIMMs; 512 GB

	Size / speed
	4GB, 8GB DDR3 1066MHz 1333MHz
	4GB, 8GB, 16GB DDR3 1066MHz 1333MHz
	4GB, 8GB DDR3 1066MHz
	4GB, 8GB DDR3 1066MHz
	4GB, 8GB DDR3 1066MHz, 1333MHz
	4GB, 8GB 1333MHz, 16GB DDR3 1066MHz

	RAID
	0,1
	0,1
	0,1
	0,1
	0,1
	0,1,5,6

	Disk
	2x 2.5" SFF SAS or 15mm SATA SSD
	2x 2.5" SFF SAS or 15mm SATA SSD
	2x 2.5" solid-state drives (SSD)
	2x 2.5" SFF SAS or 15mm SATA SSD
	2x 2.5" SFF SAS or 15mm SATA SSD
	4x 2.5" SFF SAS/SATA

	Adapter slots
	1
	1
	1
	2
	2
	2

	Form factor
	Half width
	Half width
	Half width
	Full width
	Full width
	Full width







The figure shows a side-by-side comparison of the Cisco UCS B-Series models.



Cisco UCS C-Series


This topic describes the Cisco UCS C-Series models and options.



Cisco UCS C-Series
















*2.5" = 6.35 cm. 3.5" = 8.89 cm.






The Cisco UCS C-Series extends unified computing innovations to an industry-standard form factor to help reduce TCO and increase business agility. Designed to operate both in standalone environments and as part of the Cisco UCS, the series employs Cisco technology to help customers manage the most challenging workloads. The series incorporates a standards-based unified network fabric, Cisco Data Center Virtual Machine Fabric Extender (VM-FEX) hardware switching virtualization support, and Cisco Extended Memory Technology. It supports an incremental deployment model and protects customer investments with a future migration path to unified computing.


The Cisco UCS C22 M3 Rack Server combines outstanding economics and a density-optimized feature set over a range of scale-out workloads, from IT and web infrastructure to distributed applications. Building on the success of the Cisco UCS C-Series servers, the C22 M3 server and the Cisco UCS P81E VIC further extend the capabilities of the Cisco UCS portfolio in a 1-RU form factor with the addition of the Intel Xeon processor E5-2400 product family. In addition, the Cisco UCS C22 M3 two-socket server offers 12 DIMM slots, up to 8 disk drives, two PCIe Generation 3.0 slots, and two 1 Gigabit Ethernet LOM ports, providing both an excellent price-to-performance ratio and a compact form factor.


The Cisco UCS C24 M3 Rack Server is designed for both outstanding economics and internal expandability over a range of storage-intensive infrastructure workloads, from IT and web infrastructure to big data. Building on the success of the Cisco UCS C-Series Rack Servers, the Cisco UCS C24 M3 server and the Cisco UCS P81E VIC extend the capabilities of the Cisco UCS portfolio in a 2-RU form factor with the Intel Xeon processor E5-2400 product family. In addition, the Cisco UCS C24 M3 offers up to 12 DIMM slots, 24 disk drives, five PCIe slots, and two 1 Gigabit Ethernet LOM ports to provide an exceptional internal storage capacity and price-to-performance ratio.


The Cisco UCS C200 M2 High-Density Rack-Mount Server is a high-density server with balanced compute performance and I/O flexibility. This two-socket, 1-RU rack-mount server is designed to optimize price-to-performance and balance simplicity, performance, and density for web infrastructure and mainstream data center, small-office, and remote-office applications. The 1-RU size makes it useful for service providers offering dedicated or multitenant hosting, and its economical price makes it well suited to the appliance market.


The Cisco UCS C210 M2 General-Purpose Rack-Mount Server is a general-purpose, two-socket, 2-RU rack-mount server housing up to 16 internal SFF SAS, SSDs, or SATA disk drives for a total of up to 16 terabytes (TB) of storage. The Cisco UCS C210 M2 server is designed to balance performance, density, and efficiency for workloads requiring economical, high-capacity, reliable, internal storage. Based on Intel Xeon 5600 series processors, the server is built for applications including virtualization, network file servers and appliances, storage servers, database servers, and content-delivery servers.


The Cisco UCS C220 and C240 M3 Rack Servers extend the capabilities of the Cisco UCS. They balance the features of Remote Access Service (RAS), simplicity, performance, and density for mainstream Java business applications. They also support virtualization, web serving, e-business, collaboration, distributed database, ERP deployments, and high-performance computing (HPC) workloads.


These new servers support up to two Intel Xeon E5-2600s to deliver new levels of dense, mission-critical performance while maintaining a low-cost entry point into unified computing. The differences between the two models include the following:


	Cisco UCS C220 M3: 1 RU, 16 DIMM slots, 4 or 8 drives

	Cisco UCS C240 M3: 2 RU, 12, 16, or 24 drives



The Cisco UCS C250 M2 Extended-Memory Rack-Mount Server is a two-socket, 2-RU rack-mount server featuring patented Cisco Extended Memory Technology. It is designed to increase performance and capacity for demanding virtualization and large-data-set workloads. It can also reduce the cost of smaller memory footprints. This server is built for virtualized workloads in enterprise data centers, service provider environments, and virtual desktop hosting. The system also helps increase performance for large dataset workloads, including database management systems and modeling and simulation applications. Applications that are memory bound benefit from the 384 GB of addressable memory that the Cisco UCS C250 M2 server offers.


The Cisco UCS C260 M2 Rack-Mount Server is a high-density, two-socket rack server that helps increase compute performance, memory capacity, internal storage or I/O capacity, while deriving maximum value from the available space in the data center. This expandability allows customers to decrease costly licensing fees by increasing performance without adding new CPU sockets. Applications that are memory bound benefit from the more than 1 TB of addressable memory that the Cisco UCS C260 M2 server offers. With 64 DIMM slots available, Cisco UCS C260 M2 server design is unique among two-socket servers that are based on the Intel Xeon processor E7-2800 product family. It can alleviate memory bottlenecks in situations where costly four-socket servers might otherwise be necessary, helping improve the price-to-performance ratio for running large-memory-footprint applications. From a memory-cost viewpoint, the server can be populated with low-cost 4- or 8-GB DIMMs for a total of up to 256 or 512 GB of main memory. This memory configuration delivers a memory footprint that other two-socket Intel Xeon processor E7-2800-based systems might require costly 32-GB DIMMS to achieve.


The Cisco UCS C460 M2 High-Performance Rack-Mount Server is a high-performance, high-memory capacity server that is designed with the performance and reliability to power compute-intensive, enterprise-critical standalone applications, and virtualized workloads. The system is a 4-RU rack-mount server supporting up to four Intel Xeon E7 series processors. It supports up to 1 TB of memory in 64 slots, and 12 small form factor (SFF) hot-pluggable SAS and SATA disk drives. Abundant I/O capability is provided by 10 PCIe slots supporting Cisco UCS C-Series network adapters, with an 11th PCIe slot reserved for SAS drive controller cards. Additional I/O is provided by four Ethernet LOM ports: Two 10 Gigabit Ethernet ports and two 1 Gigabit Ethernet ports.


Features of the Cisco UCS C-Series

Cisco UCS C22 M3:


	1 or 2 Intel Xeon Series e5-2400 processors

	Choice of processors: Intel Xeon E5-2470, E5-2450, E5-2440, E5-2420, E5-2403, or E5-2430L

	12 DIMM slots supporting up to 1333 or 1600 MHz of memory for optimal performance (192 GB max)

	Support for DDR3 low-voltage DIMMs

	Support for DDR3 registered DIMMs

	Up to 8 front-accessible, hot-swappable, internal 2.5-inch (6.35-cm) SAS and SATA drives

	Ease of access to front-panel video, 2 USB ports, and serial console

	24x CD-R and CD-RW, DVD±R and DVD±RW read and write optical drive

	RAID 0, 1, 5, 6, 10, 50, and 60 support



Cisco UCS C24 M3:


	1 or 2 Intel Xeon Series e5-2400 processors

	Choice of processors: Intel Xeon E5-2470, E5-2450, E5-2440, E5-2420, E5-2403, or E5-2430L

	12 DIMM slots supporting up to 1333 or 1600 MHz of memory for optimal performance (192 GB max)

	Support for DDR3 low-voltage DIMMs

	Support for DDR3 registered DIMMs

	Up to 24 front-accessible, hot-swappable, 2.5-inch (6.35-cm) SAS or SATA drives

	Ease of access to front-panel video, 2 USB ports, and serial console

	24x CD-R and CD-RW, DVD±R and DVD±RW read and write optical drive

	RAID 0, 1, 5, 6, 10, 50, and 60 support



Cisco UCS C200 M2:


	1 or 2 Intel Xeon Series 5600 processors

	Choice of processors: Intel Xeon X5670, X5660, X5675, L5650, E5649, E5645, E5620, E5506, or E5606

	12 DIMM slots for up to 192 GB of memory using 16-GB DIMMs

	Support for DDR3 low-voltage DIMMs

	Support for DDR3 registered DIMMs

	Up to 4 front-accessible, hot-swappable, 2.5- or 3.5-inch (6.35- or 8.89-cm) SSD, SAS, or SATA drives

	Ease of access to front-panel video, 2 USB ports, and serial console

	24x CD-R and CD-RW, DVD±R and DVD±RW read and write optical drive



Cisco UCS C210 M2:


	1 or 2 Intel Xeon 5600 series processors, choice of processors: Intel Xeon X5670, X5650, X5675, L5640, E5649, E5645, E5640, E5620, E5506, or E5606

	Up to 16 front-accessible, hot-swappable, SFF 6G SAS, SSD, or SATA drives for local storage, providing redundancy options and ease of serviceability

	RAID 0, 1, 5, 6, 10, 50, and 60 support

	12 DIMM slots supporting up to 192 GB of RAM with 16-GB DIMMs

	5 PCIe 2.0 slots



Cisco UCS C220 M3:


	2 Intel Xeon E5-2600 series processors

	Up to 16 front-accessible, hot-swappable, SAS, SSD, or SATA drives for local storage, providing redundancy options and ease of serviceability

	16 DIMM slots supporting up to 512 GB of RAM

	2 PCIe 3.0 slots



Cisco UCS C240 M3:


	2 Intel Xeon E5-2600 series processors

	Up to 24 front-accessible, hot-swappable, SAS, SSD, or SATA drives for local storage, providing redundancy options and ease of serviceability

	24 DIMM slots supporting up to 768 GB of RAM

	5 PCIe 3.0 slots



Cisco UCS C250 M2:


	1 or 2 Intel Xeon 5600 series processors, choice of processors: Intel Xeon X5690, X5680, X5675, X5670, X5650, E5649, E5640, or E5620

	Up to 8 front-accessible, hot-swappable, SFF 6-GB SAS, SSD, or SATA drives

	RAID 0, 1, 5, 6, 10, 50, and 60 support

	Up to 48 DIMM slots, up to 384 GB of main memory using 8-GB DIMMs

	5 PCIe 2.0 slots



Cisco UCS C260 M2:


	1 or 2 Intel Xeon E7-2800 series processors

	Up to 16 front-accessible, hot-swappable, SFF 6-GB SAS, SSD, or SATA drives

	RAID-0, 1, 5, 6, 10, 50, and 60 support

	Up to 64 DIMM slots, up to 1 TB of main memory using 16-GB DIMMs

	7 PCIe 2.0 slots



Cisco UCS C460 M2:


	2 or 5 Intel Xeon E7-4800 or E7-8800 series processors

	Up 1 TB of main memory (using 64 16-GB DIMMs)

	Up to 12 internal SFF SAS, SSD, or SATA drives for a total of up to 12 TB

	RAID 0, 1, and 10 support for up to 12 SAS or SATA drives with the optional Large Scale Integration (LSI) SAS9220-8i PCIe RAID controller

	RAID 0, 1, 5, 6, 10, 50, and 60 and up to 12 SAS or SATA drives with the optional LSI MegaRAID controller

	Support for up to 10 PCIe cards

	Dual-port 1 Gigabit Ethernet LOM, dual-port 10 Gigabit Ethernet LOM and two dedicated out-of-band (OOB) management ports




Cisco UCS C220 M3 Rack-Mount Server
















	RU form factor
	1

	CPU
	E5-2600

	Cores
	16

	DIMMs
	16

	Max GB
	512GB

	Disk*
	8 x 2.5" or 4 x 3.5"

	LoM
	2 x 1Gb

	PCIe Slots
	2 x PCIe 3.0

	Internal Storage
	USB Port FlexFlash



*2.5" = 6.35 cm. 3.5” = 8.89 cm. 






The Cisco UCS C220 M3 Rack Server is a high-density, two-socket, one–rack unit rack-mount server designed for performance and density over a wide range of business workloads. The enterprise-class C220 Server is an exceptional building block and entry point for UCS.


Each C220 contains two processors and six cores per processor, for a total of twelve cores.


Features of the Cisco UCS C220 M3:


	1 RU

	2x Intel Xeon E5-2600 series processors

	Up to 8 front-accessible, hot-swappable, SAS, SSD, or SATA drives for local storage, providing redundancy options and ease of serviceability

	16x DIMM slots supporting up to 512 GB of RAM

	2x PCIe 3.0 slots




Cisco UCS Virtual Interface Card 1225



Dual-port SFP+ 10 GigE/FCoE PCIe card for Cisco UCS C-Series Rack Servers:


	Support for lossless Ethernet

	PCIe Gen2 x16 provides 20 Gbps bandwidth to servers

	Centrally managed by Cisco UCS Manager

































Cisco UCS Virtual Interface Card (VIC) 1225 Product Overview

A Cisco® innovation, the Cisco UCS Virtual Interface Card (VIC) 1225 (Figure 1) is a dual-port Enhanced Small Form-Factor Pluggable (SFP+) 10 Gigabit Ethernet and Fibre Channel over Ethernet (FCoE)-capable PCI Express (PCIe) card designed exclusively for Cisco UCS C-Series Rack Servers. With its half-height design, the card preserves full-height slots in servers for third-party adapters certified by Cisco. It incorporates next-generation converged network adapter (CNA) technology from Cisco, providing investment protection for future feature releases. The card enables a policy-based, stateless, agile server infrastructure that can present up to 256 PCIe standards-compliant interfaces to the host that can be dynamically configured as either network interface cards (NICs) or host bus adapters (HBAs).


The Cisco UCS VIC 1225 implements the Cisco Virtual Machine Fabric Extender (VM-FEX), which unifies virtual and physical networking into a single infrastructure. It provides virtual-machine visibility from the physical network and a consistent network operations model for physical and virtual servers.


In virtualized environments, this highly configurable and self-virtualized adapter provides integrated, modular LAN interfaces on Cisco UCS C-Series Rack Servers. Additional features and capabilities include:


	Higher performance: Supports up to 256 PCIe virtual devices, either virtual network interface cards (vNICs) or virtual host bus adapters (vHBAs), with high I/O operations per second (IOPS), support for lossless Ethernet, and 20 Gbps to servers

	Better bandwidth: PCIe Generation 2 x16 helps assure optimal bandwidth to the host for network-intensive applications with a redundant path to the fabric interconnect

	Rack-optimized form factor: Half-height design reserves full-height slots in servers for Cisco certified third-party adapters

	Support for Cisco VM-FEX technology: Provides virtual machine visibility from the physical network and a consistent network operations model for physical and virtual servers

	Centralized management: Centrally managed by Cisco UCS Manager with support for Microsoft Windows, Red Hat Enterprise Linux, SUSE Linux, VMware vSphere, and Citrix XenServe




Power Requirements


This topic describes the power requirements of the Cisco UCS B-Series.



Cisco UCS 5108 Chassis Power Supply



	Redundancy modes:

			Nonredundant mode

	N+1 redundancy

	Grid redundancy






	Input power lead failure: Power loss on associated power supply

	All active power supplies available for all components

	AC and DC options




















The Cisco UCS 5108 is powered by 208-V (220-V outside the US) 50- or 60-Hz AC. Up to four power supply units (PSUs) can be installed, each with a single C20 connector for input power connection to the Cisco UCS 5108 power supply. The total number of power supplies that are required for operation depends on the total system power consumption and the level of redundancy that is required by the end user.


Cisco UCS Redundancy Modes

Cisco UCS enables end users to select the power mode that fits their redundancy needs through the Cisco Unified Computing System Manager (Cisco UCS Manager). Within the Cisco UCS Manager, the power redundancy mode is selected via the global policy tab. Because this power policy option is global, it is inherited by all the chassis that are managed by the Cisco UCS Manager instance. Nonredundant means that uptime cannot be guaranteed in the event of a failure. N+1 means that the system can tolerate the failure of one supply. Grid redundancy (N+N) means that the system, if wired correctly into dual independent AC feeds, can tolerate the loss of one of those grids or half of the power supplies.


The power inlet ports on the Cisco UCS 5108 chassis do not exist in a matrix. In other words, there are no crossover connections between the inlet ports and the chassis power supplies. If any component in the path to the power supply fails, that power supply will not receive power. The power supply outputs are in a matrix within the Cisco UCS 5108 chassis. This means that a failure of any one power supply does not affect any other component, assuming the correct redundant configuration.



Cisco UCS Power Calculator




















The Cisco UCS Power Calculator (available at http://www.cisco.com/assets/cdc_content_elements/flash/dataCenter/cisco_ucs_power_calculator/) can be used to estimate chassis power consumption for any given configuration. As a general principle, if the maximum AC power estimate from the Cisco UCS Power Calculator exceeds the 5000-W limit, then configuring the system so that it enters high-density mode should be considered for optimal performance.


It is important to point out that the Cisco UCS Power Calculator makes maximum power estimates based on worst case utilization. Maximum estimates are only achievable by HPC applications. Enterprise applications typically use much lower power, and 50 percent utilization is a reasonable figure to use.



Determining Supported Configurations


This topic describes how to determine supported configurations.



Cisco UCS HW and SW Interoperability Tool




















The interactive Cisco UCS Hardware and Software Interoperability Utility lets you view the supported components and configurations for a selected server model and software release. This utility is available at the following URL:


http://www.cisco.com/web/techdoc/ucs/interoperability/matrix/matrix.html.


Hardware compatibility is filtered by the following:


	Cisco UCS B-Series and C-Series server models

	Operating system notes

	Relevant components:

			CNA adapter

	NIC Adapter

	RAID adapter

	VM-FEX adapter, or All adapters.








Software compatibility is filtered by the following:


	Cisco UCS releases

	Operating system vendors

	Operating system versions



Operating system vendors include the following:


	Citrix

	Microsoft

	Novell

	Oracle

	Red Hat

	VMware




UCS Central



UCS Central software manages multiple, globally distributed UCSM domains with thousands of servers from a single pane:


	Scale aggregate to 10,000 servers

	Centralized Inventory of all UCS components

	Centralized, policy-based firmware upgrades

	Global ID pooling




















Cisco UCS Central software manages multiple, globally distributed Cisco UCS domains with thousands of servers from a single pane. It can simplify global policy compliance. The Cisco UCS provides unified, embedded management of all software and hardware components with Cisco UCS Manager. Every instance of Cisco UCS Manager and all of the components managed by it form a domain. Cisco UCS Central integrates with Cisco UCS Manager, and utilizes it to provide global configuration capabilities for pools, policies, and firmware.


Features and Benefits

Cisco UCS Central Software enables global management of many Cisco UCS domains, making staff more efficient and effective. It gives Cisco UCS administrators a high-level view and management of all, or groups of, Cisco UCS domains with:


	Centralized Inventory of all Cisco UCS components for a definitive view of the entire infrastructure and simplified integration with current Information Technology Infrastructure Library (ITIL) processes

	Centralized, policy-based firmware upgrades that can be applied globally or selectively through automated schedules or as business workloads demand

	Global ID pooling to eliminate identifier conflicts

	Global administrative policies that enable both global and local management of the Cisco UCS domains

	An XML API, building on the Cisco UCS Manager XML API for easy integration into higher-level data center management frameworks




UCS Central Architecture and Operation 



Cisco UCS Central Software is a virtual appliance:


	Prepackaged as a VMware virtual disk or a Microsoft Windows Server Hyper-V virtual hard disk (VHD)

	Hosted as an active-standby configuration outside the managed UCS domains

	Downloads firmware packages directly from Cisco.com




















Product Architecture and Operation

Cisco UCS Central Software is a virtual appliance that is prepackaged as a VMware virtual disk or a Microsoft Windows Server Hyper-V virtual hard disk (VHD). Typically deployed in an active-standby configuration, the product is hosted outside the managed Cisco UCS domains. Cisco UCS Central Software securely communicates with Cisco UCS Manager instances to:


	Collect inventory and fault data from Cisco UCS Managers throughout the enterprise (Figure 1)

	Create resource pools of servers available to be deployed within minutes

	Enable role-based management of all resources

	Support the creation of global policies, Cisco UCS service profiles, and templates

	Enable the downloading of and selective or global application of firmware updates

	Invoke individual Cisco UCS Manager GUIs for more detailed management



Cisco UCS Central Software stores global resource information and policies that are accessible through an XML API. In addition, operation statistics are stored in an Oracle or Microsoft SQL database, which allows customers to create their own reports and charts by directly querying the data in the database.


Like Cisco UCS Manager, Cisco UCS Central Software can be accessed through an intuitive GUI, CLI, or XML API for ease of integration with high-level management and orchestration tools. It also downloads firmware packages directly from Cisco.com to facilitate automated global infrastructure and server firmware management.



UCS Central Inheritance Capability



UCS Central Software Globally Centralizes Policy and Configuration Definitions for an Entire Infrastructure:


	Global service profiles and resource pools are centrally defined, then passed to UCS Manager instances

	Each UCS Manager instance controls inheritance

	Can choose which resource classes to obtain locally or from UCS Central Software




















Cisco UCS Central Software is designed and operates similar to Cisco UCS Manager in that policies and configuration definitions, which make up a Cisco UCS service profile, can be created at a central location and then applied to the endpoint recipient, where they are resolved. With Cisco UCS Manager, the endpoint recipients are the Cisco UCS infrastructure (servers, network, etc.).


For Cisco UCS Central Software, the recipients are individual Cisco UCS Manager instances that have been registered with Cisco UCS Central Software. With Cisco UCS Central Software, global Cisco UCS service profiles are defined centrally and are passed to Cisco UCS Manager instances according to the way they are registered with Cisco UCS Central Software.


Through this inheritance capability, Cisco UCS Central Software provides centralized resource pools in which all or part of the configuration can be defined centrally and deployed quickly and efficiently. Inheritance is controlled by each Cisco UCS Manager instance, which can choose which resource classes to obtain locally and which to obtain from Cisco UCS Central Software.



UCS Central Features―Release 1.0



	Domain Groups

	Integrated Dashboard

	Global ID Management

	Centralized Firmware Management

	Global Operation Policies

	Backup Repository and Restore – immediate or scheduled

	UCS KVM and GUI Cross Launch
















Initial Release provides management for up to 5 domains without license, a license must be purchased for additional domains.






UCS Central creates a hierarchy of domain groups for managing multiple UCS domains. Domains can be grouped or ungrouped. A domain group policy can be used to automatically assign new domains to the group upon registering with UCS Central. A domain can only be assigned to one group, and when assigned inherits all the management policies specified for the group. Admin privileges are need to assign domain to group.


Integrated Dashboard provides a summary of the UCS Domains including the Fabric Interconnects, Chassis, IO Modules, Blade and Integrated Rack Servers Inventory. Other items included domain level Service Profiles & Templates, Firmware versions, Fault, Audit, Event Logs, and FSM status.


Centralized Global ID management of MAC, WWN, UUID, IP addresses, and IQNs allows the creation of global pools of entities that can be shared between domains. Tracks and avoids conflicts which are reported as faults.


Centralized Firmware Management aids consistency of firmware across domains.


Global Operation Policies includes DNS, Timezone, Call Home, SNMP, HTTP, Telnet, Web Sessions, CIM XML, Management Interface Monitoring, SSH, LDAP, TACACS, RADIUS, Roles, Locales, Trust Points, Authentication Domains, Fault, Core Export, Syslog , Power Allocation, PSU, System Event Logging and Global Backup/Export Policy.


Backup and Restore of database and configurations can be done per domain groups – you can set the backup of the database or full configuration policy for the domain group – you can backup and restore the UCS Central itself. Backups can be scheduled or done immediately. You can use SCP, FTP, TFTP or SFTP for transfer. You can restore any managed domain, including full state backups.


You can access the KVM console of a server in any domain that has registered and configured, requires the server or SP to have a CIMC address assigned.


In the Release 1.0 management for up to 5 domains is provided for free without license, a license must be purchased for each additional domain to be managed.



Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	The Cisco UCS architecture optimizes three key areas to consider within virtualization: Memory, I/O, and CPU.

	The Cisco UCS chassis provides all of the necessary power, cooling, and networking interconnects to the server nodes (blades).

	Cisco UCS Fabric Interconnects and Fabric Extenders (IOMs) provide both storage and data traffic to the server nodes using 10 Gigabit Ethernet channels. All traffic is aggregated over available uplinks and switching is performed by the fabric interconnect.

	Cisco UCS blade servers are available in half-width and full-width models. Both types support multiple CPUs and hot-swappable hard disks as well as industry-leading memory and I/O capability.

	Cisco UCS Central can be used to manage multiple UCS domains from a single pane.

	Cisco UCS C-Series servers extend unified computing innovations to an industry-standard form factor to help reduce TCO and increase business agility.

	Power requirements and supported hardware or software combinations can be determined using the interactive Cisco UCS Power Calculator and the Cisco UCS Hardware and Software Interoperability Utility.

	The interactive Cisco UCS Hardware and Software Interoperability Utility lets you view the supported components and configurations for a selected server model and software release.








Lesson 3
Cisco UCS Management Framework

Overview

This lesson describes Cisco UCS Management framework and explains the purpose and use of Service Profiles in the Cisco UCS deployment and operations model.


Objectives

Upon completing this lesson, you will be able to describe Cisco UCS Management framework. You will able to meet these objectives:


	Describe the Cisco UCS management architecture

	Describe third-party tools written to the Cisco UCS XML API for monitoring, deployment, and orchestration

	Describe Cisco UCS Service Profiles

	Explain the use of Cisco UCS Service Profile templates

	Describe a hierarchical resource management model for Cisco UCS environments




Lesson 3
Cisco UCS Management Framework

Overview

This lesson describes Cisco UCS Management framework and explains the purpose and use of Service Profiles in the Cisco UCS deployment and operations model.


Objectives

Upon completing this lesson, you will be able to describe Cisco UCS Management framework. You will able to meet these objectives:


	Describe the Cisco UCS management architecture

	Describe third-party tools written to the Cisco UCS XML API for monitoring, deployment, and orchestration

	Describe Cisco UCS Service Profiles

	Explain the use of Cisco UCS Service Profile templates

	Describe a hierarchical resource management model for Cisco UCS environments




Cisco UCS Management Architecture


This topic describes Cisco UCS management architecture.



Cisco UCS Management Architecture



	Management is performed via CLI commands, Cisco UCS Manager GUI, or an XML API.

	Runs on Cisco UCS Fabric Interconnects.

	Access via management IP address on mgmt0 port.

	Management IO traverses FI-Chassis links over dedicated VLANs.




















The entire Cisco UCS is managed from the fabric interconnects using one of the following:


	CLI commands

	the Cisco UCS Manager (UCSM) GUI

	an XML API




Fabric Interconnect Management Connectivity




















Both the Cisco UCS 6120XP and 6140XP Fabric Interconnect can be accessed for OOB management using the mgmt0 Ethernet interface, which supports 10, 100, or 1 Gb/s. The mgmt0 interface is configured with an IPv4 management address. There is also an mgmt1 port, but that port is unused.


In a cluster, both fabric interconnects have an individual IP address, as well as a third cluster IP address that floats between them. The cluster IP address is active on the high availability primary for the control plane, as both fabric interconnects forward data plane traffic simultaneously. Configuration sync is performed automatically in the cluster. In addition to the OOB Ethernet network, there is a console port that operates at 9600 b/s, 8 data bits, 1 stop bit, and no parity (9600 8N1).


On the Cisco UCS 6248UP and 6296UP Fabric Interconnects, the management Ethernet interface and console ports are located on the opposite side.



Cisco UCS Manager: Unified and Extensible




















Extensible Management with Cisco UCS Manager

Cisco Unified Computing System (UCS) Manager provides unified, embedded management of all components of the Cisco UCS and the ability to allocate resources through an XML API. Building on this API, Cisco collaborates with a broad set of higher-level systems management leaders to deliver optimized, tested integrations covering the entire operational lifecycle.


Cisco UCS Manager gives the administrator extensive controls for managing all aspects of the Cisco Unified Computing System. Cisco UCS Manager service profiles allow users to define hardware configurations in software and allocate resources through an open API. Building on this API, Cisco collaborates with a broad set of higher-level systems management leaders to integrate unique Cisco UCS capabilities within their native functions and user interfaces. These integrations enhance performance monitoring of operating system and higher layers of the application stack and consistent cross-system management, especially in heterogeneous environments.


Save Time, Add Value

Cisco UCS Manager delivers granular system visibility and control to minimize time spent on manual operations and troubleshooting of third-party management solutions. In addition, it expedites data center consolidation and allows users to extract more value from Cisco UCS assets.


The Cisco UCS programmable infrastructure allows unified management and provisioning for data center automation. To learn more about the Cisco UCS management ecosystem, read UCS Manager: Operating with the Infrastructure.


Custom Development with Cisco UCS Manager

The Cisco UCS Manager XML API is a fully documented interface to over 9000 managed UCS objects. Cisco provides public access to this API through the Cisco Developer Network (CDN).


To support you in developing for the Cisco UCS XML API, the following resources are available from the CDN:


	Programmer's guides, forums, and blogs

	Cisco UCS Platform Emulator

	Online testing in a hosted sandbox



Cisco UCS Manager Ecosystem partners:


	BMC:

			CA

	Compuware

	Dynamic Ops






	EMC:

			HP

	IBM

	Microsoft






	SolarWinds:

			Symantec

	Vmware

	Zenoss








Combining Cisco UCS Manager with a third-party management solution reduces the time required to deliver new services simply, reliably, and securely from days or weeks to minutes.


Supporting Seamless Migration to the Cisco UCS Platform

Because Cisco UCS Manager is integrated with a broad management ecosystem, administrators can ensure policy consistency and follow best practices for deploying x86 application stacks on the Cisco Unified Computing System with their existing management solutions. Users can easily migrate physical or virtual server images from existing platforms to the Cisco Unified Computing System in software, retaining control over that image’s configuration and health.


Optimizing IT Operations on the Cisco Unified Computing System

Cisco UCS Manager integrations with third parties cover the entire service management lifecycle, from service orchestration, through deployment and configuration, to monitoring and analysis. This approach helps ensure the availability of complete solutions for a variety of important use cases, including the following:


	Workload agility: Third-party tools use Cisco UCS service profiles to optimize the use of Cisco UCS resources, reducing overall IT service costs while accelerating service delivery.

	IT can respond more rapidly to new business requests without disrupting other priorities or taxing limited staff resources to provision new equipment.



The Cisco UCS portfolio encompasses a range of options for addressing I/O, CPU- and memory-bound application challenges. Cisco UCS service profiles enable service orchestration tools to automatically allocate a given workload to the optimal type of server, helping to ensure high-quality application performance.


The Cisco UCS “wire-once” architecture, with network and storage access settings managed in software, is an additional advantage for IT managers. The ability to dynamically allocate a workload to any available machine within a pool of appropriate servers entirely through management tools, without the need for physical set-up, largely eliminates the rationale for maintaining underutilized servers dedicated to specific workgroups or workloads. Instead, the same servers can be rapidly repurposed for different workloads based on peak and off-peak use. Thus, fewer physical servers need to be purchased and maintained. This in turn helps reduce server sprawl, simplifying asset management.


By removing traditional barriers to workload agility, Cisco and its management ecosystem partners help IT to transition the business more easily and reliably to lower-cost, more agile computing models based on shared infrastructure and service abstraction.



UCS Manager: Rapid, Stateless Provisioning 



UCSM Installed on the Fabric Interconnects:


	Service Profile configuration

	Inventory discovery

	Monitoring, diagnostics, statistics collection, fault detection . . .

	Auditing with RBAC




















In order to provide a computing infrastructure that is not tied to physical devices, the Cisco UCS platform separates physical devices (resources) from their configurations (personalities). This abstraction provides a flexible environment where resources can be provisioned and migrated rapidly between physical devices.


To allow rapid, stateless provisioning of the computing cloud, Cisco UCS categorizes physical resources separately from the configurations that may reside on them:


	The Cisco UCS Manager running on the Fabric Interconnect controls various LAN resources, including MAC addresses, IP addresses, VLANs, and LAN ports.

	The Cisco UCS Manager also manages SAN connectivity through VSAN and WWN assignment. The central assignment of WWNs is critical to the portability of service profiles.

	The compute nodes (blade servers) and firmware are also managed as compute resources by the Cisco UCS Manager.

	All system management is handled through the Cisco UCS Manager running on the Fabric Interconnect. The management VLANs, server KVM attachment and low level control are all handled as Cisco UCS resources.




Cisco UCSM HA Cluster



Redundant Cisco UCS fabric interconnects are configured as an administrative cluster:


	Each FI runs an instance of UCSM.

	L1 / L2 connections provide HA and synchronization between UCSM instances.

	L1 / L2 do not provide data path for server IO.




















Configuring two Fabric Interconnects in your Cisco UCS system can provide redundancy for both the Cisco UCS Manager functionality and the switching functionality that the Cisco UCS Fabric Interconnect provides.


High availability requirements include the following:


	Connect L1 of Fabric Interconnect A to L1 of Fabric Interconnect B.

	Connect L2 of Fabric Interconnect A to L2 of Fabric Interconnect B.

	Connect Fabric Interconnect A to IOM A of each Chassis using 1-8 uplinks.

	Connect Fabric Interconnect B to IOM B of each Chassis using 1-8 uplinks.




Cisco UCSM HA Cluster Verification




s6100-A# show cluster extended-state
Cluster Id: 0x76cf5f1a431711df-0xb1f8000decb21744

Start time: Fri Oct  1 14:29:04 2010
Last election time: Fri Oct  1 14:30:12 2010

A: UP, PRIMARY
B: UP, SUBORDINATE

A: memb state UP,lead state PRIMARY, mgmt services state:UP
B: memb state UP,lead state SUBORDINATE, mgmt services state:UP
   heartbeat state PRIMARY_OK

INTERNAL NETWORK INTERFACES:
















HA READY ← Indicates successful Cluster-State update to chassis EEPROM
Detailed state of the chassis selected for HA storage:
Chassis, serial: FOX1307H0M8, state: active






The show cluster extended-state command includes information that administrators need to validate cluster configuration and troubleshoot cluster operation. The output on this screen indicates that this fabric interconnect is the primary management node. The first fabric interconnect in a cluster is always assigned as fabric A.


All configuration is performed on the active management node. Before the active node commits a configuration transaction to the data management engine (DME), it first replicates the transaction to the subordinate node. After the subordinate acknowledges that its DME committed the transaction, the primary node commits the transaction.


On the active management node, the management plane and data planes are both active.


The subordinate node is not in a passive “standby” mode. The DME is active and accepts data that is synchronized from the active management node. Therefore, a distinction is made that it is subordinate active. If the active management node fails, the subordinate will wait a predetermined time and then declare the peer dead. At that time, the subordinate becomes the active management node.


Local Storage

Each Cisco UCS Fabric Interconnect maintains its own local storage in NVRAM and flash memory. Local storage contains static data; that is, storage that does not change with cluster membership changes. For example, installable images are stored in the /bootflash partition of internal flash memory. Data such as installable images are replicated at runtime, while both cluster members are present in the cluster. You do not need to (nor can you) download images via the Cisco UCS Manager interface to individual nodes. The download is replicated to both nodes. If a node is not present during an image download, then that image is replicated to that node when the node rejoins the cluster.


Chassis EEPROM

Each chassis management controller maintains its own part of the shared chassis storage in the serial EEPROM. Chassis storage contains a combination of static and dynamic information. For example, the static portion contains the node ID for each node that is configured in the cluster. The dynamic portion contains the version of the configuration as seen by that node. There is no need to replicate the contents of the serial EEPROM. Each node maintains its own portion, whereas both nodes may read from both topics.



Cisco UCS Manager GUI




















The Cisco UCSM GUI is divided into two primary panels. The navigation panel lets you move through the configuration elements for physical and logical components. The content panel displays the current context of the navigation panel. As you move through the GUI, the navigation bar above the content panel indicates your path in the hierarchy. You can click any element in the navigation bar to change the current context of the content panel.


		
The navigation bar is sometimes referred to casually as the breadcrumb trail.





Standard Cisco UCS Interface Options



	Provide access to a complete Cisco UCS solution

	Standardized protocols

	Read-only access vs. read/write access

	Access through the Cisco UCS management IP address




















The native methods of communicating with the Cisco UCSM continue with the following: 


	SNMP: SNMP is a TCP/IP application layer protocol that facilitates the exchange of management information between network devices. It is used mostly in network management systems to monitor attached devices for conditions that may require administrative attention. SNMP exposes management data in the form of variables that describe the system configuration. You can query these variables with managing applications. On the Cisco UCS, SNMP data is read-only through the fabric interconnects and is typically used for fault monitoring on all of the Cisco UCS components. If a fault occurs, the Cisco UCS can send an SNMP trap describing the fault. SNMP version 1, 2c, and 3 are supported in the Cisco UCSM.

	SMASH-CLP: SMASH-CLP is a standard method for performing remote OOB management of server hardware. SMASH-CLP enables administrators to use a consistent CLI for servers, independent of vendor, operating system, or hardware platform. In the Cisco UCS, you can use this interface for monitoring and debugging. On the Cisco UCS, SMASH-CLP data is read-only. SMASH-CLP standards are developed and maintained by the DMTF industry group. For more information, visit the DMTF website at: http://www.dmtf.org.

	CIM-XML: CIM is an open standard that defines how managed elements in an IT environment are represented as a common set of objects and their relationships. CIM enables the consistent management of servers, independent of vendor or provider. CIM standards are developed and maintained by the DMTF industry group. CIM is composed of a schema and a specification. The schema provides the model descriptions. The specification defines the details for integration with other management models. CIM-XML, another DMTF standard, is a Web-Based Enterprise Management (WBEM) protocol. This standard uses XML over HTTP to exchange CIM information. In the Cisco UCS, you can use this interface for blade and chassis-related monitoring, debugging, and inventory collection by software frameworks that follow the CIM-XML standard. For information on CIM-XML specifications, go to the DMTF website at: http://www.dmtf.org.

	Syslog: Syslog is a valuable tool for helping you to define rules and policies for the Cisco UCS. Syslog is an industry standard message logging program. Using syslog data, you can create scripts to separate software-generated messages and to store generated reports. Other programs can also be used to analyze this information. Syslog also provides devices with a method for evaluating performance and for notifying administrators of problems. Syslog data can be used for computer system management and security auditing and for generalized analysis and debugging. When using syslog for active monitoring, you must determine if monitored items should trigger an action that is based on the occurrence of specific events. For more information on syslog, syslog tools, and syslog usage examples, go to http://www.syslog.org.

	Cisco Call Home and Smart Call Home: These Cisco applications provide email-based notification for supporting critical system policies. You can use these applications to page a network support engineer, or email a Network Operations Center (NOC). Cisco Call Home provides email- and web-based notification of critical system events. Cisco Smart Call Home can be used to automatically generate a case with the Cisco Technical Assistance Center (Cisco TAC). A range of message formats are available for compatibility with pager services or XML-based automated parsing applications.




Cut-Through Interface Options



	Provide direct access to a single server

	Standardized protocols

	Access through unique external management IP address

	Assigned by the Cisco UCSM




















The native methods of communicating with the Cisco UCSM continue with the following: 


	SoL: SoL is a mechanism that directs the input and output of a system serial port to a network. This capability allows an administrator to remotely manage a single server through a console with full keyboard and video (text) access. To use this feature, the operating system must be configured correctly and there must be an event listening device that is configured on the console port. SoL uses UDP port 623 for communication.

	KVM: KVM technology can control multiple computers from one keyboard, video monitor, and mouse. A KVM switch is useful where there are multiple computers, and a dedicated keyboard, monitor, and mouse is not required for each one. A KVM switch is commonly used in data centers where multiple servers are placed in a single rack. KVM can be implemented so that an administrator can conveniently connect to any server in the data center. On the Cisco UCS, KVM can be launched from the GUI or web-based interface. KVM can be used remotely to interact with a single server to install an operating system or troubleshoot issues that are related to the operating system. KVM also enables virtual media access to ISO images, or to physical drives such as CD and DVD players on a client running KVM. KVM communicates with the Cisco UCS over IP; no extra cabling is required. On the Cisco UCS, you have full GUI access through KVM. With a KVM console launched from within the GUI or web-based interface, a user is not required to know the external IP address. The Cisco UCS KVM console provides a video over IP representation of the blade server video output. KVM is also provided as a standalone application that authenticates through the Cisco UCSM. Although the Cisco UCS provides a powerful, single point-of-management, getting quick access remotely to a server through KVM is a very effective method of troubleshooting an operating system problem.

	IPMI: The IPMI defines interfaces and messaging for OOB server management. Using this interface, you bypass the Cisco UCSM and directly monitor and manage single server hardware information such as voltage, CPU statistics, and ambient temperature. This data is captured by sensors and made available at the Cisco IMC. The IPMI interface is commonly used by data center management software to collect hardware-related information, and to perform basic management operations like reboot and power on or off. In the Cisco UCS, this functionality is a small subset of the capabilities that are provided natively by the Cisco UCSM through the XML API. A common example of IPMI usage is the Distributed Power Management (DPM) capability of VMware vSphere. With DPM, vCenter powers off ESXi hosts when demand is low, and then powers them on again when needed. For information on IPMI specifications, go to http://www.intel.com/design/servers/ipmi/index.htm.




Web-Based KVM Access




















On the Cisco UCS, KVM can be launched from the GUI or web-based interface and can be used remotely to interact with a single server to install an operating system or troubleshoot operating system-related issues. KVM also enables virtual media access to ISO images or to physical drives such as CD or DVD players on a client running KVM. KVM communicates with the Cisco UCS over IP; no extra cabling is required.


On the Cisco UCS, you have full GUI access through KVM. With a KVM console launched from within the Cisco UCS GUI or web-based interface, a user is not required to know the external IP address. The Cisco UCS KVM console is a video-over-IP representation of the blade server video output. KVM is also provided as a standalone application, but it authenticates through the Cisco UCSM.


Although the Cisco UCS provides a powerful, single point-of-management, getting quick access remotely to a server through KVM is a very effective method of troubleshooting an operating system problem.



Third Party Management and Orchestration Tools


This topic describes examples of current list of third-party tools written to the Cisco UCS XML API for monitoring, deployment, and orchestration.



Management Interface Integration




















The XML-API and industry-standard interfaces, such as IPMI and SNMP, make it easy to integrate the Cisco UCS into various data center management and monitoring applications.


The Cisco UCSM XML API

The Cisco UCSM XML API is a programmatic interface to the Cisco UCS. The API accepts XML documents through HTTP or HTTPS. Developers can use any programming language to generate XML documents that contain the API methods. Configuration and state information of the Cisco UCS is stored in a hierarchical tree structure that is known as the management information tree, which is completely accessible through the XML API.


The API model is recursively driven and provides major functionality for application development. For example, changes can be made on a single object, an object subtree, or the entire object tree. With a single API call, changes can be made to a single attribute of an object, or to the entire Cisco UCS structure, including the configuration of chassis, blades, adapters, polices, and most other hardware and software components.


The API operates in forgiving mode. Missing attributes are substituted with default values (if applicable) that are maintained in the internal data management engine (DME). The DME ignores incorrect attributes. If multiple managed objects (MOs) are being configured (for example, virtual NICs), and any of the MOs cannot be configured, the API stops its operation. It returns the configuration to its prior state, stops the API operation that listens for API requests, and sends a fault notification.


The API leverages an asynchronous operations model to improve scalability and performance. Because slower API processes are nonblocking, faster processes can proceed. A process receives a Success message upon a valid request, and a complete message when the task is finished. Full event subscription is enabled. After subscribing, any event notification is sent along with its type of state change.


Updates to MOs and properties conform to the existing object model, ensuring backward compatibility. If existing properties are changed during a product upgrade, they are managed during the database load after the upgrade. New properties are assigned default values.


Operation of the API is transactional and terminates on a single data model. The Cisco UCS is responsible for all endpoint communication, such as state updates; users cannot communicate directly to endpoints. In this way, developers are relieved from the task of administering isolated, individual component configurations.


The API model includes the following programmatic entities:


	Classes: Define the properties and states of objects in the management information tree

	Methods: Actions that the API performs on one or more objects

	Types: Object properties that map values to the object state (for example, equipmentPresence)



A typical request comes into the DME and is placed in the transactor queue in FIFO order. The transactor gets the request from the queue, interprets the request, and performs an authorization check. After the request is confirmed, the transactor updates the management information tree. This complete operation is performed in a single transaction.



Cisco UCS Manager Ecosystem*



	Partner
	Service Orchestration
	Monitoring and Analysis
	Deployment and Configuration

	VMware
	vCloudDirector - vCenter Orchestrator
	
	

	EMC
	EMC Ionix Unified Infrastructure Manager 
	
	

	Microsoft
	System Center Orchestrator 
	System Center Operations Manager
	System Center Operations Manager

	BMC
	Cloud Lifecycle Manager
	ProactiveNet Performance Manager
	Bladelogic Server Automation - including Compliance Manager

	CA
	Spectrum Automation Manager
	eHealth Performance Manager - Spectrum Infrastructure Manager
	Spectrum Automation Manager

	HP
	HP Operations Orchestration
	HP Operations Manager
	Network Automation 

	IBM
	Service Delivery Manager
	Tivoli Monitoring - Netcool/Omnibus
	Tivoli Provisioning Manager - Network Mgr

	Cloupia
	Unified Infrastructure Controller
	
	







Cisco Unified Computing System (UCS) is certified as a platform for leading enterprise applications, virtualization, and operating systems. For more detailed matrices on application, OS and hypervisor interoperability please see the Unified Computing System – Technical References: http://www.cisco.com/en/US/products/ps10477/prod_technical_reference_list.html page.


Cisco UCS Manager is fully interoperable with the major systems management tools commonly found in heterogeneous data centers.


		
This is not a comprehensive list of the third-party tools written to integrate with Cisco UCS Manager but just an example list of some leading solutions.





Cisco UCS Service Profiles


This topic describes Cisco UCS Service Profiles for hardware ‘state’ abstraction.



Cisco UCS Service Profiles



A Service Profile is a logical representation of a bare-metal server, specifying:


	UUID, MAC, WWN, etc.

	Boot order, firmware, etc.

	VLAN, VSAN, QoS, etc.




















Understanding the Service Profile concept is key to understanding blade server management in Cisco UCS. Service Profiles contain the personality (behavior and identity information) for a bare-metal server. In this manner, servers are not tied to a specific physical compute node. The service profile defines server hardware (configuration, firmware, identity, and boot information), fabric connectivity, policies, external management, and high availability information.


The Service Profile represents a logical view of a single blade server, without requiring you to know exactly which physical blade server it might be running on. The profile object contains the server personality (identity, network information, and so on) and connectivity requirements. The profile can then be associated with a single physical blade server (one blade server at a time). The concept of profiles is important to the concept of mobility, transferring the identity of a logical server transparently from one physical blade server to another, as well as pooling concepts (discussed in the next lesson).


In order to provide a computing infrastructure that is not tied to physical devices, the Cisco UCS platform separates physical devices (resources) from their configurations (personalities). This abstraction provides a flexible environment where resources can be provisioned and migrated rapidly between physical devices.


To allow rapid, stateless provisioning of the computing cloud, Cisco UCS categorizes physical resources separately from the configurations that may reside on them:


	The Cisco UCS Manager running on the Fabric Interconnect controls various LAN resources, including MAC addresses, IP addresses, VLANs, and LAN ports.

	The Cisco UCS Manager also manages SAN connectivity through VSAN and WWN assignment. The central assignment of WWNs is critical to the portability of service profiles.

	The compute nodes (blade servers) and firmware are also managed as compute resources by the Cisco UCS Manager.



All system management is handled through the Cisco UCS Manager running on the Fabric Interconnect. The management VLANs, server KVM attachment and low level control are all handled as Cisco UCS resources.


Every physical compute node that is running concurrently must have its own Service Profile, even if their profiles seem very similar (having exactly the same connectivity requirements, for example). Service Profile Templates provide an easy way for creating large numbers of very similar Services Profiles.


The figure shows the Cisco UCS server deployment model in which compute nodes (servers) are stateless. Servers obtain their properties from the Cisco UCS Manager, via pools of identity resources, and according to the policies that the Cisco UCS administrator specifies. Policies include things like boot order policy, scrub (cleanup) policy after a migration, and so on.



Service Profile Components




















Service Profiles contain the personality (behavior and identity information) for a logical server. In this manner, servers are not tied to a specific physical compute node. The service profile defines server hardware (configuration, firmware, identity, and boot information), fabric connectivity, policies, external management, and high availability information.


The figure summarizes the contents of a Service Profile. Examples of blade server identity information that are unrelated to specific network or disk adapters includes the Universally Unique Identifier (UUID), BIOS boot order, and so on. The UUID for the server can have a significant impact on the portability of operating systems and applications. Some systems tie licensing to the UUID of the authorized server. The Service Profile also holds network adapter identity information. All of this information is physically applied to the blade server that is associated at a particular time with that profile.


In this manner, blade servers that are associated with a particular profile at different points in time are indistinguishable from each another, which can be advantageous when dealing with identity-based licensing schemes or network and external storage configurations.



Service Profiles Provide Server Mobility



Service Profiles can be migrated to another compute node for upgrade or fault-tolerance.



















Every physical compute node that is running concurrently must have its own Service Profile, even if their profiles seem very similar (having exactly the same connectivity requirements, for example). Two easy ways for creating large numbers of very similar Service Profiles are available. These are cloning and templates. These methods will be discussed later.


The figure shows the server mobility enabled through Service Profiles and stateless compute nodes. A logical server or Service Profile is defined with an identity including the following:


	Universally Unique identifier (UUID)

	MAC

	WWN

	VLAN/VSAN assignment



The Service Profile is associated with one blade at a time but can be migrated in the event of failure or required maintenance. Mobile logical servers are most effective when booted from SAN environments where the server’s operating system image resides on a shared storage LUN. When the Service Profile is migrated, the new compute node inherits the server-specific properties that are stored within that Service Profile configuration. No configuration changes are required on the LAN or SAN infrastructures due to the portability of unique server characteristics such as WWPN and MAC address. Internal disks on compute nodes can be used for temporary storage or swap files.



Virtualization Layer and Service Profiles



	A Service Profile is not directly comparable with the function of a hypervisor

	OS (hypervisor) is unaware of underlying hardware state abstraction provided by the Service Profile




















Server virtualization allows multiple application and operating system environments to run on a single physical blade. What Cisco UCS provides is not directly comparable with the function of a hypervisor, which provide a software-based solution that enables server virtualization at a software level.


Cisco UCS ensures server hardware compatibility across the entire Cisco UCS system. What this means is that the hypervisor will run on any blade in the system, and our software ensures that the hardware is identically configured, regardless of the blade from which the VMware ESX server is chosen to run.



Resource Pools for Service Profiles



Pools ensure uniqueness of identities:



















Stateless computing requires identifiers that are not hardware derived, including universally unique identifiers (UUIDs), MAC addresses, and world wide names (WWNs) for Fibre Channel. The use of pooled resources helps to ensure the consistent application of policy, and that identities are unique within the Cisco UCS Manager.



Server Resource Pools
















Assign blades into multiple pools as spares.


Qualification Policies dictate pool assignments based on:


	RAM | CPU | blade model | adapter | more . . .







Groups of compute nodes can be pooled. This pooling is typically based on processors, memory, and I/O device requirements. By selecting a pool instead of a physical blade, a Service Profile can be quickly deployed, based on pre-defined requirements. Administrators can also choose to manually pool servers for organizational or functional reasons.



Service Profile Policies



Policies define the way in which a Service Profile selects or utilizes Cisco UCS resources.



















Policies defined within Service Profiles allow specific criteria to be selected during server deployment. These policies also define the specific configurations that will be applied at the time of deployment. In this example, disk configuration policy will configure the directly attached drives in a mirrored configuration and protect the information stored on these drives in the event of profile disassociation.


This figure illustrates the various types of Service Policies that can be defined within a Server Profile.



Service Profile Templates


This topic explains the use of Cisco UCS Service Profile templates.



Service Profile Templates




















Service Profile Templates enable you to create a large number of similar service profiles. With a service Profile Template, you can quickly create several Service Profiles with the same basic parameters, such as the number of vNICs and vHBAs, and with identity information drawn from the same pools.


A Service Profile Template is very similar to a service profile except it cannot be associated with a physical server. It can, however, be used for instantiation of multiple service profiles. This is useful for operators that need to instantiate a large number of service profiles. A senior server administrator can create a template for a certain purpose, like a Database Template, or Application Template. This enables other administrators to instantiate new Service Profiles with little or no expertise of requirements for a certain application or software.


		
If you need only one service profile with similar values to an existing service profile, you can clone a service profile in the Cisco UCS Manager GUI. 





Service Profile Template Types








Modifications must be made to individual profile.













There are two types of templates: Initial Templates and Updating Templates. The difference is that a service profile created from an Inital Template does not inherit any new configuration changes made to the template after the service profile has been instantiated. A service profile that has been created from an Updating Template maintains the relationship and keeps inheriting any configuration changes made to the template:


	Initial Template: Service profiles created from an Initial Template inherit all of the properties of the template. However, after you create the profile, it is no longer connected to the template. If you need to make changes to one or more profiles created from this template, you must change each profile individually.

	Updating Template: Service profiles created from an Updating Template inherit all properties of the template and remain connected to the template. Any changes to the template automatically update the Service Profiles created from the template.




Managing Cisco UCS Resources


This topic describes a hierarchical resource management model for Cisco UCS environments.



Cisco UCS Management Hierarchy



Organizations define hierarchical management structure for multitenant environments:


	Creates logical isolation between organizations, divides large physical infrastructure

	Compute nodes are independent of organizations; can be associated with Service Profile in any organization




















Cisco UCS enables administrators to divide a large physical infrastructure into logical entities to provide logical isolation. This division eliminates the need to use a dedicated physical infrastructure for each organization. Such entities are called organizations within Cisco UCS.


Organizations are hierarchically organized in a tree where a top level organization is always root. Cisco UCS organizations enable administrator to assign unique resources via related organizations. These resources can include policies, pools, and Service Profiles and Templates.


The policies and pools that you create in root are system-wide and are available to all organizations in the system. However, any policies and pools created in other organizations are only available to organizations that are above it the same hierarchy. For example, if a system has organizations named Finance and HR that are not in the same hierarchy, Finance cannot use any policies in the HR organization, and HR cannot access any policies in the Finance organization. However, both Finance and HR can use policies and pools in the root organization.


Organizations affect only the management of resources, and have no impact on server management. At the operating system level and below, management roles are determined by the operating system management system, such as Microsoft Active Directory.


Compute nodes are never part of an organization. Any compute node can appear in as many pools in as many different organizations as you like. Any compute node can be associated with a Service Profile in any organization.


While server pools are part of an organization, they are advisory only. There is nothing to prevent associating any individual compute node to any Service Profile in any organization (bypassing the pools), regardless of how many pools the compute node might reside in.


Note how the rule is basically “first come, first served”. The first profile that successfully selects a compute node for association (through pools, or directly without pools) is the owner of that blade server until it is made available again through disassociation.



Role Based Access Control



Method of authorizing user system access based on user roles and privileges:


	Ability to grant privileges based on user responsibilities

	Privileges inherited by all sub-organizations

	Does not control access to server-deployed operating system




















Role-Based Access Control is a method of restricting or authorizing system access for users based on user roles and locales. A role defines the privileges of a user in the system. Privileges give users assigned to user roles access to specific system resources and permission to perform specific tasks. There are default privileges and the user roles given those privileges in the Cisco UCS. Roles can be created, modified to add new or remove existing privileges, or deleted. When a role is modified, the new privileges are applied to all users assigned to that role. Privilege assignment is not restricted to the privileges defined for the default roles. That is, you can use a custom set of privileges to create a unique role.


A user can be assigned one or more roles. A user assigned multiple roles has the combined privileges of all assigned roles. For example, if Role1 has storage-related privileges, and Role2 has server-related privileges, then users who are assigned to both Role1 and Role2 have storage- and server-related privileges. All roles include read access to all configurations on the system. The difference between the read-only role and other roles is that a user who is only assigned the read-only role cannot modify the system state. A user assigned another role can modify the system state in that user's assigned area or areas.


There is a predefined set of privileges within the Cisco UCS. The following two privileges grant some special actions:


	Admin: User assigned role with this privilege can do anything in the Cisco UCS in any organization.

	AAA: User assigned role with this privilege can do the administration of the RBAC features.



Cisco UCS Manager also predefines the following roles:


	Administrator: Enables complete read-and-write access to the entire system.

	Network administrator: Enables read-and-write access to switch infrastructure and network security operations. Read access to the rest of the system.

	AAA administrator: Enables read-and-write access to users, roles, and AAA configuration. Read access to the rest of the system.

	Server administrator: Enables read-and-write access to server-related operations. A user with this role has privileges to manage logical servers. Read access to the rest of the system.

	Storage administrator: Enables read-and-write access to storage operations. Read access to the rest of the system.

	Read only: Enables read-only access to system configuration with no privileges to modify the system state.

	Operations: Enables read-and-write access to systems logs, including the syslog servers, and faults. Enables read access to the rest of the system.



Using RBAC without Organizations

In cases where RBAC is used without organizations (everything is in the root organization), the RBAC style of delegated management advantages can still be used. Users can be segregated based on their responsibilities. There can be users that have control over the logical server structure and other users that have control over the border (LAN and SAN) configuration, for example.


Using Organizations without RBAC

In cases where organizations are used without RBAC, the organizations structural management hierarchy can be utilized to better manage large amounts of equipment and large numbers of logical objects. In addition, some of the pool inheritance features can scale the design logic.



Locales



Locales are containers of organizations:


	Organizations do not have to be related in hierarchy

	Prohibits role inheritance to sub-organizations

	Locales are assigned to users; useful for assigning privileges to many organizations with one role




















Locales provide a mechanism to group multiple organizations together for purposes of granting roles to users. A user can be assigned one or more locales. Each locale defines one or more organizations to which the user is allowed access.


Locales can be used without external authentication systems for local users, but are required for external AAA servers, such as RADIUS, TACACS+, or Lightweight Directory Access Protocol (LDAP).



Centralized Authentication




















Cisco UCS supports RADIUS, TACACS+, and LDAP for centralized RBAC authentication. These are common authentication systems in today’s data centers.


Cisco UCS supports two methods to authenticate user logins:


	Local authentication with user database deployed locally to the Cisco UCS Manager

	Remote authentication by using one of the following protocols:

			LDAP

	RADIUS

	TACACS+









Cisco UCS Backups



Full-State


Complete binary dump of UCS database:


	Contains all configuration, FSM, and status

	Restored only with complete configuration wipe and reboot

	Tar-ball (gzip) format
















Configuration


XML format; excludes FSM:


	Logical: Service Profiles, templates, VLAN and VSAN configuration, Organizations, Locales

	System: AAA configuration, RBAC, User database, Cisco UCSM configuration




















Full-state is a disaster recovery-style backup that is intended for a catastrophic failure of both your Cisco UCSMs. This backup type includes all the runtime state information (for example, finite state machine [FSM] state of blades, associated state of Service Profiles, and so on), as well as configuration information (for example, users, policies, and so on).


Since the full-state backup includes runtime state, this type of backup can very quickly become out-of-date. Any changes to blade Service Profile associations will render this backup obsolete. If a full-state backup is applied that contains out-of-date association information, Cisco UCS does not make any effort to validate the associations. Only through a manual (and disruptive) rediscovery and re-association process can the runtime state be brought back into sync.


Configuration backups are Extensible Markup Language (XML) representations of the configuration of the Cisco UCS Manager. You can edit configuration backups using an XML editor or text editor. This makes them useful for creating templates for other Cisco UCS implementations, or for adjusting them if changes were made to the environment since the backup was last taken. No runtime state data (Service Profile associations, and so on) is stored within these backups.


XML configuration backups can be very useful for more than just recovery of a failed system. XML is easily edited, and can be modified or customized before import into a new system.


There are three types of configuration backup options when creating a backup job:


	config-logical: All configuration that is not associated with authentication, authorization, and accounting (AAA) (such as configured organizations, configured threshold policies, and configured VLANs and VSANs in your LAN and SAN clouds, respectively).

	config-system: All configuration that specifically pertains to the AAA role. Examples include RADIUS, Lightweight Directory Access Protocol (LDAP), TACACS, and users.

	config-all: A combination of config-logical and config-system.




Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	Service Profiles contain identity and state information for a logical server.

	Extensible Management with Cisco UCS Manager is demonstrated by the large number of partner applications written for the UCS XML API.

	Using pooled resources ensures consistent application of policy and reasonable assurances that identities are unique within the Cisco UCS Manager. Use SSH protocol to access the Cisco UCS Manager CLI.

	Policies defined within Service Profiles allow specific criteria to be selected during server deployment.

	With a Service Profile template, you can quickly create several Service Profiles with the same basic parameters.

	Cisco UCS enables administrator to divide large physical infrastructure into logical entities called organizations.

	Cisco UCS organizations enable administrator to assign unique resources via related organizations.

	Role-Based Access Control is a method of restricting or authorizing system access for users based on user roles and locales.








Lesson 4
Cisco UCS Core Network Connectivity

Overview

This lesson provides a detailed description of Cisco UCS LAN and SAN networking components.


Objectives

Upon completing this lesson, you will be able to provide a detailed description of Cisco UCS core network connectivity. You will able to meet these objectives:


	Describe Cisco UCS core network connectivity requirements

	Describe End Host Mode on the Cisco UCS Fabric Interconnect

	Describe EHM with L2 Disjoint Networks in Cisco UCS Release 2.0

	Describe Cisco UCS SAN connectivity requirements

	Describe SAN End Host Mode on the Cisco UCS Fabric Interconnect




Lesson 4
Cisco UCS Core Network Connectivity

Overview

This lesson provides a detailed description of Cisco UCS LAN and SAN networking components.


Objectives

Upon completing this lesson, you will be able to provide a detailed description of Cisco UCS core network connectivity. You will able to meet these objectives:


	Describe Cisco UCS core network connectivity requirements

	Describe End Host Mode on the Cisco UCS Fabric Interconnect

	Describe EHM with L2 Disjoint Networks in Cisco UCS Release 2.0

	Describe Cisco UCS SAN connectivity requirements

	Describe SAN End Host Mode on the Cisco UCS Fabric Interconnect




Cisco UCS Core Network Connectivity


This topic describes Cisco UCS core network connectivity requirements.



Cisco Data Center Topology with UCS




















The figure shows the graphical detail of the core network connectivity for the Cisco UCS.



Cisco UCS | Nexus | MDS Connectivity




















As depicted in the figure, deploying Cisco Nexus and MDS switches in the core provides distinct advantages, specifically, the ability to configure several link-aggregation options.


Virtual Port Channels (VPCs) are configured on Cisco Nexus switches provide link-aggregation between a single fabric-interconnect and two uplink Nexus switches. This capability is also available using the Virtual Switching System (VSS) on the Catalyst 6500, which creates a single control-plane across the two Catalyst switches (stacking). VSS is not supported on the Cisco Nexus platform. The VPC capability allows control-plane isolation on each Nexus switch for additional fault-tolerance.


Fibre channel uplinks from the fabric interconnects to Cisco MDS 9000 Multilayer data switches provides link aggregation using FC port channels. Fibre Channel Protocol (FCP) has no industry standard similar to LCAP for link aggregation. It is strictly a vendor-proprietary feature.


Also depicted in the figure is the ability to aggregated links between the blade-chassis IO Modules and the fabric interconnects. This feature is called fabric port channels and is new with Cisco UCS release 2.0. Fabric port channels are supported only on the Cisco UCS 6200 Series.


Traffic exiting the fabric interconnects are Ethernet, FCoE, or Fibre Channel. FCoE traffic is always used between the blade and the fabric interconnect for FC block-based I/O. FCoE is not yet supported end-to-end in Cisco UCS platforms.



Nexus Virtual Port Channels



VPC allows link aggregation from the fabric interconnect to two different Cisco Nexus Switches:


	A single port channel is created across both Nexus switches.

	LCAP 802.3ad must be enabled on the VPC:


(config-int)# channel-group n mode active






















A virtual port channel (vPC) allows links from the Cisco UCS Fabric Interconnect that are physically connected to two different Cisco Nexus Switches to appear as a single port channel by a third device. The Cisco UCS fabric interconnects can be used to connect to two upstream Nexus Switches for physical redundancy and to make use of all the upstream bandwidth using IEEE 802.3ad port channels.


vPC Terminology

The terminology used in vPCs is as follows:


	vPC: The combined port channel between the vPC peer devices and the downstream device.

	vPC peer device: One of a pair of devices that are connected with the special port channel known as the vPC peer link.

	vPC peer link: The link used to synchronize states between the vPC peer devices. Both ends must be on 10-Gigabit Ethernet interfaces.

	vPC domain: This domain is formed by the two vPC peer link devices. It is also a configuration mode for configuring some of the vPC peer link parameters.

	vPC peer-keepalive link: The peer-keepalive link between vPC peer devices to ensure that both devices are up. The peer-keepalive link sends configurable, periodic keep-alive messages between devices connected by the vPC peer link on a separate link. We recommend that you configure a separate VRF, with a Layer 3 port from each vPC device in this VRF, for the peer-keepalive link. If you do not configure a separate VRF, the system uses the management VRF by default.

	The peer-keepalive link: Provides a Layer 3 communications path for a secondary test to determine whether the remote link is operating properly. No data or synchronization traffic moves over the vPC peer-keepalive link. The only traffic on this link is a message that indicates that the originating switch is operating and running vPC.

	vPC member port: Interfaces that belong to the vPCs.



For known unicast traffic, you should use the local links of the vPC because you cannot load balance traffic across the peer link. Unknown unicast, multicast, and broadcast traffic are flooded across the vPC peer link. The software keeps the multicast forwarding state synchronized between the two peers for groups learned over the vPC link. Configuration information flows across the vPC peer links using the Cisco Fabric Services over Ethernet (CFSoE) protocol. All MAC addresses for those VLANs configured on both switches are synchronized between vPC peer switches. The software uses CFSoE for this synchronization.



Fabric Port Channel Mode



Fabric port channels supported only on the Cisco UCS 6200UP using UCS 6200 Series connected to UCS 2204XP IOM or 2208XP IOM:


	System will automatically fallback to discrete mode when connecting to Cisco UCS 2104XP IOM, regardless the chassis discovery policy

	Deploying the Cisco UCS 2208XP IOM with the UCS 6100 Series, all eight ports in discrete mode




















The connectivity options from the Fabric Interconnects to the IOMs in the chassis focusing on all Generation 2 hardware allow two modes of operation for the IOM: discrete mode and port-channel mode. Depending on the IO module employed, it is possible to configure 1, 2, 4, or 8 uplinks from each IOM in either discrete mode (non-bundled) or port-channel mode (bundled.)


In port-channel mode, all available links are aggregated and a port-channel hashing algorithm (TCP/UDP + Port VLAN, non-configurable) is used for load-balancing server traffic. In this mode, all server links are pinned to the logical bundle rather than to individual IOM uplinks.


In this scenario, when a port fails on an IOM port-channel, the load-balancing algorithms handle failing the server traffic flow to another available port in the channel. This failover will typically be faster than NIC-teaming/bonding failover. This will decrease the potential throughput for all flows on the side with a failure, but will only effect performance if the links are saturated.


Fabric port channels are supported only on the Cisco UCS 6200 Series. It can be enabled when the Cisco UCS 6200 Series is connected to the Cisco UCS 2204XP or 2208XP IOM. When connecting to the Cisco UCS 2104XP IOM, the system will automatically fallback to discrete mode, even with the chassis discovery policy set to Port Channel (PO). When the Cisco UCS 2208XP IOM is connected to the Cisco UCS 6100 Series, it can take advantage of all eight ports in discrete mode.



UCS 2104XP IOM Discrete Mode Connectivity
















	Number of Uplinks
	Uplink Assignment

	1
	Uplink 1: All slots

	2
	Uplink 1: Odd slots
Uplink 2: Even Slots


	4

	Uplink 1: Slots 1, 5

Uplink 2: Slots 2, 6

Uplink 3: Slots 3, 7

Uplink 4: Slots 4, 8








In discrete mode a static pinning mechanism is used to map each blade to a given IO module uplink port (dependent on number of uplinks used). This means that each blade will have an assigned uplink on each IOM for inbound and outbound traffic. In this mode, if a link failure occurs, the blade will not ‘re-pin’ on the side of the failure. Instead, it relies on NIC-Teaming/bonding or Fabric Failover for failover to the redundant IOM/Fabric.


The pinning behavior is as follows with the exception of 1-Uplink in which all blades use the only available Port. The same port-pinning will be used on both IOMs. Therefore, in a redundant configuration, each blade will be uplinked via the same port on separate IOMs to redundant fabrics.


The draw of discrete mode is that bandwidth is predictable in link failure scenarios. If a link fails on one IOM, that server will fail to the other fabric rather than adding additional bandwidth draws on the active links for the failure side. In summary, it forces NIC-teaming/bonding or Fabric Failover to handle failure events rather than network based load-balancing.



UCS 2208XP IOM Discrete Mode Connectivity
















	Number of Uplinks
	Uplink Assignment

	1
	Uplink 1: All slots


	2
	Uplink 1: Odd slots
Uplink 2: Even Slots



	4
	Uplink 1: Slots 1, 5
Uplink 2: Slots 2, 6

Uplink 3: Slots 3, 7

Uplink 4: Slots 4, 8 


	8
	1 Uplink to 1 Slot (1:1)



	UCS 6100 to 2208XP

	UCS 6200 to 2208XP







With the Cisco UCS 2208XP IOM and the Cisco UCS 6200UP Series, the chassis can be uplinked in discrete mode in the same manner that is used with the Cisco UCS 2108XP and Cisco UCS 6100 Series, or a port channel can be created with a two-, four-, or eight-port configuration.



Fabric Interconnect: End Host Mode


This topic describes EHM (EHM) and its importance in forwarding over multiple Layer 2 links and maintaining a loop-free topology.



Ethernet EHM versus Switching Mode



	EHM (default) allows multiple active Layer 2 forwarding links by pinning server MAC addresses.

	Switching mode enables Spanning Tree Protocol, which places all but one uplink in blocking state.




















Although the fabric interconnects are capable of operating in Ethernet switching mode, default EHM is the preferred mode of operation. In Ethernet switching mode, the fabric interconnects must run STP to maintain a loop-free topology. STP places all but one redundant uplink into blocking mode, which places constraints on uplink bandwidth and delays recovery from path failures. In EHM mode, a loop-free topology is maintained by pinning server MAC addresses to one specific uplink. In this manner, all uplinks are actively forwarding traffic.


If desired, the Fabric Interconnects can be set to operate in traditional Ethernet switching mode. This introduces the need for STP to avoid broadcast loops. Despite operating in this manner, not all typical Cisco switch options are available. For this reason, this deployment method is not typically recommended unless required by northbound network topologies.



Ethernet EHM Uplink Pinning: Round-Robin



	Round-robin process used to pin Server MAC uplink; all upstream traffic follows the pinned port




















It is important to understand the difference between IOM pinning and uplink pinning. IOM pinning is static and based on the number of links from the IOM to the fabric interconnect.


Recall from the discussion of EHM that a loop-free topology is assured by pinning MAC addresses to uplink ports. This pinning process can be either automatic or statically configured. By default, server MAC addresses are pinned to uplink interfaces in a round-robin process.



Automatic Uplink Re-Pinning



	Automatic uplink re-pinning occurs upon uplink failure

	Gratuitous ARP sent to core switch




















With automatic uplink pinning, a link failure causes all servers to be repinned to remaining uplinks. When one of the links goes down, the server is simply repinned to the remaining uplink. The fabric interconnect will send a GARP to the northbound switch, on behalf of the servers, to announce them on the new port. The switch updates its MAC forwarding table to reflect the new interface.



Fabric Failover



	If all Fabric A uplinks go down, the IOM fails all host links

	Servers failover to Fabric B and round-robin pinning occurs.




















If all uplink ports on the fabric interconnect lose connectivity, the IOM instructs the I/O multiplexer (MUX) to shut down all eight of the host ports. The impacted servers use either NIC teaming or hardware failover to re-establish connectivity on Fabric B. If the servers are not configured for high availability in the operating system or service profile, they are down until at least one uplink is restored on Fabric A.


You can change this default behavior to allow for local switching even if the uplinks are down by changing the Network Control Policy under the LAN tab. Follow the path: Policies > root > Network Control Policies. If you select Warning, the server interfaces will remain up.



EHM Reverse Path Forwarding Check



Server-to-server traffic in the same VLAN is locally switched:


	Network-to-server unicast traffic is forwarded to the server only if it arrives on the pinned uplink: A Reverse Path Forwarding (RPF) check.

	A déjà vu check will drop server traffic received on any uplink except the pinned border link.

	Unlearned unicast received from core is dropped.

	Broadcast received from core is forwarded to server interfaces.

	Broadcast sent from blade is flooded to pinned uplink and all active server ports; déjà vu check prevents broadcast returned from a different uplink.




















Each server link is pinned to exactly one uplink. Pinning logic load balances blade interfaces to various uplinks:


	Server-to-server traffic is locally switched.

	Server-to-network traffic goes out on pinned uplink.

	Network-to-server unicast traffic is forwarded to the server only if it arrives on the pinned uplink, a Reverse Path Forwarding (RPF) check.

	Server traffic that is received on any uplink, except the pinned border link, is dropped, a déjà vu check.

	Fabric interconnects perform local switching for server-to-server traffic. They learn MAC addresses from inside the chassis only.

	Unlearned unicast messages that are received on an uplink interface are dropped. Unlearned unicast messages that are received on the server interface are forwarded to the pinned port.

	Broadcast messages that are received on the border interface are forwarded to server interfaces. Broadcast messages that are received on the server interface are flooded to the pinned port and all active server ports.




Ethernet MAC Address Learning



Learning disabled on uplinks:


	MAC addresses are pinned to an uplink.



Learning enabled on server links:


	Traffic to server is forwarded based on destination MAC.

	Learned MAC addresses never age unless server link goes down or is deleted.

	Server MAC addresses can move in the event of repinning.




















A key concept in EHM is that a MAC forwarding table, as used in traditional Ethernet switching, is not used to forward traffic to the uplink switch. Instead, a new server MAC address becomes associated with one uplink. All subsequent communications from that MAC address are forwarded to the uplink to which it is pinned. A MAC address forwarding table is maintained only for server-to-server communications on the same VLAN.



EHM with L2 Disjointed Networks


This topic describes support for Layer 2 Disjoint Networks in End-Host Mode in Cisco UCS release 2.0.



EHM with L2 Disjointed Networks



Upstream disjoint layer-2 networks are required when 2 or more separate Ethernet “clouds” must be accessed by servers or VMs:


	Servers or VMs to access a public network and a backup network

	In a multi-tenant system, servers for more than one customer are located in the same UCS instance and need to access the L2 networks for both customers




















Upstream disjoint layer-2 networks (disjoint L2 networks) are required if you have two or more Ethernet “clouds” that never connect, but must be accessed by servers or virtual machines located in the same Cisco UCS instance. For example, you could configure disjoint L2 networks if you require one of the following:


	Servers or virtual machines need to access a public network and a backup network.

	In a multitenant system, servers or virtual machines for more than one customer are located in the same Cisco UCS instance and need to access the L2 networks for both customers.



By default, data traffic in Cisco UCS works on a principle of mutual inclusion. All traffic for all VLANs and upstream networks travels along all uplink ports and port channels. If you have upgraded from a release that does not support upstream disjoint layer-2 networks, you must assign the appropriate uplink interfaces to your VLANs, or traffic for those VLANs continues to flow along all uplink ports and port channels.


The configuration for disjoint L2 networks works on a principle of selective exclusion. Traffic for a VLAN that is designated as part of a disjoint network can only travel along an uplink Ethernet port or port channel that is specifically assigned to that VLAN, and is selectively excluded from all other uplink ports and port channels. However, traffic for VLANs that are not specifically assigned to an uplink Ethernet port or port channel can still travel on all uplink ports or port channels, including those that carry traffic for the disjoint L2 networks.


In Cisco UCS, the VLAN represents the upstream disjoint L2 network. When you design your network topology for disjoint L2 networks, you must assign uplink interfaces to VLANs, not the reverse.



Upstream Disjoint L2 Networks Guidelines



Requires End-Host Mode and UCSM 2.0 or higher


	Overlapping VLANs are not supported; each VLAN only connects to one upstream disjointed L2 domain.

	Server must have a Cisco VIC adapter that supports more than two vNICs.

	Default VLAN 1 cannot be configured explicitly on an uplink.




















Guidelines for Configuring Upstream Disjoint L2 Networks

When you plan your configuration for upstream disjoint L2 networks, consider the following:


	Ethernet Switching Mode Must be End-Host Mode: Cisco UCS only supports disjoint L2 networks when the Ethernet switching mode of the fabric interconnects is configured for end-host mode. You cannot connect to disjoint L2 networks if the Ethernet switching mode of the fabric interconnects is switch mode.

	Symmetrical configuration is Recommended for High Availability: If a Cisco UCS instance is configured for high availability with two fabric interconnects, we recommend that both fabric interconnects are configured with the same set of VLANs.

	VLAN Validity Criteria are the Same for Uplink Ethernet Ports and Port Channels: The VLAN used for the disjoint L2 networks must be configured and assigned to an uplink Ethernet port or uplink Ethernet port channel. If the port or port channel does not include the VLAN, Cisco UCS Manager considers the VLAN invalid and does the following:

			Displays a configuration warning in the Status Details area for the server.

	Ignores the configuration for the port or port channel and drops all traffic for that VLAN.

	The validity criteria are the same for uplink Ethernet ports and uplink Ethernet port channels. Cisco UCS Manager does not differentiate between the two.






	Overlapping VLANs are Not Supported: Cisco UCS does not support overlapping VLANs in disjoint L2 networks. You must ensure that each VLAN only connects to one upstream disjoint L2 domain.



A vNIC can only communicate with one disjoint L2 network. If a server needs to communicate with multiple disjoint L2 networks, you must configure a vNIC for each of those networks. To communicate with more than two disjoint L2 networks, a server must have a Cisco VIC adapter that supports more than two vNICs.


	Appliance Port Must be Configured with the Same VLAN as Uplink Ethernet Port or Port Channel: For an appliance port to communicate with a disjoint L2 network, you must ensure that at least one uplink Ethernet port or port channel is in the same network and is therefore assigned to the same VLANs that are used by the appliance port. If Cisco UCS Manager cannot identify an uplink Ethernet port or port channel that includes all VLANs that carry traffic for an appliance port, the appliance port experiences a pinning failure and goes down.



For example, a Cisco UCS instance includes a global VLAN named vlan500 with an ID of 500. vlan500 is created as a global VLAN on the uplink Ethernet port. However, Cisco UCS Manager does not propagate this VLAN to appliance ports. To configure an appliance port with vlan500, you must create another VLAN named vlan500 with an ID of 500 for the appliance port. You can create this duplicate VLAN in the Appliances node on the LAN tab of the Cisco UCS Manager GUI or the eth-storage scope in the Cisco UCS Manager CLI. If you are prompted to check for VLAN Overlap, accept the overlap and Cisco UCS Manager creates the duplicate VLAN for the appliance port.


	Default VLAN 1 cannot be Configured Explicitly on an Uplink Ethernet Port or Port Channel: Cisco UCS Manager implicitly assigns default VLAN 1 to all uplink ports and port channels. Even if you do not configure any other VLANs, Cisco UCS uses default VLAN 1 to handle data traffic for all uplink ports and port channels. After you configure VLANs in a Cisco UCS instance, default VLAN 1 remains implicitly on all uplink ports and port channels. You cannot explicitly assign default VLAN 1 to an uplink port or port channel, nor can you remove it from an uplink port or port channel. If you attempt to assign default VLAN 1 to a specific port or port channel, Cisco UCS Manager raises an Update Failed fault. Therefore, if you configure a Cisco UCS instance for disjoint L2 networks, do not configure any vNICs with default VLAN 1 unless you want all data traffic for that server to be carried on all uplink ethernet ports and port channels and sent to all upstream networks.

	Configuring Cisco UCS for Upstream Disjoint L2 Networks: When you configure a Cisco UCS instance to connect with upstream disjoint L2 networks, you need to ensure that you complete all of the following steps:




	Configure Ethernet switching mode for both fabric interconnects in Ethernet End-Host Mode.

	Configure the ports and port channels that See Configuring Ports and Port Channels you require to carry the traffic for the disjoint L2 networks.

	(Optional) Configure the LAN pin groups required to pin the traffic for the appropriate uplink Ethernet ports or port channels.

	Create one or more VLANs.

	Assign the desired ports or port channels to the VLANs for the disjoint L2 networks.

	Ensure that the service profiles for all servers that need to communicate with the disjoint L2 networks include the correct LAN connectivity configuration to ensure the vNICs send the traffic to the appropriate VLAN.



		
Before you begin this configuration, ensure that the ports on the fabric interconnects are properly cabled to support your disjoint L2 networks configuration.





Cisco UCS SAN Connectivity


This topic describes Cisco UCS SAN connectivity requirements.



Virtual Storage Area Network (VSAN)



VSANs partition fabric services at the port-level on a single switch or across multiple switches:


	Provide SAN consolidation; lower capital investment and administrative overhead

	VSANs and zones are complementary

	Isolation of fabric services provides fault tolerance; a disruption does not impact entire physical topology

	Cisco supports up to 256 VSANs configured per switch




















Today, many SAN environments consist of numerous islands of connectivity. Commonly deployed SAN islands are physically isolated environments consisting of one or more interconnected switches where each island is typically dedicated to a single or multiple-related applications.


A SAN island might be independently managed by a separate administration team, while strict isolation from faults is achieved through physical network deployment separation. However, because this physical isolation restricts access by other networks and users, the sharing of critical storage assets and the economic savings of storage consolidation are limited.


VSAN functionality is a feature developed by Cisco that leverages the advantages of isolated SAN fabrics with capabilities that address the limitations of isolated SAN islands. VSANs provide a method for allocating ports within a physical fabric to create virtual fabrics. Independent physical SAN islands are virtualized onto a common SAN infrastructure. An analogy is that VSANs on Fibre Channel (FC) networks are like VLANs on Ethernet networks.


Because it is a virtual fabric, separate fabric services are available on each VSAN. The statistics which are gathered on a per-VSAN basis are also available on each VSAN. Each CPU process is common to all VSANs (for example, only one instance of the name server service runs on each switch) but the process uses separate databases for each VSAN.



VSAN Prerequisites
















VSANs are unique per Fabric Interconnect; assigned to FC uplinks:


	FC Port Channels and VSAN trunking are supported on Cisco MDS9000 switches only.

	Default VSAN 1 is preconfigured; not recommended for data traffic.

	Core SAN switch must be enabled for NPIV.







Typically, each Cisco UCS 6100/6200 Fabric Interconnect will have a unique VSAN configuration in accordance with storage industry conventions for maintaining fabric fault-tolerance. As of version 1.4, the Fibre Channel uplinks on the UCS 6100/6200 Fabric Interconnect are capable of carrying traffic for multiple VSANs, support port-channels between Cisco MDS 9000 Family switches, and can be configured for direct-connect fibre-channel storage devices.


By default, VSAN 1 exists on all Cisco UCS 6100 Fabric Interconnects and cannot be deleted. Though VSAN 1 is a fully functional VSAN, Cisco recommends creating separate ‘production VSANs’ on an as needed basis (for data traffic in the data center) and assign the FC uplinks accordingly, even if only one production VSAN is required (typical in most installations). Reliance on VSAN 1 for data traffic limits scalability and flexibility. In the case of the Data Mobility Manager (DMM) feature on the MDS 9000 platform, reliance on VSAN 1 creates a potential for fabric disruption. The DMM feature requires VSAN 1 for certain control functions. This would force an existing site planning to deploy DMM to reconfigure their VSAN environment.


The core Fibre Channel switch must be enabled for N_Port ID Virtualization (NPiV), which is covered later in the courseware.



FCoE VLANs
















	10Gb FCoE link must be configured with FCoE VLAN.

	FCoE VLAN does not trunk to L2 core.







All server Fibre Channel traffic is carried via FCoE in dedicated VLANs across the 10Gb server links. Since Ethernet links cannot be assigned to a VSAN, an FCoE VLAN is defined for each VSAN. All FCoE VLANs must be unique per VSAN and cannot conflict with core VLANs. It is recommended that the SAN and LAN teams design their VLANs accordingly and select an unused range of VLANs dedicated to FCoE traffic to ensure that no overlap occurs with existing core VLANs.


Currently, the FCoE VLAN will exist only between the 5108 chassis and the Cisco UCS 6100 fabric interconnects. Future development will allow multi-hop capability for FCoE traffic, hence, requiring FCoE VLAN trunking with core LAN switches.



Fabric Interconnect―SAN End Host Mode


This topic describes End Host Mode (EHM) and its importance in forwarding over multiple Layer 2 links and maintaining a loop-free topology.



Standard Fibre Channel Fabric Ports



Fibre Channel switch port designations are based on the connecting device:


	E Port: expansion-port link to another switch

	F Port: fabric-port connection to a node device (initiator or target)

	N Port: designating any node device; connects to switch F port

	24-bit FCID represents datalink ID in FC network (analogous to MAC in LAN); assigned to N_Port devices during FLOGI




















An N_Port (Node Port) is a port on a node that connects to a fabric:


	I/O adapters and array controllers contain one or more N_Ports.

	N_Ports can also directly connect two nodes in a point-to-point topology.



An F_Port (Fabric Port) is a port on a switch that connects to an N_Port.


An E_Port (Expansion Port) is a port on a switch that connects to an E_Port on another switch.


An FL_Port (Fabric Loop Port) is a port on a switch that connects to an arbitrated loop:


	Logically, an FL_Port is considered part of both the fabric and the loop.

	FL_Ports are always physically located on the switch.



Although Fibre Channel hubs obviously have physical interfaces, they do not contain Fibre Channel ports. Hubs are basically just passive signal splitters and amplifiers. They do not actively participate in the operation of the network. On an arbitrated loop, the node ports manage all Fibre Channel operations.


Not all switches support FL_Port operation. For example, some McDATA switches do not support FL_Port operation.


An NL_Port (Node Loop Port) is a port on a node that connects to another port in an arbitrated loop topology. There are two types of NL_Ports:


	Private NL_Ports can communicate only with other loop ports.

	Public NL_Ports can communicate with other loop ports and with N_Ports on an attached fabric.



		
The term L_Port (Loop Port) is sometimes used to refer to any port on an arbitrated loop topology. L_Port can mean either FL_Port or NL_Port. In reality, there is no such thing as an L_Port.





Fibre Channel End Host Mode



NPIV technology allows multiple WWPN associations to a single physical N_Port, thus multiple FCIDs associated with it.


NPV-enabled switch has the ability to register multiple FLOGIs to an upstream NPIV-switch via NP_Port link:


	NPV enables the ability to scale in larger SAN deployments.

	NPV-enabled switch appears as end host, requires no domain ID assignment.

	NPV-enabled by default on fabric interconnect




















The Switched Fabric Address Space

Fabric Login (FLOGI) is the function used to assign the N_Port ID, often referred to as the Fibre Channel ID (FCID) to the connecting N Port device. The FCID is a 24-bit FC address consisting of three 8-bit elements:


	The Domain ID is used to define a switch. Each switch receives a unique Domain ID. The ANSI spec designates the allowed range of domain IDs from 1 to 239.

	The Area ID is often used to identify groups of ports within a Domain. Areas can be used to group port ports within a switch, and are also used to uniquely identify fabric-attached arbitrated loops. Each fabric-attached loop receives a unique Area ID.

	The Port ID is used to identify each individual port within an Area. Loop devices use this byte for the AL-PA.



N_Port ID Virtualization (NPIV) 

NPIV is a Fibre Channel facility allowing multiple N_Port IDs to share a single physical N_Port. This allows multiple Fibre Channel initiators to occupy a single physical port, easing hardware requirements in Storage Area Network (SAN) design, especially where virtual SANs are called for. NPIV is defined by the Technical Committee T11 in the Fibre Channel - Link Services (FC-LS) specification.


Normally, a single N_Port will have a single, associated FCID assigned during the FLOGI process. The FCID is not the same as the World Wide Port Name (WWPN). Typically, there is a one-to-one relationship between WWPN and FCID. Hence, for any given physical N_Port, there would be exactly one WWPN associated to one FCID.


NPIV technology allows multiple WWPN associations to a single physical N_Port. Thus, multiple FCIDs can be associated with one physical N Port. After the normal FLOGI process, an NPIV-enabled N_Port can subsequently issue additional commands to register more WWPNs and receive more FCIDs, one for each WWPN. The FC switch must be enable for NPIV, as the F_Port on the other end of the link would “see” multiple WWPNs and multiple FCIDs coming from the host and must know how to handle this behavior.


Once the WWPNs have been registered, each can be used for SAN zoning or LUN presentation.


N_Port Virtualization (NPV)

While NPIV is primarily a host-based solution, NPV is primarily a switch-based technology. Because the NPV-enabled switch appears as an end host, no domain ID assignment is required, which enables the ability to scale in larger SAN deployments. It became a necessity with the introduction of embedded FC switches in blade-server systems, causing enterprise data centers to breech the domain ID limit of their existing storage network.


NPV introduces the node-proxy port type (NP_Port), which acts as a proxy for other N_Ports connected to the NPV-enabled switch. The NPV-enabled switch has the ability to register multiple FLOGIs to an upstream Fibre Channel NPIV-switch via the NP_Port link. The physical N_Ports require no additional drivers or support for this function. The FLOGI exchange (with the NPIV core switch) is handled by the NPV-enabled switch on behalf of the N_Ports connected to it.



Fibre Channel Switching Mode



Use Case: Smaller pod-like deployments:


	UCS behaves like an FC switch; allows direct attached FC and FCoE storage

	Zoning configuration in UCSM:

			Zone merge will occur if connected to MDS9000 switch

	Can only connect to MDS 9000






	Non-MDS connections require default-zoning with LUN Masking on array

	Cisco UCS Fabric Interconnect assigned Domain ID through Principal Switch




















Fibre Channel Switching Mode

The Fibre Channel switching mode determines how the fabric interconnect behaves as a switching device between the servers and storage devices. The fabric interconnect operates in either of the following Fibre Channel switching modes:


	End-Host Mode: End-host mode allows the fabric interconnect to act as an end host to the connected Fibre Channel networks, representing all server (hosts) connected to it through vHBAs. This is achieved by pinning (either dynamically pinned or hard pinned) vHBAs to Fibre Channel uplink ports, which makes the Fibre Channel ports appear as server ports (N-ports) to the rest of the fabric. When in end-host mode, the fabric interconnect avoids loops by denying uplink ports from receiving traffic from one another.



		
End-host mode is synonymous with NPV mode. This is the default Fibre Channel Switching mode. When end-host mode is enabled, if a vHBA is hard pinned to a uplink Fibre Channel port and this uplink port goes down, the system cannot re-pin the vHBA, and the vHBA remains down. 




	Switch Mode: Switch mode is the traditional Fibre Channel switching mode. Switch mode allows the fabric interconnect to connect directly to a storage device. Enabling Fibre Channel switch mode is useful in POD models where there is no SAN (for example, a single Cisco UCS system connected directly to storage), or where a SAN exists with an upstream Cisco MDS 9000 Family switch.



		
In Fibre Channel switch mode, SAN pin groups are irrelevant. Any existing SAN pin groups will be ignored.




Switch mode is not the default Fibre Channel switching mode. Enabling Fibre Channel switching mode requires a license.



FC Uplink Port Pinning



Core switch must be NPIV-enabled to interpret multiple FLOGIs on the same port:


	Two types of interfaces in NPIV topologies:















	Each downstream server is pinned to an uplink port based on a round robin algorithm

	All upstream traffic follows the pinned border interface







The core switch must be enabled for NPIV in order to interpret multiple logins from the same port.


Using NPV, each downstream device (server or blade server) will be pinned to an uplink port based on a round robin algorithm. The NPV mode switch will no longer handle FLOGI login requests or make routing decisions using Fabric Shortest Path First (FSPF). Instead, these operations are passed to the upstream switch, which is known as the NPV core switch.


There are two types of interfaces in NPIV topologies: 


	Server Interfaces: Server-facing interfaces are either physical Fibre Channels, or virtual Fibre Channel interfaces with F_Port modes:

			There is no local switching.

	All packets are forwarded.

	FLOGI-related processing is relayed in software (FLOGI, FDISC, and corresponding LS_ACC, LS_RJT, and so on) to the same uplink interface.

	NPIV is supported on F_Ports.






	Border Interfaces: Border interfaces are network facing and will always be port type NP:

			Internal FLOGI is sent to the core Fibre Channel switches.

	Border interfaces register with name servers on the successful internal FLOGI.

	Every uplink can be connected to different Fibre Channel switches and virtual storage area networks (VSANs).









VSAN-Based Uplink Pinning



	Server interfaces are only pinned to uplink interfaces with matching VSAN.

	If no interface is available with a matching VSAN, the link is kept down.




















Uplink Interfaces and Node Interfaces can only be configured for a single VSAN. Node Interfaces will only be pinned to a port of the correct VSAN.


Configuration in the figure is as follows:


	Two uplink interfaces are configured for VSAN 10 and the other two are configured for VSAN 20.

	Two blade ports are configured for VSAN 10.

	One blade port is configured for VSAN 20.

	One server port is configured for VSAN 30.

	The blades configured for VSAN 10 will be pinned on one of the VSAN 10 BIs.

	The blade configured for VSAN 20 will be pinned to one of the VSAN 20 BIs.

	The blade interface configured for VSAN 30 will be kept down because there is no matching uplink interface.




Static Pinning on Uplink Failure



	Upon uplink failure, the OS multipath driver will detect path loss and IO will failover to alternate path.




















In a static pinning environment, the operating system or hypervisor relies on its multipath I/O driver to detect path failure and discontinue using that path if there is no multipath I/O driver.



Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	The Cisco MDS 9500 Family of Multilayer Directors and fabric switches offers intelligent fabric-switching services that realize maximum performance while ensuring high reliability levels.

	The Cisco Nexus® 5000 Series Switches are part of the unified fabric component of the Cisco® Data Center Business Advantage (DCBA) architectural framework.

	Coupled with the Cisco NX-OS Software, the Cisco Nexus 7000 Series delivers a rich set of features with nonstop operation.

	A virtual port channel (vPC) allows links from the Cisco UCS Fabric Interconnect that are physically connected to two different Cisco Nexus Switches to appear as a single port channel by a third device.

	EHM allows multiple active Layer 2 forwarding links by pinning server MAC addresses.








Lesson 5
UCS Port Configuration

Overview

This lesson covers the Port Configuration on the Cisco UCS Fabric Interconnect.


Objectives

Upon completing this lesson, you will be able to describe Cisco UCS port configuration. You will able to meet these objectives:


	Describe Cisco UCS 6100 and 6200 Port Configuration

	Describe and configure Pin Groups

	Describe the Fabric Interconnect Storage network connectivity




Lesson 5
UCS Port Configuration

Overview

This lesson covers the Port Configuration on the Cisco UCS Fabric Interconnect.


Objectives

Upon completing this lesson, you will be able to describe Cisco UCS port configuration. You will able to meet these objectives:


	Describe Cisco UCS 6100 and 6200 Port Configuration

	Describe and configure Pin Groups

	Describe the Fabric Interconnect Storage network connectivity




UCS 6100 and 6200 Port Configuration


This topic describes the different port types provided on the Cisco UCS 6100 and 6200 Fabric Interconnects, including server and uplink Ethernet port types, appliance and FCoE ports, and FC Storage ports.



Cisco UCS 6200 Unified Ports




















The Cisco UCS 6248 Fabric Interconnect supports unified ports on every interface, the 32 motherboard ports and the 16 expansion card ports. This means that any port supports Ethernet, DCB, FCoE, or FC. The SFP or SFP+ type is chosen accordingly and the port is configured to the correct type by the network admin. This greatly simplifies options and means that over the course of the device’s lifetime, FC could, for example, be migrated to FCoE without a physical switch or cable change on the Fabric Interconnect.



Cisco UCS 6100 Supported Port Types




















Each fabric interconnect has a set of ports in a fixed port module that you can configure as either server ports or uplink Ethernet ports. These ports are not reserved. They cannot be used by a Cisco UCS instance until you configure them. You can add expansion modules to increase the number of uplink ports on the fabric interconnect or to add uplink Fibre Channel ports to the fabric interconnect. You need to create LAN pin groups and SAN pin groups to pin traffic from servers to an uplink port. Each fabric interconnect can include the following types of ports:


	Server Ports: Server ports handle data traffic between the fabric interconnect and the adapter cards on the servers. You can only configure server ports on the fixed port module. Expansion modules do not include server ports. The Server port designation is for interfaces connected to the IOM of any 5108 blade chassis. Server ports are not directly configured; instead, they are dynamically adjusted depending on the Service Profiles deployed to a particular chassis. This has the advantage of reducing broadcast traffic reaching a given chassis–only those VLANs actually in use within the chassis will be allowed. Like server Ethernet connections, all vHBAs are configured through the use of server profiles. No direct configuration of the vHBA is possible or permitted. Each server vHBA can support one VSAN mapping per port; no VSAN tagging of the Fibre Channel frame occurs.

	Uplink Ethernet Ports: Uplink Ethernet ports handle Ethernet traffic between the fabric interconnect and the next layer of the network. These are typically configured for 802.1q VLAN trunking and can be aggregated in a Port Channel using Link Aggregation Connection Protocol (LACP). All network-bound Ethernet traffic is pinned to one of these ports. By default, Ethernet ports are unconfigured. You can configure uplink Ethernet ports on either the fixed module or an expansion module. However, you can configure them to function in the following ways:

			Server

	Uplink

	FCoE

	Appliance






	Uplink Fibre Channel Ports: Uplink Fibre Channel ports handle FCoE traffic between the fabric interconnect and the next layer of the network. All network-bound FCoE traffic is pinned to one of these ports. By default, Fibre Channel ports are uplink. However, you can configure them to function as Fibre Channel storage ports. This is useful in cases where a Cisco UCS requires a connection to a Direct-Attached Storage (DAS) device. You can only configure uplink Fibre Channel ports on an expansion module. The fixed module does not include uplink Fibre Channel ports.



Appliance Port

The appliance port is intended for connecting Ethernet-based storage arrays (such as those serving iSCSI or NFS services) directly to the Fabric Interconnect. In the default EHV deployment mode (Ethernet Host Virtualizer), Cisco UCS selected one Uplink port to accept all broadcast and multicast traffic from the upstream switches. By adding this Appliance port type, you can ensure that any port configured as an Appliance Port will not be selected to receive broadcast/multicast traffic from the Ethernet fabric, as well as providing the ability to configure VLAN support on the port independently of the other Uplink ports.


The Appliance port type reduces the hop count and latency for communication between server and IP storage. Especially within the SMB market, many customers may not have existing 10G Ethernet infrastructures outside of Cisco UCS, or FC switches to connect storage to. For these customers, Cisco UCS could often provide a “data center in a box”, with the exception of storage connectivity. For Ethernet-based storage, all storage arrays had to be connected to some external Ethernet switch, while FC arrays had to be connected to a FC switch. Adding a 10G Ethernet or FC switch just for a few ports didn’t make a lot of financial sense, especially if those customers didn’t have any additional need for those devices beyond Cisco UCS.


FCoE Storage Port

The FCoE storage port provides functionality similar to the Appliance Port type, while extending FCoE protocol support beyond the Fabric Interconnect. It is not intended for an FCoE connection to another FCF (FCoE Forwarder) such as the Cisco Nexus 5000. Only a directly connected FCoE storage device (such as those produced by NetApp and EMC) are supported. When an Ethernet port is configured as an FCoE Storage Port, traffic is expected to arrive without a VLAN tag. The Ethernet headers will be stripped away and a VSAN tag will be added to the FC frame. As with FC port configuration, only one VSAN is supported per FCoE Storage Port. These ports are analogous to an Ethernet “access” port. The traffic is expected to arrive un-tagged, and the switching device (in this case, the Fabric Interconnect) will tag the frames with a VSAN. When the frames are eventually delivered to the destination (typically the CNA on the blade), the VSAN tag will be removed before delivery. Even though both the sending and receiving devices are expecting un-tagged traffic, it’s still tagged internally within the switch while in transit.


Storage FC Port

The storage FC port allows for the direct attachment of a Fibre Channel storage device to one of the native FC ports on the Fabric Interconnect expansion modules. Like the FCoE Storage Port type, the FC frames arriving on these ports are expected to be un-tagged. Each Storage FC Port is assigned a VSAN number to keep the traffic separated within the Cisco UCS Unified Fabric. When used in this way, the Fabric Interconnect is not providing any FC zoning configuration capabilities. All devices within a particular VSAN will be allowed, at least at the FC switching layer (FC2), to communicate with each other. The expectation is that the devices themselves, through techniques, such as LUN Masking and so on, will provide the access control. This is acceptable for small implementations, but does not scale well for larger or more enterprise-like configurations. In those situations, an external FC switch should be used either for connectivity or to provide zoning information, the so-called “hybrid model”.



Configuring Fixed Ethernet Ports




















All fixed ports are FCoE-capable and support full line-rate 10 Gigabit Ethernet. None of these ports are enabled or configured by default. You can only configure Server ports on the fixed port module. Expansion modules do not include Server ports.


On the Cisco UCS 6100 Series Fab, only Ports 1/1 through 1/8 on the Cisco UCS 6120 Fabric Interconnect and 1/1 to 1/16 on the Cisco UCS 6140 Fabric Interconnect can be configured to use 1-Gb/s transceivers (GLC-T, GLC-SX-MM, and GLC-LH-SM). Autonegotiation is not supported for this change, and you will need to set the speed manually in the Cisco UCS Manager. All member ports of a port channel must use these ports at the same speed. If one member port is using a 1-Gb/s port, then the speed of all other members must match.


Ethernet ports on the fabric interconnect can be in one of three states: unconfigured, server, or uplink. By default, all Ethernet ports on the fabric interconnect are initially unconfigured. A state of either server or uplink must be configured before the port can pass traffic.


On the Cisco UCS 6200 Series Fabric Interconnects, all ports can be either Ethernet or Fibre Channel. Ports can operate in either 1 or 10 Gigabit Ethernet, Data Center Bridging (DCB), FCoE, or 1-, 2-, 4-, or 8-Gigabit Fibre Channel.



Configuring FC Expansion Module Ports



FC Expansion module port options include (6100 only):


	Uplink Port

	FC Storage Port

	6-port FC Module ports are 1/2/4/8 Gbps

	8-port FC Module and Combination module FC ports are 1/2/4 Gbps




















The expansion modules provide connectivity into your enterprise Ethernet LAN and Fibre Channel SAN networks. There are three expansion modules from which to choose:


	Fibre Channel-only

	Combination Fibre Channel and Ethernet

	Ethernet-only



The two Fibre Channel-Only expansion modules are as follows:


	8-port supporting 1-, 2-, and 4-Gbps Fibre Channel

	6-port supporting 1-, 2-, 4-, and 8-Gbps Fibre Channel



The combination expansion module contains four SFP+ ports that support 10-GE and four SFP ports that support 1-, 2-, and 4-Gbps Fibre Channel. The Ethernet-Only expansion module contains six SFP+ ports that support 10-GE.


		
Expansion Module ports are for uplinks only and cannot be connected to a chassis.





Cisco UCS 6200 Unified Port Panel in UCSM



	Only even number of ports can be configured as Fibre Channel




















Cisco unified port technology allows ports to be dynamically allocated to support Fibre Channel, iSCSI or FCoE data, or lossless Ethernet, to provide configuration flexibility. With unified ports, you do not need to predetermine the number of physical ports required for convergence. This removes the need for guesswork regarding the selection of port types and ratios, and simplifies purchasing decisions.


Unified port technology provides variable connectivity options and flexible choice for customer-paced network convergence. With unified ports, the customer can shift protocol support to provide service based on dynamic demand and bandwidth requirements. On the 6200UP fabric interconnect, all ports can be either Ethernet or Fibre Channel, and operating in either 1 or 10 Gigabit Ethernet, DCB/FCoE, or 1/2/4/8 Gigabit Fibre Channel.


With the Cisco UCS 6200UP Fabric Interconnect, any port can be a 1 or 10 Gigabit Ethernet port, DCB/FCoE, or a 1, 2, 4, or 8 Gb/s Fibre Channel port. However, the ports must be grouped together.


With the built-in ports, any Fibre Channel ports must occupy the highest port numbers, and Ethernet must occupy the lowest port numbers. This is true when adding the 16-port expansion module (Cisco UCS-FI-E16UP) as well. The ports must be allocated in even blocks, so the boundary between Ethernet and Fibre Channel must be on an even port number. Any changes in Ethernet and Fibre Channel port allocations require a reboot.


Ports on the base card or the Cisco Unified Port GEM Module can either be Ethernet or Fibre Channel:


	Only a continuous set of ports can be configured as Ethernet or Fibre Channel.

	Ethernet ports must be the first set of ports.

	Port type changes take effect after the next reboot of the switch for Base card ports or after power-off and power-on of the GEM for GEM unified ports.

	GUI allows only even numbers of Ethernet and Fibre Channel ports.

	CLI allows odd number of Ethernet or Fibre Channel port.



The Cisco UCS Manager version 2.0 software provides a slider mechanism for defining the mix of Fibre Channel and Ethernet ports. Note that any change to the configuration requires a reboot of the fabric interconnect. Using the GUI, only even port numbers can be configured.



Pin Groups


This section describes the Cisco UCS 6100/6200 Fabric Interconnect configured for storage network connectivity.



Pin Groups Prior to Cisco UCS Release 2.0



Use Case: uplinks to different Layer 2 Domains:















	Pin Groups tie traffic from a specific network adapter to specific uplink(s) using the Service Profile.

	Cisco UCS automatically chooses uplink for Service Profiles not configured with a Pin Group.

	Use Switch Mode for separate Layer 2 Domains unless Rel. 2.0 or later used.







Pin Groups are a mechanism for pinning traffic from a particular blade server adapter (through the specification in its Service Profile) to a particular individual port or Port Channel on each switch.


Without Pin Groups (or even if you create them but do not use them in a Service Profile), the Cisco UCS automatically chooses an uplink port or Port Channel for the adapter on the node associated with each profile. If your environment uses some concept of a “correct” uplink for a particular node, then you must create Pin Groups and tie them to your Service Profiles.



UCS Port Channels


This section describes the Cisco UCS 6100/6200 Fabric Interconnect configured for storage network connectivity.



Creating Port Channels




















Port channels (sometimes referred to as EtherChannels) allow multiple links to be combined into an aggregation channel. This combination increases the available bandwidth of the uplink, and prevents a link from becoming a single point of failure.


The fabric interconnect offers the standards-based LACP to negotiate the link with its peer switch. Both ends of all links must be configured correctly.


To create a port channel for uplinks, do the following:


	In the Navigation panel, click the LAN tab.

	On the LAN tab, expand LAN > LAN Cloud.

	Expand the node for the fabric interconnect where you want to add the port channel.

	Right-click the Port Channels node and choose Create Port Channel.



In this example, a port channel has been created with ID 1. Particular uplink ports (19 and 20) are chosen on a given Cisco UCS Fabric Interconnect. A matching configuration must be configured on the uplink switch. Note that this wizard shows all ports on the Fabric Interconnect regardless of current usage or configuration. A warning is displayed before reconfiguration.


Normally, all links in a port channel must terminate on the same switch. Cisco Nexus 7000 Series Switches support a feature called virtual PortChannel (vPC). vPC supports bandwidth aggregation, similar to a traditional port channel, but adds the benefit of a redundant path without the need to run Spanning Tree Protocol (STP). Spanning Tree recovery on path failure can be tuned as low as six seconds, but a port channel can recover in less than one second.


All vPC configuration is performed on the Cisco Nexus 7000 or 5000 uplink switch. The fabric interconnect is unaware that the port channel is split between two switches.



Fabric Port Channel Mode



Interfaces from fabric interconnect to the IOMs can be configured as port channels; requires Cisco UCS Release 2.0 or later:


	Created during chassis discovery according to chassis discovery policy

	Created after chassis discovery according to chassis connectivity policy




















The connectivity options from the Fabric Interconnects to the IOMs in the chassis focusing on all Gen2 hardware allow two modes of operation for the IOM: Discrete mode and Port-Channel mode. Depending upon which IO module is employed, it is possible to configure 1, 2 , 4, or 8 uplinks from each IOM in either Discrete mode (non-bundled) or port-channel mode (bundled.)


In Port-Channel mode, all available links are aggregated and a port-channel hashing algorithm (TCP/UDP + Port VLAN, non-configurable) is used for load-balancing server traffic. In this mode all server links are pinned to the logical bundle rather than individual IOM uplinks.


In this scenario, when a port fails on an IOM port-channel, the load-balancing algorithms handle failing the server traffic flow to another available port in the channel. This failover will typically be faster than NIC-teaming/bonding failover. This will decrease the potential throughput for all flows on the side with a failure, but will only effect performance if the links are saturated.


Fabric port channels are supported only on the Cisco UCS 6200 Series. It can be enabled when the Cisco UCS 6200 Series is connected to the Cisco UCS 2204XP or 2208XP IOM. When connecting to the Cisco UCS 2104XP IOM, the system will automatically fallback to discrete mode, even with the chassis discovery policy set to Port Channel (PO). When the Cisco UCS 2208XP IOM is connected to the Cisco UCS 6100 Series, it can take advantage of all eight ports in discrete mode.



Configuring a Fabric Port Channel






	Step 1
	

	
To include all links from IOM to fabric interconnect in a fabric port channel during chassis discovery, set the link grouping preference in the Chassis Discovery Policy to port channel.







	Step 2
	

	
To include links from individual chassis in a fabric port channel during chassis discovery, set the link grouping preference in the Chassis Connectivity Policy to port channel.







	Step 3
	

	
After chassis discovery, enable or disable additional fabric port channel member ports.
























	Re-acknowledge the chassis to add or remove chassis links from a fabric port channel after making a change to chassis discovery policy or chassis connectivity policy.







Configuring a Fabric Port Channel




	Step 1
	

	
To include all links from the IOM to the fabric interconnect in a fabric port channel during chassis discovery, set the link grouping preference in the Chassis Discovery Policy to port channel.







	Step 2
	

	
To include links from individual chassis in a fabric port channel during chassis discovery, set the link grouping preference in the Chassis Connectivity Policy to port channel.







	Step 3
	

	
After chassis discovery, enable or disable additional fabric port channel member ports.










What to Do Next

To add or remove chassis links from a fabric port channel after making a change to the chassis discovery policy or the chassis connectivity policy, re-acknowledge the chassis. Chassis re-acknowledgement is not required to enable or disable chassis member ports from a fabric port channel.


Configuring the Chassis Discovery Policy




	Step 1
	

	
In the Navigation pane, click the Equipment tab.







	Step 2
	

	
On the Equipment tab, click the Equipment node.







	Step 3
	

	
In the Work pane, click the Policies tab.







	Step 4
	

	
Click the Global Policies subtab.







	Step 5
	

	
In the Chassis Discovery Policy area, complete the following fields:

	Action field: Specifies the minimum threshold for the number of links between the chassis and the fabric interconnect. This can be one of the following:

			1-link

	2-link

	4-link

	8-link

	Platform Max






	Link Grouping Preference field: Specifies whether the links from the IOMs to the fabric interconnects are grouped in a port channel. This can be one of the following:

			None: No links are grouped in a port channel

	Port Channel: All links from an IOM to a fabric interconnect are grouped in a port channel

















Configuring the Chassis Connectivity Policy

The chassis connectivity policy determines the whether a specific chassis is included in a fabric port channel after chassis discovery. This policy is helpful for users who want to configure one or more chassis differently from what is specified in the global chassis discovery policy. The chassis connectivity policy also allows for different connectivity modes per fabric interconnect, further expanding the level of control offered with regard to chassis connectivity.


By default, the chassis connectivity policy is set to global. This means that connectivity control is configured when the chassis is newly discovered, using the settings configured in the chassis discovery policy. Once the chassis is discovered, the chassis connectivity policy controls whether the connectivity control is set to none or port channel.


The chassis connectivity policy is created by Cisco UCS Manager only when the hardware configuration supports fabric port channels. At this time, only the 6200 series fabric interconnects and the 2200 series IOMs support this feature. For all other hardware combinations, Cisco UCS Manager does not create a chassis connectivity policy:





	Step 1
	

	
In the Navigation pane, click the Equipment tab.






	Step 2
	

	
On the Equipment tab, expand Equipment > Chassis.







	Step 3
	

	
Click the chassis for which you want to configure the connectivity between the IOMs and fabric interconnects.






	Step 4
	

	
In the Work pane, click the Connectivity Policy tab.






	Step 5
	

	
For each IOM in the chassis, choose one of the following values in the Admin State field for the chassis and fabric connectivity:

	None: No links are grouped in a port channel

	Port Channel: All links from an IOM to a fabric interconnect are grouped in a port channel.

	Global: The chassis inherits this configuration from the chassis discovery policy. This is the default value.








	Step 6
	

	
Click Save Changes.










Changing the connectivity mode for a chassis could result in decreased VIF namespace. Changing the connectivity mode for a chassis results in chassis re-acknowledgement. Traffic may be disrupted during this time.



Hardware Port Channel on Cisco UCS 1280VIC



Requires 6200 Fabric Interconnects


	4x10 Gbps Ether channel from VIC1280 to UCS 2208 IOM

	vNIC flows are 7-tuple Load Balanced across links

	Each individual flow limited to 10Gb

	Fabric failover available

	No user configuration required




















One of the most beneficial features of the next generation Cisco UCS hardware is the ability to port channel both 2208 IOM chassis fabric uplinks and the server adapter port uplinks on the 1280 VIC. The major benefit of this feature is better distribution of server traffic load across more links, across both the server ports and the fabric uplink ports.


Cisco UCS VIC 1280 creates fully functional unique and independent PCIe adapters and interfaces (NICs or HBAs) without requiring single-root I/O virtualization (SR-IOV) support from OSs or hypervisors:


	Allows these virtual interfaces and adapters to be configured and operated independently, just like physical interfaces and adapters

	Creates a highly flexible I/O environment needing only one card for all I/O configurations

	Unifies virtual and physical networking into a single infrastructure

	Provides virtual machine visibility from the physical network and a consistent network operations model for physical and virtual servers

	Enables configurations and policies to follow the virtual machine during virtual machine migration

	Provides a pre-standard implementation of the IEEE 802.1Qbh Port Extender standard

	Centralized management Enables the mezzanine card to be centrally managed and configured by Cisco UCS Manager

	Provides a redundant path to the fabric interconnect using hardware-based fabric failover

	Supports customer requirements for Microsoft Windows Server 2008, Red Hat Enterprise Linux 5.4, and VMware vSphere 4 Update 1



The 1280 VIC utilizes 4x10Gbps links across the mid-pane per IOM to form two 40Gbps port-channels. This provides for 80Gbps total potential throughput per card. This means a half-width blade has a total potential of 80Gbps using this card and a full-width blade can receive. As with any port-channel, link-aggregation can only utilize the max of one physical link (or back plane trace) of bandwidth. This means every flow from any given Cisco UCS server has a max potential bandwidth of 10Gbps, but with 8 total uplinks 8 different flows could potentially utilize 80Gbps.


This becomes very important with NFS-based storage within hypervisors. Typically, a hypervisor will handle storage connectivity for all VMs. This means that only one flow (conversation) will occur between host and storage. In these typical configurations, only 10Gbps will be available for all VM NFS data traffic, even though the host may have a potential 80Gbps bandwidth. This is not necessarily a concern, but a design consideration as most current/near-future hosts will never use more than 10Gbps of storage I/O.



Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	10-Gigabit Ethernet ports on the Cisco UCS fabric interconnect are un-configured by default, and can be set as server links or uplinks.

	Cisco UCS uses only standards-based LACP to negotiate port channels.

	End-host mode allows forwarding over multiple Layer 2 links while maintaining a loop free topology and does not require spanning-tree blocking.

	Server MAC addresses are statically pinned to the IOM links.

	Automatic uplink pinning allows dynamic recovery from uplink loss without a fabric failover. Manual uplink pinning is more flexible. Fabric failover may be required based on uplink failure.

	UCS connects to Cisco MDS or any other Fibre Channel switch with NPIV support; usually NPV mode is used on the Fabric Interconnects.

	Node Interfaces will only be pinned to a port of the correct uplink port based on VSAN.








Lesson 6
Creating Service Profiles

Overview

In this Lesson, you will learn how to create Service Profiles.


Objectives

Upon completing this lesson, you will be able to describe how to create Service Profiles. You will able to meet these objectives:


	Describe how to compare the simple to the expert Service Profile Wizards

	Describe how to use the simple Service Profile wizard to create a Service Profile

	Describe how to use the expert wizard to create a Service Profile




Lesson 6
Creating Service Profiles

Overview

In this Lesson, you will learn how to create Service Profiles.


Objectives

Upon completing this lesson, you will be able to describe how to create Service Profiles. You will able to meet these objectives:


	Describe how to compare the simple to the expert Service Profile Wizards

	Describe how to use the simple Service Profile wizard to create a Service Profile

	Describe how to use the expert wizard to create a Service Profile




Simple versus Expert Service Profile Wizards


This topic compares the simple to the expert Service Profile Wizards.



Service Profile Wizards



Cisco UCS supports two Service Profile creation wizards:


	Basic Wizard resembles traditional server deployments:

			Hardware default values used for server identifiers

	Service Profiles can be moved, but identity changes






	Expert Wizard leverages identity virtualization:

			Uses pools of identifiers to make profile mobile

	Frees administrators to handle strategic concerns instead of tactical












The primary difference between the simple service profile wizard and the expert service profile wizard is the scope of tools available to manipulate within the wizard:


	The simple wizard provides a fast, single-page form for the rapid provisioning of a blade server using all derived identity values.

	The basic wizard is a single-page form that allows the creation of a Service Profile using all derived values.

	Service Profiles created with the basic wizard do not support stateless computing and have limited options.

	The expert Service Profile wizard provides the administrator with a rich set of options for identity and policy assignment.

	The expert wizard allows the creation of mobile Service Profiles that can be moved from compute node to compute node without the need to modify parameters in the operating system or applications.




Comparison of Service Profile Wizards



	Basic Wizard
	Expert Wizard

	Assign single-access VLAN
	Assign access VLAN or trunk

	Use derived MAC address only
	Assign locally administered MAC address or use pool

	Use derived WWNN only
	Assign locally administered WWNN or use pool

	Use derived WWPN only
	Assign locally administered WWPN or use pool

	Use derived UUID only
	Assign locally administered UUID or use pool

	Only two devices in the boot order
	More than two devices in the boot order

	No policy or threshold assignment
	Policy and threshold assignment for each element in the service profile







The table summarizes the most important differences between the simple and expert Service Profile wizards.



Simple Service Profile Wizard


This topic describes how to use the wizard to create a Service Profile.



Simple Service Profile Wizard




















The wizard for creating Service Profiles works well for the basic opt-in model. The wizard will produce a Service Profile with basic characteristics and hardware-derived identifiers. If you want to customize the Service Profile after the wizard is finished (for example, to add more vNICs, or attach vNICs to different or multiple VLANs, and so on), you can still do so.



Simple Service Profile Wizard (Cont.)




















This figure shows the options for vNIC and vHBA connectivity available in the wizard. Note that only one VLAN can be chosen per vNIC, and no capability for 802.1q VLAN trunking is provided. These configuration options can be changed later after the wizard has finished. Also, note that vHBA configuration does not allow the selection of a VSAN. The default VSAN object is automatically chosen.


This figure shows the configuration options for boot order in the wizard. Up to two boot devices can be specified.



Service Profile Server Association
















	A physical compute node can be selected at time of profile creation (shown), or manually later.

	If selected, Service Profile will automatically be associated with selected compute node.

	Association via Server Pools is recommended.







The wizard allows association with a specific compute node, if desired. Alternately, the profile may be manually associated with a compute node later.



Impact Analysis of Configuration Changes




















Impact Analysis of Configuration Changes

A simple “pre-flight check” compatibility check allows administrators to recognize potential compatibility issues before applying a Service Profile to a particular server.


In the diagram, the impact analysis detects the presence of the VIC as the mezzanine adapter, which does not advertise a MAC address. The simple Service Profile is designed to derive the MAC from the adapter; it provides no option to specify a MAC address during profile creation. This association will fail as a result. The administrator may edit the vNIC MAC and reassociate the profile.



Service Profile Expert Wizard


This topic describes how to use the expert wizard to create a Service Profile.



Service Profile Wizard (Expert)



Expert wizard provides the most flexibility in configuring a Service Profile















Configure UUID:


	Manually

	From Domain Pool

	Hardware default (derived)



An optional description can be added to the Service Profile (recommended)






The expert wizard allows a greater degree of customization of the Service Profile at the time of creation. Profiles created using either wizard can be customized after creation in the same manner.


The first dialog defines the Profile name and specifies the UUID assignment. The UUID can be configured manually or chosen from a pool (shown), or the hardware default (derived) value can be used.



vNICs and MAC Address




















vNICs are added in the same manner as vHBAs. Click the Add button to create a vNIC.


The figure shows the configuration options when creating a vNIC. Select the MAC address pool to use, or assign one manually. This example shows the creation of a vNIC that will receive only one VLAN as a Native or non-tagged VLAN.


Select the MAC address pool to use (shown), or assign one manually. This example shows the creation of a vNIC that will receive an 802.1q VLAN trunk. In this example, two VLANs will be trunked to this vNIC, with one of them untagged as the Native VLAN.



Local Storage and HBA Configuration




















Local Storage Policies determine how the local disks on the compute node should be configured. The policy can specify that the local disks not be used, and RAID-0 Striping, or RAID-1 Mirroring.


The Scrub Policy defines what actions should be taken when this Service Profile is disassociated from a compute node, such as erasing the local disks or BIOS settings.


This figure also shows the expert options for configuring the WWNN for the Service Profile. The simple options are the same as the simplified wizard shown earlier:


	The WWN can be assigned manually or from a pool (shown)

	Simple configuration is the same as the simplified wizard



In the expert wizard, no vHBAs are created by default. Use the Add button to add vHBAs, as necessary.


As many vHBAs can be created as desired. The Service Profile can only be associated with physical hardware that can support the number of vHBAs:


	No vHBAs supported on Oplin-based mezzanine cards

	2 vHBAs supported per Menlo-based mezzanine card

	Larger numbers of vHBAs require virtual interface card



This figure shows the configuration options available when creating a vHBA in the expert wizard. The WWPN can be selected automatically from a pool (shown), or assigned manually. Each vHBA is associated with one VSAN and one Fabric Interconnect.



Zoning
















	Zoning available in UCS Release 2.1

	Zoning is not applied in FC End-Host Mode







The Zoning dialog allows configuring the virtual HBAs in a zone database. The fabric-interconnects must be rendered in FC switch-mode for zoning to be applied.


	Click Add to create and name a Zone.

	Select the newly created zone and move the desired HBA into the zone.




Dual Mezzanine vNIC Placement
















	vNIC placement determines which Mezzanine is used for each vNIC/vHBA in Full-width blades with two mezzanine cards or M3 blades with LAN on Motherboard (LOM).

	vNIC placement can be automatic or manual.







The Cisco UCS B250 and B440 full-slot blade servers include two slots for mezzanine cards. Because a vNIC is a virtual definition of a network interface card, it could be placed on the appropriate fabric on either of the mezzanine cards present in the full-slot server.


In a half-slot blade with a single mezzanine card, simply allow the system to select the only mezzanine card. If manual control is desired, select Specify Manually from the Select Placement drop-down list. vCon1 maps vNICs to the left mezzanine slot, and vCon2 maps a vNIC to the right mezzanine slot (as viewed from the front panel of the blade server). Click Next to continue in the wizard.



Boot Order




















Boot Policy

The boot policy determines the following:


	Configuration of the boot device

	Location from which the server boots

	Order in which boot devices are invoked



You must include this policy in a service profile, and that service profile must be associated with a server for it to take effect. If you do not include a boot policy in a service profile, the server uses the default settings in the BIOS to determine the boot order.


Changes to a boot policy may be propagated to all servers created with an updating service profile template that includes that boot policy. Reassociation of the service profile with the server to rewrite the boot order information in the BIOS is auto-triggered.


Creating a Boot Policy

You can also create a local boot policy that is restricted to a service profile or service profile template. However, except for iSCSI boot, we recommend that you create a global boot policy that can be included in multiple service profiles or service profile templates. Configure one or more of the following boot options for the boot policy and set their boot order:


	SAN Boot: To boot from an operating system image on the SAN, continue with Configuring a SAN Boot for a Boot Policy.



You can specify a primary and a secondary SAN boot. If the primary boot fails, the server attempts to boot from the secondary.


	iSCSI Boot: To boot from an iSCSI LUN, continue with Creating an iSCSI Boot Policy. Note that iSCSI Boot requires Cisco UCS Release 2.0 or above.

	LAN Boot: To boot from a centralized provisioning server, continue with Configuring a LAN Boot for a Boot Policy.

	Local Disk boot: To boot from the local disk on the server, continue with Configuring a Local Disk Boot for a Boot Policy.

	Virtual Media Boot: To boot from virtual media that mimics the insertion of a physical CD or floppy drive into a server, continue with Configuring a Virtual Media Boot for a Boot Policy.



What to Do Next

Include the boot policy in a service profile and/or template. After a server is associated with a service profile that includes this boot policy, you can verify the actual boot order in the Boot Order Details area on the General tab for the server.


The figure shows the options for specifying the boot order for the compute node. Local disk, or virtual CD-ROM and floppy drives (discussed later in this module) are shown here. Booting from SAN is also supported and shown later in this lesson.


To configure a compute node to boot from SAN, first add the vHBA to the boot order (as shown previously) and then click the Add SAN Boot Target link. In the following dialog box, provide the boot target WWN and LUN number.



Boot Order: iSCSI Boot



	Configure server to boot from an iSCSI target

	Supported UCS hardware:

			UCS M51KR-B Broadcom BCM57711 network adapter:

			Use default MAC






	UCS M81KR VIC

	UCS VIC1280

	UCS VIC 1240








	UCSM iSCSI vNIC and iSCSI boot information posts an iSCSI Boot Firmware Table (iBFT) to host memory and a valid bootable LUN to system BIOS

	iBFT contains initiator and target configuration programmed on primary iSCSI VNIC




















As of Cisco UCS Release 2.0, iSCSI boot enables a server to boot its operating system from an iSCSI target machine located remotely over a network.


iSCSI boot is supported on the following Cisco UCS hardware:


	Cisco UCS server blades that have the Cisco UCS M51KR-B Broadcom BCM57711 network adapter and use the default MAC address provided by Broadcom

	Cisco UCS M81KR Virtual Interface Card



iSCSI Boot Process

The Cisco UCS Manager iSCSI vNIC and iSCSI boot information created for the service profile is used in the association process to program the mezzanine adapter, located on the blade server. After the adapter is programmed, the blade server reboots with the latest service profile values. After the power on self-test (POST), the adapter attempts to initialize using these service profile values. If the adapter can use the values and log into its specified target, the adapter initializes and posts an iSCSI Boot Firmware Table (iBFT) to the host memory and a valid bootable LUN to the system BIOS. The iBFT that is posted to the host memory contains the initiator and target configuration that is programmed on the primary iSCSI VNIC.


		
The iBFT only uses the first iSCSI vNIC and only Target 1 for the initiator-to-target initialization. This scenario is true even if a second target (Target 2) exists for the first iSCSI vNIC.




The next step, which is the installation of the operating system (OS), requires an OS that is iBFT capable. During installation of the OS, the OS installer scans the host memory for the iBFT table and uses the information in the iBFT to discover the boot device and create an iSCSI path to the target LUN. In some OS's a NIC driver is required to complete this path. If this step is successful, the OS installer finds the iSCSI target LUN on which to install the OS.


		
The iBFT works at the OS installation software level and might not work with HBA mode (also known as TCP offload). Whether iBFT works with HBA mode depends on the OS capabilities during installation. Also, the iBFT normally works at a maximum transmission unit (MTU) size of 1500, regardless of the MTU jumbo configuration. If the OS supports HBA mode, you might need to set HBA mode (also known as TCP offload), dual-fabric support, and jumbo MTU size after the iSCSI installation process.




iSCSI Boot Prerequisites

These prerequisites must be met before configuring iSCSI boot:


	To set up iSCSI boot from a Windows 2008 server where the second vNIC (failover vNIC) must boot from an iSCSI LUN, consult the Microsoft Knowledge Base Article 976045. Microsoft has a known issue where Windows might fail to boot from an iSCSI drive or cause a bugcheck error if the networking hardware is changed. To work around this issue, follow the resolution recommended by Microsoft.

	The storage array must be licensed for iSCSI boot and the array side LUN masking must be properly configured.

	Two IP addresses must be determined, one for each iSCSI initiator. If possible, the IP addresses should be on the same subnet as the storage array. The IP addresses are assigned statically or dynamically using the Dynamic Host Configuration Protocol (DHCP).

	You cannot configure boot parameters in the Global boot policy. Instead, after configuring boot parameters, you need to include the boot policy in the appropriate service profile.

	The operating system (OS) must be iSCSI Boot Firmware Table (iBFT) compatible.

	For Cisco UCS M51KR-B Broadcom BCM57711 network adapters:

			Blades that use iSCSI boot must contain the Cisco UCS M51KR-B Broadcom BCM57711 network adapter.

	Set the MAC addresses on the iSCSI device.

	If you are using the DHCP Vendor ID, the MAC address of an iSCSI device needs to be configured in /etc/dhcpd.conf.

	HBA mode (also known as TCP offload) and the boot-to-target setting are supported. However, only Windows OS supports HBA mode during installation.

	Before installing the OS, disable the boot-to-target setting in the iSCSI adapter policy, then after installing the OS; reenable the boot-to-target setting.

	When installing the OS on the iSCSI target, the iSCSI target must be ordered before the device where the OS image resides. For example, if you are installing the OS on the iSCSI target from a CD, the boot order should be the iSCSI target and then the CD.

	After the server has been iSCSI booted, do not modify the Initiator Name, Target name, LUN, iSCSI device IP, or Netmask/gateway using the Broadcom tool.

	Do not interrupt the POST (power on self-test) process or the Cisco UCS M51KR-B Broadcom BCM57711 network adapter will fail to initialize.






	For Cisco UCS M81KR Virtual Interface Card:

			Do not set MAC addresses on the iSCSI device.

	HBA mode and the boot to target setting are not supported.

	When installing the OS on the iSCSI target, the iSCSI target must be ordered after the device where the OS image resides. For example, if you are installing the OS on the iSCSI target from a CD, the boot order should be the CD and then the iSCSI target.

	If you are using the DHCP Vendor ID (Option 43), the MAC address of the overlay vNIC needs to be configured in /etc/dhcpd.conf.

	After the server has been iSCSI booted, do not modify the IP details of the overlay vNIC.








Creating an iSCSI vNIC for a Service Profile:





	Step 1
	

	
In the Navigation pane, click the Servers tab.






	Step 2
	

	
On the Servers tab, expand Servers > Service Profiles.






	Step 3
	

	
Expand the node for the organization that contains the service profile for which you want to create an iSCSI vNIC.






	Step 4
	

	
Expand the service profile for which you want to create an iSCSI vNIC.






	Step 5
	

	
Right-click the iSCSI vNICs node and choose Create vNICs.






	Step 6
	

	
In the Create iSCSI vNIC dialog box, complete the following fields: Name, Overlay vNIC, iSCSI Adapter Policy, MAC Address (or MAC Pool), VLAN.










Service Profile Maintenance Policy




















To understand the impact, first look at the way disruptive changes were handled prior to this release. When changing a configuration setting on a service profile, updating service profile template, or many policies, if the change would cause a disruption to running service profiles (that is, requiring a reboot), you had two options : yes, or no. When modifying a single profile, this wasn’t a big issue. You could simply change the configuration when you were also ready to accommodate a reboot of that particular profile. Where it became troublesome was when you wanted to modify an updating service profile or policy that affected many service profiles. Your choice was really only to reboot them all simultaneously or modify each individually. Obviously for large deployments using templates and policies (the real strength of Cisco UCS), this wasn’t ideal.


With Cisco UCSM 1.4, we now have the concept of a Maintenance Policy. Creating a Maintenance Policy allows the administrator to define the manner in which a service profile (or template) should behave when disruptive changes are applied.


A policy of “Immediate” means that when a disruptive change is made, the affected service profiles are immediately rebooted without confirmation. A normal “soft” reboot occurs, whereby a standard ACPI power-button press is sent to the physical compute node. If the operating system traps for this, the OS should gracefully shut down and the node will reboot.


A much safer option is to use the “user-ack” policy option. When this option is selected, disruptive changes are staged to each affected service profile, but the profile is not immediately rebooted. Instead, each profile will show the pending changes in its status field, and will wait for the administrator to manually acknowledge the changes when it is acceptable to reboot the node.


The most interesting new option is the “timer-automatic” setting. This setting allows the maintenance policy to reference another new object, the Schedule.



Server Assignment




















The Service Profile can optionally be associated with a compute node or server pool. Additionally, a qualification policy can be selected to further refine the compute node selected.



Service Profile Operational Policies




















Operational Policies define various functions such data collection, power control, disk scrubbing, or other tasks based on specified criteria.


Examples include the following:


Fault Collection Policy

The fault collection policy controls the lifecycle of a fault in a Cisco UCS instance, including when faults are cleared, the flapping interval (the length of time between the fault being raised and the condition being cleared), and the retention interval (the length of time a fault is retained in the system).


Flow Control Policy

Flow control policies determine whether the uplink Ethernet ports in a Cisco UCS instance send and receive IEEE 802.3x pause frames when the receive buffer for a port fills. These pause frames request that the transmitting port stop sending data for a few milliseconds until the buffer clears. For flow control to work between a LAN port and an uplink Ethernet port, you must enable the corresponding receive and send flow control parameters for both ports. For Cisco UCS, the flow control policies configure these parameters.


When you enable the send function, the uplink Ethernet port sends a pause request to the network port if the incoming packet rate becomes too high. The pause remains in effect for a few milliseconds before traffic is reset to normal levels. If you enable the receive function, the uplink Ethernet port honors all pause requests from the network port. All traffic is halted on that uplink port until the network port cancels the pause request. Because you assign the flow control policy to the port, changes to the policy have an immediate effect on how the port reacts to a pause frame or a full receive buffer.


Scrub Policy

This policy determines what happens to local data and to the BIOS settings on a server during the discovery process and determines when the server is disassociated from a service profile.


Depending upon how you configure a scrub policy, the following can occur at those times:


	Disk Scrub: One of the following occurs to the data on any local drives on disassociation:

			If enabled, destroys all data on any local drives

	If disabled, preserves all data on any local drives, including local storage configuration.








One of the following occurs to the BIOS settings when a service profile containing the scrub policy is disassociated from a server.


	BIOS Settings Scrub:

			If enabled, erases all BIOS settings for the server and resets them to the BIOS defaults for that server type and vendor

	If disabled, preserves the existing BIOS settings on the server








Serial over LAN Policy

This policy sets the configuration for the serial over LAN connection for all servers associated with service profiles that use the policy. By default, the serial over LAN connection is disabled. If you implement a serial over LAN policy, we recommend that you also create an IPMI profile. You must include this policy in a service profile and that service profile must be associated with a server for it to take effect.


Statistics Collection Policy

A statistics collection policy defines how frequently statistics are to be collected (collection interval) and how frequently the statistics are to be reported (reporting interval). Reporting intervals are longer than collection intervals so that multiple statistical data points can be collected during the reporting interval, which provides Cisco UCS Manager with sufficient data to calculate and report minimum, maximum, and average values. For NIC statistics, Cisco UCS Manager displays the average, minimum, and maximum of the change since the last collection of statistics. If the values are 0, there has been no change since the last collection. Statistics can be collected and reported for the following five functional areas of the Cisco UCS system:


	Adapter: Statistics related to the adapters

	Chassis: Statistics related to the blade chassis

	Host: This policy is a placeholder for future support

	Port: Statistics related to the ports, including server ports, uplink Ethernet ports, and uplink Fibre Channel ports

	Server: Statistics related to servers




Impact Analysis of Configuration Changes




















Impact Analysis of Configuration Changes

A simple “pre-flight check” compatibility check allows administrators to recognize potential compatibility issues before applying a service profile to a particular server.


In the diagram, the impact analysis detects the presence of the VIC as the mezzanine adapter, which does not advertise a MAC address. The simple Service Profile is designed to derive the MAC from the adapter; it provides no option to specify a MAC address during profile creation. This association will fail as a result. The administrator may edit the vNIC MAC and reassociate the profile.



Considerations for SAN Boot



Storage arrays have differing requirements for SAN boot:


	LUNs may be Active/Active or Active/Passive on redundant controllers

	May fail to boot on Active/Passive (or “Trespass”) type arrays

	Use Primary and Secondary boot targets for Active/Passive arrays

	Consult with your storage manufacturer for more details about your particular array

	Even arrays that claim to be Active/Active may require secondary target configuration







SAN boot configurations vary, depending on the target storage device. It is important to consult with the administrators of the storage environment to determine which targets and settings are appropriate for each compute node.



Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	A blade server requires a service profile to achieve external communication through the mezzanine card.

	Service Profiles can be configured using a Wizard in Simple or Expert Mode.

	The expert service profile wizard allows complete control over the assignment of identity, policy, and thresholds.

	The basic and expert service profile wizards are initiated from the Server tab in the navigation pane.

	UUID can be assigned from a pool, manually assigned, or derived from the server BIOS.

	WWNN and WWPN assignment can be performed from a pool, manually assigned, or (depending on the mezzanine model) derived from hardware.

	MAC address assignment can be performed from a pool, manually assigned, or derived from hardware.

	Full-slot blade servers can include two mezzanine slots in the service profile, and offer manual or automatic selection binding vNICs and vHBAs to a physical adapter.

	You must configure the binding of a vHBA to a Fibre Channel boot target.

	Server assignment can be directly selected from a list of unassociated servers, assigned at a later time, or assigned from a pool.








Lesson 7
Managing Service Profiles

Overview

In this lesson, you will learn how to manage Service Profiles.


Objectives

Upon completing this lesson, you will be able to describe how to manage Service Profiles. You will able to meet these objectives:


	Describe how to use Cisco UCS Manager to associate and disassociate a Service Profile to a server blade

	Describe how to set the server management IP address and use KVM and virtual media utilities to access a compute node




Lesson 7
Managing Service Profiles

Overview

In this lesson, you will learn how to manage Service Profiles.


Objectives

Upon completing this lesson, you will be able to describe how to manage Service Profiles. You will able to meet these objectives:


	Describe how to use Cisco UCS Manager to associate and disassociate a Service Profile to a server blade

	Describe how to set the server management IP address and use KVM and virtual media utilities to access a compute node




Associating and Disassociating a Service Profile to a Server Blade


This topic discusses how to associate and disassociate a Service Profile to a server blade using the Cisco UCS Manager (UCSM).



Associating Blade Server: Two-Step Process




















Every physical compute node that is running concurrently must have its own Service Profile, even if their profiles seem very similar (having exactly the same connectivity requirements, for example). Two easy ways for creating large numbers of very similar Service Profiles are available. These are cloning and templates. These methods will be discussed later.


During association, Cisco UCS Manager tries to assign the compute node to the Service Profile. This does not modify the compute node, but rather verifies that the compute node is compatible with the profile. If it is not compatible, the association action will fail.


Examples of incompatible blades:


	Too few physical network adapters for number of virtual network interface cards (vNICs)

	vNIC specified for failover, but compute node mezzanine adapter is Oplin-based

	vNIC specified for failover, but single Fabric Interconnect

	Once the compute node is successfully assigned, the actual association configuration process begins. This involves the Cisco UCS Manager booting the Cisco UUOS to configure the compute node



The figure shows the server mobility enabled through Service Profiles and stateless compute nodes. A logical server or Service Profile is defined with an identity including Universally Unique identifier (UUID), MAC, WWN, and VLAN/VSAN assignment. The Service Profile is associated with one blade at a time but can be migrated in the event of failure or required maintenance.


Mobile logical servers are most effective when booted from SAN environments where the server’s operating system image resides on a shared storage LUN. When the Service Profile is migrated, the new compute node inherits the server-specific properties that are stored within that Service Profile configuration. No configuration changes are required on the LAN or SAN infrastructures due to the portability of unique server characteristics such as WWPN and MAC address. Internal disks on compute nodes can be used for temporary storage or swap files.



Observe Profile Association FSM Status




















You can follow the complete process of association by clicking the FSM tab in the content pane. Recall from “Monitoring System Events” that Service Profile association and disassociation are complex processes that are assigned to a finite state machine (FSM).


If the Service Profile is unable to associate with the compute node that is selected, the FSM will provide information on which step of the process a failure occurred. This is very useful for troubleshooting Service Profile association issues.


		
Be aware that the FSM status indicator may appear to stop and lock up. Some stages of the association process can take one minute or longer to complete. This is normal.





What Happens During Service Profile Association?




















The processes that occur during Service Profile association and disassociation are very interesting. The first step in associating a Service Profile to a compute node begins by powering up the server. Next, the server Preboot Execution Environment (PXE) boots a small Linux distribution over a private network connection to the fabric interconnect.


The screenshot in the figure highlights the term “pnuosing.” Before Cisco UCS was released to the public, this Linux operating system was referred to as Processor Node Utility Operating System, or PNuOS. The official name for this is Cisco UCS Utility Operating System. The old terminology still appears in some contexts.


		
The black text on white background was reversed from the standard keyboard, video, mouse (KVM) output of white text on black background in a graphics program, for readability. The KVM does not have a choice of text or background colors.




UUOS serves as a pre-operating system configuration agent for the compute node. Cisco UCS Manager automatically powers on the compute node and sets it up for a PXE (network) boot. Cisco UCS Manager provides the DHCP response and UUOS download on a special dedicated management VLAN (4047) to the compute node. Once UUOS is booted, the Cisco UCS Manager communicates with it to configure the compute node according to the specifications of the profile.


When UUOS has finished configuring, the compute node boots using its BIOS boot order, which may or may not have been provided by the profile.



Changes Trigger UUOS



When a Service Profile is associated with blade, certain changes made to the Service Profile trigger a UUOS update (and instant server reboot).


	Configuration Change
	Triggers UCS UUOS to Run?

	Change pooled identity
	Yes

	Change non-pooled identity
	Yes

	Change Fibre Channel boot target
	Yes

	Change boot order
	Maybe

	Change BIOS config policy
	Yes

	Change vNIC failover
	Yes

	Change adapter policy
	No

	Change VSAN on vHBA
	No

	Change VLAN on vNIC
	No

	Change from access VLAN to trunk
	No







It is important to understand what types of Service Profile modifications can be made outside of a change control maintenance window. The table summarizes changes that will trigger a Cisco UCS Utility Operating System to run. As of version 1.2 of Cisco UCS Manager, the system alerts you to the changes that will result in the compute node being immediately rebooted.



Disassociate Service Profile




















To disassociate a Service Profile from its compute node, select the Service Profile in the navigation pane. In the content pane, click the link Disassociate Service Profile. A pop-up warning dialog asks you to verify the operation. Note also the small comment about observing the process in the FSM tab.



Moving a Service Profile



Disassociate from current compute node > associate to target compute node:















	Target compute node boots UUOS for association.

	vNIC/vHBA connectivity is established according to profile.

	Target compute node reboots using BIOS boot information.

	With virtualized WWN, SAN boot operates identically.

	“Change Association” (without disassociating first) does both actions sequentially.







Every physical compute node that is running concurrently must have its own Service Profile, even if their profiles seem very similar (having exactly the same connectivity requirements, for example). Two easy ways for creating large numbers of very similar Service Profiles are available. These are cloning and templates. These methods will be discussed later.


The figure shows the server mobility enabled through Service Profiles and stateless compute nodes. A logical server or Service Profile is defined with an identity including Universally Unique identifier (UUID), MAC, WWN, and VLAN/VSAN assignment. The Service Profile is associated with one blade at a time but can be migrated in the event of failure or required maintenance.


Mobile logical servers are most effective when booted from SAN environments where the server’s operating system image resides on a shared storage LUN. When the Service Profile is migrated, the new compute node inherits the server-specific properties that are stored within that Service Profile configuration. No configuration changes are required on the LAN or SAN infrastructures due to the portability of unique server characteristics such as WWPN and MAC address. Internal disks on compute nodes can be used for temporary storage or swap files.



A Compute Node Hardware has Failed



	Service Profile must be relocated to a functioning compute node




















The blade server in slot 8 has experienced a severe failure. An administrator used Cisco UCS Manager to decommission the blade in slot 8. The Service Profile that is associated with this compute node is automatically de-linked.



Management IP Address, KVM, and Virtual Media


The topic describes the options for setting the IP address for CIMC and Service Profiles, the use of the Keyboard/Video/Mouse (KVM) facility of Cisco UCS, and the use of virtual media.



Server External Management Addresses
















	UCSM automatically assigns each blade CIMC this address from pool.

	Access is through the management interface on the Fabric Interconnect.

	The same address is used for IPMI or Serial-over-LAN connectivity to compute node.







Management IP Address

Each server in a Cisco UCS instance must have a management IP address assigned to its Cisco Integrated Management Controller (CIMC) or to the Service Profile associated with the server. Cisco UCS Manager uses this IP address for external access that terminates in the CIMC. This external access can be through one of the following:


	KVM console

	Serial over LAN

	An IPMI tool



The management IP address used to access the CIMC on a server can be one of the following:


	A static IPv4 address assigned directly to the server.

	A static IPv4 address assigned to a Service Profile. You cannot configure a Service Profile template with a static IP address.

	An IP address drawn from the management IP address pool and assigned to a Service Profile or Service Profile template.



You can assign a management IP address to each CIMC on the server and to the Service Profile associated with the server. If you do so, you must use different IP addresses for each of them.


You cannot assign a static IP address to a server or Service Profile if that IP address has already been assigned to a server or Service Profile in the Cisco UCS instance. If you attempt to do so, Cisco UCS Manager warns you that the IP address is already in use and rejects the configuration.


		
A management IP address that is assigned to a Service Profile moves with the Service Profile. If a KVM or SoL session is active when you migrate the Service Profile to another server, Cisco UCS Manager terminates that session and does not restart it after the migration is completed. You configure this IP address when you create or modify a Service Profile.





Service Profile Management IP Address




















You can assign a management IP address to each CIMC on the server and to the Service Profile associated with the server. If you do so, you must use different IP addresses for each of them.


The procedure for setting the Management IP Address on a Service Profile is as follows:





	Step 1
	

	
In the Navigation pane, click the Servers tab.






	Step 2
	

	
On the Servers tab, expand Servers >Service Profiles.






	Step 3
	

	
Expand the node for the organization that contains the Service Profile for which you want to set the management IP address. If the system does not include multi-tenancy, expand the root node.






	Step 4
	

	
Click the Service Profile for which you want to set the management IP address.






	Step 5
	

	
In the Work pane, click the General tab.






	Step 6
	

	
Expand the Management IP Address area.






	Step 7
	

	
In the Management IP Address Policy field, click one of the following radio buttons:

	none: No management IP address is assigned to the Service Profile. The management IP address is set based on the CIMC management IP address settings on the server.

	static: A static management IP address is assigned to the Service Profile, based on the information entered in this area.

	pooled: A management IP address is assigned to the Service Profile from the management IP address pool.













Launching the KVM Console
















	Click the Equipment tab to access KVM for a specific physical compute node.

	Click the Servers tab to access KVM for the compute node to which this profile is associated.







Access the KVM Console from the compute node view by clicking that link from the General tab of the content pane. After profile association is complete, you will also be able to access that same console from the Service Profile view.


The KVM Console is a Video-over-IP representation of the video output on the blade. The service is provided by the blade server’s BMC via the external IP address only. You need not specify that IP address to bring up the KVM Console, but it must be up and reachable from the system where you are running the Cisco UCS Manager GUI.


You can also access the KVM Console from the Service Profile view, by clicking that link from the General tab of the content pane. Because the IP address is associated with the physical compute node and not the Service Profile, this method is preferred because it will always connect with the proper compute node, regardless of association.



KVM Console




















This figure shows the KVM Console. You will interact with it exactly as if it were the real server console.


This Cisco splash screen is generated by the compute node BIOS during the boot process.



Multi-User KVM Security



A Session Sharing dialog appears when launching KVM console where another session exists.















Existing KVM user receives request:


	Approve grants read-write session access




















Multi-User CIMC Enhancements

Cisco UCS’s remote KVM feature, called Cisco Integrated Management Controller or CIMC, now provides enhancements for multi-user access. The first user accessing the KVM gets RW privileges to the session while subsequent users are granted permission by the first user to join as read-only (by default). It also includes the ability for the Cisco UCS admin user to force termination of the KVM for a user.


If there is a currently active KVM session (for a server or Service Profile), another attempt to launch a KVM console to the same object will display a Session Sharing dialog. The second user must specify one of two options:


	Yes: which sends a read-only session request to the first session user

	No: which will terminate the KVM console application



On the active session, the first user will see a Session Sharing dialog to respond to the request:


	Approve: provides full read-write access; results in two, concurrent active sessions

	Reject

	Allow read-only




Terminating a KVM Session



Equipment tab > blade > Recover Server | Reset KVM Session



















To terminate an open KVM session from another UCS Manager client instance perform the following operations:





	Step 1
	

	
Select the blade from the Equipment tab.






	Step 2
	

	
Choose Recover Server.






	Step 3
	

	
In the dialog, select Reset KVM Server.






	Step 4
	

	
Click OK.










KVM: Accessing BIOS




















You can access the BIOS at the beginning of the reboot process just as you do for any x86 server, by pressing F2 when prompted at the beginning of the boot process.


The figure shows the advanced processor configuration options in the BIOS.



KVM Console Virtual Media
















	Virtual Media Manager allows mapping of physical or logical devices to compute node.

	Access is exactly like a physical device.







The KVM Console provides a sub-utility that allows you to use a physical CD/DVD, or an .iso file on the client (the machine on which you are running the GUI and KVM Console) that presents itself to the blade as a virtual CD medium.


The compute node can boot from this medium for installations, or for later emergency repairs.


If you are using VMware on the compute node, the Cisco UCS KVM Console virtual media is used by the compute node itself (for example, for installing VMware), not for the guest VM machines. The guest machines have their own Virtual Console, or virtual media methods, just as they do in any VMware installation.


The figure shows how to invoke the Virtual Media Manager from the KVM Console. This example uses an .iso file on the client as virtual media for the compute node.



Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	Cisco UCS Manager is used to associate and disassociate a Service Profile with a compute node.

	Certain modifications to Service Profiles already associated with the compute node can trigger Cisco UUOS to reboot the computer node.

	Service Profiles cannot be moved or renamed after being created under a given organization.

	Cisco UCS Manager allows admins to move a Service Profile from a failed compute node to a replacement.

	The KVM Console is a Video-over-IP representation of the video output on the blade.








Lesson 8
Use Cases

Overview

This lesson provides use case examples of how Cisco UCS can improve the profile of application deployments and systems management initiatives. Completing this lesson will allow you to describe how Cisco UCS provides agility and business value.


Objectives

Upon completing this lesson, you will be able to explain use cases that demonstrate how Cisco UCS can benefit application deployments and provide business value. You will able to meet these objectives:


	Provide high-level examples of application deployments on Cisco UCS, also an overview of Cisco UCS CVDs

	Describe Cisco UCS integration in the Vblock and FlexPod infrastructure stacks and the benefits of a pre-validated solution

	Discuss CIAC Full and Starter Edition as an out-of-the-box solution for orchestration and services management on Cisco UCS that allows customers drastically reduced time-to-value




Lesson 8
Use Cases

Overview

This lesson provides use case examples of how Cisco UCS can improve the profile of application deployments and systems management initiatives. Completing this lesson will allow you to describe how Cisco UCS provides agility and business value.


Objectives

Upon completing this lesson, you will be able to explain use cases that demonstrate how Cisco UCS can benefit application deployments and provide business value. You will able to meet these objectives:


	Provide high-level examples of application deployments on Cisco UCS, also an overview of Cisco UCS CVDs

	Describe Cisco UCS integration in the Vblock and FlexPod infrastructure stacks and the benefits of a pre-validated solution

	Discuss CIAC Full and Starter Edition as an out-of-the-box solution for orchestration and services management on Cisco UCS that allows customers drastically reduced time-to-value




UCS Application Solutions


This topic describes application deployments on Cisco UCS and where to find additional details for tested, validated examples of application deployment on Cisco UCS.



Integration with Microsoft System Center




















Cisco Management Tools for Data Centers Running Microsoft Software

With Cisco systems, you have choices for your data center management software. Cisco offers powerful management capabilities in its data center offerings, and Cisco management tools also complement and extend the management capabilities of the tools you are using.


For businesses in which Microsoft is a core part of the IT strategy, Cisco offers tight integration between the Microsoft System Center suite of products and Cisco data center offerings such as Cisco UCS, Cisco UCS Manager, and Cisco Nexus® Family network switches. Tools like Microsoft System Center Operations Manager and System Center Orchestrator can take advantage of the programmatic control Cisco provides through the Cisco UCS Manager XML API framework. This approach allows architects and administrators to monitor, manage, and orchestrate both hardware and software by using Cisco management technologies with Microsoft System Center server management tools and a Cisco feature that integrates with Microsoft Windows PowerShell, called Cisco UCS PowerTool.


This unified approach to infrastructure and software management in the data center is part of the Cisco Unified Data Center, a complete IT infrastructure platform for the operation and management of computing, networking, storage, and application resources. Cisco designed the platform with its view that data center solutions should be optimized for exceptional performance in a highly networked environment.


Cisco data center offerings such as Cisco UCS, together with Microsoft System Center and Windows PowerShell, provide a complete data center solution for users of Microsoft software to simplify operations and streamline service delivery. This integration helps IT personnel consolidate and unify the operation and management of the data center so that architects and administrators can manage ITaaS more confidently, efficiently, and economically and deliver value to the business through:


	More efficient operations

	Increased agility

	Greater flexibility

	Enhanced visibility and control




End-to-End Application Management for Microsoft Exchange 2010 on Cisco UCS
















	Visually correlate UCS blades, service profiles, OS, VM and Exchange Application Events

	SCOM Correlation Engine for alerts across the entire APP / infra stack

	Rapid scalability and problem resolution in Exchange Environments







Manageability

The Cisco UCS Manager application is the administrator’s entry point for managing Cisco UCS. In addition, Cisco UCS has tight integration with the Microsoft System Center suite, leading to best-in-class management and monitoring for Microsoft Exchange. With the addition of the Cisco VN-Link technology, you can rapidly migrate virtual machines by using Microsoft Hyper-V live migration, and the network and storage port mappings on the fabric will move with the virtual machines.


Microsoft provides an extensive management pack for monitoring Microsoft Exchange Server 2010 with Microsoft System Center Operations Manager. Monitoring, however, stops at the bottom of the operating system (OS) layer because there is no insight into the hardware layer. The Cisco UCS management pack for Microsoft System Center Operations Manager provides the missing layer, with insight from the chassis all the way to the virtual machine layer. In conjunction with Microsoft System Center Virtual Machine Manager, System Center Operations Manager can also manage and monitor virtual machines and the hosts on which they are running.


You can use Microsoft System Center Virtual Machine Manager to dynamically rebalance workloads in a Cisco UCS environment without concern for the availability of network and storage resources across host machines. The Cisco VN-Link technology that connects virtual machines to the unified fabric integrates with Microsoft Hyper-V at a low level. This approach delivers greatly increased flexibility through the capability to move virtual machines across many host nodes and increased performance because of the direct-memory-access (DMA) level of integration with Microsoft Hyper-V.



Cisco Unified Communications




















All of the information needed for unified computing on Cisco UCS deployments is provided in the dokwiki at: http://www.cisco.com/go/uc-virtualized.


The Cisco Unified Communications Sizing Tool (CUCST) allows administrators to calculate the required amount of Cisco Unified Communications applications. CUCST supports Cisco Unified Communications applications like Cisco Unified Communications Manager, Cisco Unified Presence, Cisco Unity Connection, Cisco Unified Contact Center Express and others. To access the CUCST, go to following URL (CCO login required): http://tools.cisco.com/cucst.


Sizing UC Applications for Virtualization

Verify the supported UC Application products and versions listed in Unified Communications Virtualization Supported Applications. Sizing virtualized UC applications is the same as appliance sizing. Use the Cisco Unified Communications System Design Guidance and Unified Communications Sizing Tool. Instead of determining appliance size and count, determine Virtual Machine size and count.


Select an appropriate OVA/OVF template for each “Server” required for a UC application. Each UC application has one or more OVA/OVF template options. See Unified Communications Virtualization Downloads (including OVA/OVF Templates).


Follow the coresidency policy in Sizing Guidelines to determine which OVAs can share physical servers. Which OVAs should share a physical server depends on customer placement logic, which includes but is not limited to considerations for geographic distribution, minimizing server footprint, server/site redundancy, security domains, change management, service level agreements and assessed business criticality of the individual UC applications.


Verify alignment of virtualization support details (such as supported hardware or VMware features). These details may vary based on each UC application. Because UC applications share physical resources, be sure to verify alignment of these details for all UC applications in your deployment, particularly for UC applications that share a physical server.


Key Differences between Appliances and Virtualization of Cisco Unified Communications

Deployments on Cisco 7800 Series Media Convergence Servers that are shifting to virtualization should prepare for the following:


	Virtualization of Cisco Unified Communications is “not an appliance.“

	You must independently configure, manage, and monitor the hardware, VMware software, and virtualized UC applications.



Higher level of expertise with server administration, VMware vSphere/vCenter administration, and storage administration is expected from whoever is managing the UC deployment. This expectation is more for deployments of Specification-Based Hardware Support and UC on Cisco UCS B-Series than for UC on Cisco UCS C-Series Tested Reference Configurations.


If you are using Shared Storage (SAN or NAS), be aware that it is a critical solution component and if improperly configured, monitored, or managed will cause significant performance and availability issues for UC apps. Ensure that whoever is managing the UC deployment is experienced and proficient with storage management.


Adhere to all licensing requirements for virtualized UC applications. For example, license keys may be different when virtualized than they are on appliances. UC application features that are dependent on physical USB ports are not supported. See the required UC Application documentation for alternatives and workarounds.



RISC Migration to Cisco UCS




















Five years ago, RISC/UNIX was the right choice but today, there are challenges with these platforms.


RISC/UNIX is unable to meet business flexibility and agility needs for the following reasons:


	Complex system management

	Inflexible environment from both a server and cabling perspective

	Very few build private or public clouds on RISC: inflexible and poor economic model

	Complexity and high costs make it difficult to upgrade/migrate applications to another RISC/UNIX platform

	Transitioning with corporate mergers/acquisitions is difficult because open, standard platforms are not being used

	New projects coming online and user growth leads to increased need for greater scalability, capacity and better utilization (should this be targeted more towards the need for private clouds and virtualization with applications moving around the data center)

	Skills shortages: Since graduates coming out of school are experienced primarily with Linux/x86 and Windows/x86, few graduates have RISC/UNIX experience. The current population with RISC/UNIX background is aging and very expensive



There are high costs while IT department budgets are shrinking:


	High environmental, support, and maintenance costs with current RISC/UNIX infrastructure

	Extremely high capital costs to upgrade to another RISC/UNIX platform and high operational costs with greater environmental and maintenance costs

	High software licensing costs

	High levels of complexity leading to high administrative costs

	Too many different platforms with different management paradigms

	High cost of trained staff/talent and difficulty in finding them



There is poor price-performance:


	Current installations have insufficient performance, and are not keeping pace with the market, that is, the latest Intel Xeon processor performance capabilities.

	Upgrading to the latest RISC/UNIX platform is VERY expensive

	Out-of-band high costs for small modular capacity increase



There are environmental factors:


	Large footprint

	High power and cooling requirements

	RISC/UNIX server sprawl leading to need/desire to consolidate



Customers are uneasy about future plans of Oracle for SPARC/Solaris:


	Not a hardware company: Uncertain roadmap

	Already has a Linux offering and may be positioning x86 as a SPARC replacement

	Long release schedules

	Vendor lock-in

	Support policies that require bundling: difficult to do business with

	Many long term Sun partners no longer on-hand as trusted advisors



You are hesitant to migrate mission-critical apps to new OS/Platform:


	Possible multi-year pain of migration

	Lack of experience/trust of Services/OS/Platform vendor

	Unknown after-effects in operational processes

	Presumed lack of staff knowledge and the costs of retraining

	(RASS) Need 4/5-9’s availability, not sure x86 platforms can deliver



The following are some reasons to migrate off a current RISC/UNIX platforms. Fundamentally, the landscape of the business-critical application market has changed. IT organizations have come up against the real limits and uncertainty within their legacy RISC/UNIX implementations. IT departments are experiencing the pain of increasing RISC/UNIX maintenance and software license costs while IT department budgets are shrinking. At the same time, the aging infrastructure doesn’t provide the performance or the flexibility required to support the requirements of the business coupled with overarching uncertainty about RISC/UNIX futures foreshadowed by missed deadlines, changes in roadmaps and dropped hardware and software support.


At the same time, the x86 architecture has matured into the de facto industry standard, fostering a rich application, operating system, server, and partner ecosystem. The majority of enterprises are adopting Intel® Xeon® processors in all aspects of enterprise computing, taking advantage of the improved economics and flexibility of industry-standard architectures.



RISC / x86 Data Integrity and System Availability Comparison



RAS features of the latest Intel Xeon processor E7 family have caught up with RISC:


	Red Hat RHEL supports advanced RAS features:

			Machine check architecture recovery

	Predictive failure analysis

	PCI hot plug

	High availability






	Cisco UCS stateless computing significantly improves availability characteristics
















* Other names and brands may be claimed as the property of others.






Historically, RAS features were a key differentiator for the RISC-based platform. However, the IA has not only closed the gap but in some cases raised the bar. Cisco UCS with its energy efficient, redundant design and stateless architecture also improves availability. Because of the Red Hat Enterprise Linux support for advanced RAS features, the Cisco, Intel, and Red Hat combination can deliver excellent levels of reliability and performance.



Converged Infrastructure


This topic illustrates Cisco UCS integration in the Vblock and FlexPod infrastructure stacks and the benefits of a pre-validated solution.



Validated Converged Infrastructure




















Key Takeaway: There are four different categories of stack or bundled cloud solutions for positioning converged infrastructure. Because FlexPod and Vblock are validated architecture, as opposed to a bundle or stack, cloud or virtual system IT teams benefit from better integration and scalability between the server, network and storage stack resources, and can extend Cisco UCS compute resources to existing IT infrastructures with Nexus and V-series features.


This list provides some positioning features:


	Integrated infrastructure stacks that include the Cisco UCS server, Nexus networking, and storage components; all of the solutions in this category have a validated design from the manufacturers and in some cases even come pre-wired.

	Legacy server, network and storage technology that customer or service providers bundle together on their own.

	Cloud stacks that include pre-assembled hardware with cloud orchestration software layered as a turnkey solution from the manufacturer.

	Virtualized bundles that are not pre-wired but are sold and assembled by the manufacturer as a single SKU, which do not include cloud orchestration layer software.




Cisco Validated Designs




















Lab-Validated Guidance

Cisco Validated Designs consist of systems and solutions that are designed, tested, and documented to facilitate and improve customer deployments. These designs incorporate a wide range of technologies and products into a portfolio of solutions that have been developed to address the business needs of our customers.


Cisco Validated Designs are organized by solution areas and will list one, two or all three primary types of documents:


	Design Guides

	System Assurance Guides

	Application Deployment Guides



Cisco Validated Design (CVD) Guides are comprehensive, design and implementation guides. The validated systems and solutions have undergone thorough architectural design development and lab testing, and provide guidance for the introduction of new technologies, emerging architectures, or the enhancement of the customer's network.


To qualify as a Cisco Validated Design Guide, the design must meet the following criteria:


	Products that are incorporated in the design are generally available.

	Deployment, operation, and management of components within the system are repeatable processes:

			Validates a specific design or architectural practice on a limited scale and duration

	Is generally focused on key technology or products integrated as a system

	Ensures the viability of theoretical designs or concepts






	Detailed system design and implementation guidance are available to provide:

			Customer with examples that define the problems solved by the design

	A list of products that were validated as part of the design testing

	List of software that was used for each component of the design

	Configurations used to support the design tests

	Design limitations that were discovered during the testing








Where the customer network requirements extend beyond the scope of a Cisco Validated Design, Cisco Advanced Services can provide additional services in support of your specific requirements.



Prewired and Integrated Data Center Pods



Why do cloud architectures use pods?


Modular physical and logical DC building block based on standardized elements:


	Predictable infrastructure characteristics:

			Power, cooling, cabling, airflow






	Predictable scalability:

			Network, Compute and Storage






	Deterministic functions and replicable




















The key goal of an integrated infrastructure design is to create a scalable and manageable infrastructure with predictable performance that also provides the necessary level of security for a multitenant deployment.


The concept of a pod is key, because a pod identifies a modular unit of data center components with a well-defined set of resources, including network, compute, and storage resource pools, and predictable power, cooling, and space requirements. Using pods allows the cloud to be expanded on demand quickly and efficiently by adding additional pods, without any change in the overall architecture.


Pods can be implemented using one of the validated architectures of the FlexPod for VMware reference architecture defined jointly by NetApp, Cisco, and VMware.



Platform Characteristics



FlexPod


	Highly flexible configurations that allow customers to size the FlexPod to their specific needs

	Qualified partner assembles hardware on-site―can reuse existing customer equipment as long as it is supported

	Supports multiple third-party orchestration platforms
















Vblock


	Highly standardized configurations that scale in predefined increments

	VCE pre-stages hardware in their build center and delivers ready-to-power

	Supports EMC UIM and Cisco IAC




















The figure highlights the basic differences between the Vblock and FlexPod architecture models.



Accelerating the Business




















Anticipating future demands that will be placed on IT and responding in a timely manner is the Achilles’ heel of today’s siloed IT infrastructures. Many organizations perform the following steps when servicing a new applications or requirement:


	Collect detailed requirements.

	Design and Size: Work with the vendors to architect and size the system. Depending on the number of applications and the number of vendors involved, the sizing and architecting phase can take many weeks and often months.

	DC Planning: Work with the Data Center team and negotiate power, floorspace, and cooling requirements for the new system.

	Procure Equipment: This involves many weeks of getting quotes, ordering, and receiving the new gear. Dealing with a large organization plus shipping times can easily take six weeks.

	Detailed Design: This phase involves working out exact configuration for compute configuration, network connectivity, storage layout, security, and isolation.

	Deploy: Configure the equipment for use.

	Test: Create and run a test plan to ensure that the equipment is correctly installed and meets the original requirements.



By using a pre-integrated, IaaS-type of architecture like Vblock or FlexPod, many of these phases are compressed or removed altogether. Customers taking the standardized approach to IT have repeatedly demonstrated better than 50% faster deployment times over traditional approaches.



Cisco IAC


This topic describes CIAC full capabilities and Cisco IAC Starter Edition as an out of the box solution for orchestration and services management on Cisco UCS that allows customers drastically reduced time-to-value.



What is CIAC?



Cisco Intelligent Automation for Cloud or Compute with advanced automation and orchestration:



















The CIAC portfolio is primarily focused on the solution at the commercial and enterprise market segments, and provides a key component of Cloud Computing: self service. Cisco’s Cloud Portal (CCP) product, formerly newScale, provides this functionality. This is the portal where you can provision and de-provision resources with automated fulfillment.


Cisco’s Process Orchestrator and Server Provisioner (Formerly Tidal) engine is the orchestrator that executes these tasks and automated instructions. Other key functionality includes resource management, managing dynamic resource allocation, capacity, operational process automation, and so on.


Only 20% of an application total cost of ownership is the infrastructure. The other 80% are the people manpower to implement it and to keep it running over those years. This is really where Intelligent Automation with Cisco’s CIAC portfolio solutions can solve problems.


These tools enable rapid and dynamic provisioning at all levels:


	Hardware

	Virtualized systems

	Software (Applications and Operating systems)



Although there is some overlap between products such as CPO and CCP, this offers the ability to automate at two different levels:


	IT Systems automation only (non-cloud): CCP stand alone, IT does not require end-user portal or billing, or already has their own automation engines in place.

	Full Cloud automation: CPO/CSP/CCP. Full service “Cloud” including end-user facing portal and service catalogs.



Network Services Manager (formerly LindSider Overdrive) completes the picture with Network based configuration and automation, with additional benefits in a multitenant environment.



CIAC Self-Service Portal and Service Catalog




















This figure describes a high-level flow for the major components involved in defining, managing and delivering a service with Cisco IAC.



CIAC Starter Edition v3.0




















The Cisco Intelligent Automation for Cloud 3.0 Starter Edition is a simple, compute-only, single point-of-delivery (POD) cloud solution that provides quick and easy deployment. The Cloud Starter Edition automation pack that ships with the solution contains content that is designed to work out-of-the-box so you can get it up and running quickly.


The Cloud Starter Edition integrates with the following products to provide cloud compute and cloud orchestration:


	Cisco Cloud Portal: Provides the self-service portal from which employees of the organization can order services

	Tidal Enterprise Orchestrator: Provides the orchestration and reporting for services that are ordered through Cloud Portal

	Cisco Server Provisioner: Provides bare metal provisioning of operating systems on physical or virtual servers

	Cisco UCS Manager: Provides the provisioning of physical servers



Starter Edition follows a different approach in positioning and taking cloud solutions to the market:


	New Functionality:

			Pre-built Portal Screens and Orchestrator Workflows for Physical and Virtual

	Provisioning on Cisco UCS and vCenter

	Added Cloud Administration and Organizational Admin Roles

	Added new Registration and Administrator Screens / Console






	Starting Point for Journey to Cloud:

			For customers who want to start with Starter Edition functionality (that is, no enhancements) a much faster technical turn on

	Services Engagement methodology that ensures customer success from best practices






	New Channel Program for Intelligent Automation:

			ATP Program for 20+ partners for Delivery and Ecosystem Partner engagement, training, shadowing and validation

	Professionally created multi-module training curriculum for the Starter Edition









Starter Edition Home Page: Order Servers Tab




















The figure describes the Order Servers tab in Cisco IAC Starter Edition portal.



Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	Cisco UCS Application Solutions are demonstrated in the large number of tested and validated examples found in Cisco Validated Designs.

	Vblock and FlexPod integrated architectures provide customers rapid time-to-value that can not be achieved with standard compute bundles and stacks.

	Cisco IAC is an enterprise class solution for Service Catalog, Service Delivery Orchestration, and Lifecycle Management.

	CIAC Starter Edition is a pre-validated Compute as a Service solution specifically designed to be deployed quickly on Cisco UCS.








Lesson 9
Intel Reliability, Performance, and Security Optimizations

Overview

In this lesson, you will learn how to explain the value of the Intel chipset for providing highly-available, easily-maintained servers.


Objectives

Upon completing this lesson, you will be able to explain the value of the Intel chipset for providing highly-available, easily-maintained servers. You will able to meet these objectives:


	Identify features of the Intel Xeon processors

	Identify Reliability, Availability, and Serviceability features of the Intel Xeon processors

	Identify the features of the Intel Xeon processors that optimize performance

	Identify the Security Features of the Intel Xeon processors

	Discuss the Cisco UCS BIOS Best Practice settings for desktop virtualization




Lesson 9
Intel Reliability, Performance, and Security Optimizations

Overview

In this lesson, you will learn how to explain the value of the Intel chipset for providing highly-available, easily-maintained servers.


Objectives

Upon completing this lesson, you will be able to explain the value of the Intel chipset for providing highly-available, easily-maintained servers. You will able to meet these objectives:


	Identify features of the Intel Xeon processors

	Identify Reliability, Availability, and Serviceability features of the Intel Xeon processors

	Identify the features of the Intel Xeon processors that optimize performance

	Identify the Security Features of the Intel Xeon processors

	Discuss the Cisco UCS BIOS Best Practice settings for desktop virtualization




Intel Processor Overview


This topic explores the characteristics of the Xeon processor.



New Technologies Reduce Total Cost of Ownership



Technical Advancements Since 2007:


	Intel® Turbo Boost Technology 2.0

	Higher frequencies to reduce hand workload spikes

	Intel® AES-NI

	Accelerate encryption by up to 10X1

	Intel® Trusted Execution Technology

	Defend against attacks during launch

	Intel® Advanced Vector Extensions

	Increases FLOPS per clock up to 2X2

	Intel® Integrated I/O

	Reduce I/O latency up to 30%3

	Intel® Hyperthreading

	Double the number of threads per core

	Intel® Virtualization Technology


	Platform capabilities to enhance virtualization performance


	Intel® Node Manager 2.0


	Power monitoring & limiting to maximize operating efficiency


	Intel® Data Center Manager


	Data to enable improved dynamic workload placement & migration


	Intel® QuickPath Interconnect


	High bandwidth processor interconnect delivering up to 3.5x the bandwidth4


	Integrated Power Gates


	Enables idle cores to go to near zero power draw








This slide is a summary of all key enhancements in Intel processor architecture since 2007.


Key points include:


	Compared to a system likely up for refresh in 2012 the latest generation processor has significant new technologies

	These technologies combine to make systems higher performing, more efficient, secure and easier to manage.




Story: Cisco partners with Intel because of our innovation that spans many generations of processors. All of these new technologies combine to offer the microprocessor at the heart of your next generation data center high performance, optimized I/O, flexible power management and robust security to meet the growing demands on your IT infrastructure.


Industry analysts (IDC Server Workloads 2008 MCS) estimate that dual socket servers are refreshed every 3-5 years with an average life of a bit over four years. Compared to quad core systems from that timeframe Intel now offers processors that have improvements across a range of vectors–from improved system bandwidth with Intel Quickpath interconnect, to Intel integrated IO, to adaptive performance with Intel Turbo Boost technology.


These technologies have addressed many of IT’s top concerns, including providing tools to reduce the risks of security threats, reducing energy consumption, offering better tools for power management, and of course–adding significant increases in performance, I/O throughput, and core/thread count to be able to get more work done, in less space at less cost.


Legal Information

	(AES-NI Performance) Source: Testing with Oracle Database Enterprise Edition 11.2.0.2 with Transparent Data Encryption (TDE) AES-256 shows as much as a 10x speedup when inserting one million rows 30 times into an empty table on the Intel® Xeon processor X5680 (3.33 GHz, 36 MB RAM) using Intel IPP routines, compared to the Intel® Xeon® processor X5560 (2.93 GHz, 36 MB RAM) without Intel IPP.




Intel Integrated Performance Primitives (Intel IPP) 

Intel IPP is a multi-threaded software library of functions for multimedia and data processing applications, produced by Intel.


The library supports Intel and compatible processors and is available for Windows, Linux, and Mac OS X operating systems. It is available separately or as a part of Intel Parallel Studios.


The library takes advantage of processor features including MMX, SSE, SSE2, SSE3, SSSE3, SSE4, AES-NI and multicore processors. Intel IPP includes functions for:


	Video Encode/Decode

	Audio Encode/Decode

	JPEG/JPEG2000/JPEG XR

	Computer Vision

	Cryptography

	Data Compression

	Image Color Conversion

	Image Processing

	Ray Tracing/Rendering

	Signal Processing

	Speech Coding

	Speech Recognition

	String Processing

	Vector/Matrix Mathematics



	(AVX Performance) Source: This is a performance comparison using the Linpack benchmark. The baseline score of 159.4 is based on Intel internal measurements as of 4 August, 2011, using a Supermicro* X8DTN+ system with two Intel® Xeon® processor X5690, Turbo Enabled or Disabled, EIST Enabled, Hyper-Threading Enabled, 48 GB RAM, Red Hat* Enterprise Linux Server 6.1 beta for x86_6. A new score of Y 350.3 is based on Intel internal measurements using an Intel® Rose City platform with two Intel® Xeon® processor E5-2690, Turbo Enabled or Disabled, EIST Enabled, Hyper-Threading Enabled, 64 GB RAM, Red Hat* Enterprise Linux Server 6.1 beta for x86_6.

	(Integrated I/O): This is an Intel measurement of average time for an I/O device read to local system memory under idle conditions. Improvements compare Xeon processor E5-2600 product family versus Xeon processor 5600 series.

	Intel internal measurement (Feb 2009): This is the Stream-Triad benchmark, using Red Hat Enterprise Linux Server 5.3. Intel® Xeon® processor E5472, 3.0 GHz, 2x6MB L2 cache, 1600MHz system bus, 16GB memory (8x2GB FB DDR2-800) versus Intel® Xeon® processor X5570, 2.93 GHz, 8MB L3 cache, 6.4QPI, 24GB memory (6x4GB DDR3-1333).





Xeon Processor Numbering




















The Product Family identifier provides additional information.


The first character tells how many processors are natively supported in a system; the second character, or socket type, signifies processor capability; and the third and fourth characters are SKU numbers which, along with the whole number, represent a collective group of capabilities at a given price. In the example shown here, the SKU numbers represent the E7 product line.


The 4 in the Product Family identifiers means it supports four processors natively in a system, and the 8 is the socket type. A socket type of 8 supports a higher general level of system capability, for example more memory and I/O, than a socket type of 2.


The socket type digit does not change over time, meaning the follow-on to the Intel Xeon processor E7-4800 v2 product family would be the Intel Xeon processor E7-4800 v3 product family. The '8' didn't change. All that changed was the version number (from ‘v2’ to ‘v3’).


		
The source of this information is http://communities.intel.com.





Tick-Tock Development Model: Sustained Microprocessor Leadership
















Sandy Bridge = Romley platform






Features of the new Tick Tock development process include: 


	New Micro-architecture: Large advancement in processor technology on current generation of processor size

	New Process Technology: Smaller die size with a few enhancements to processor technology features and performance



Intel has optimized their development process and nicknamed it Tick Tock. Every two years brings a new process on line (reduced processor size). Intel invested $9 Billion USD in 2011 to drive to smaller process geometries. When Intel shrinks the process, such as the Nehalem architecture from the 45nm to 32nm Westmere processor, Intel implements the current copy of existing features to ensure we get to volume with minimal time, and deliver a new set of (often pin compatible) processors to the market. On the alternating years, when Intel’s process is in high volume production, Intel does not stand still but advances their x86 micro-architecture. Termed the Intel Architecture, the new micro-architecture offers innovative features and new capabilities, along with increases in the number of cores, cache, and interfaces. With the 32nm Sandybridge, Intel is delivering PCIe 3.0, Turboboost 2.0, and enhancements to Intel AVX. With these Intel Architecture innovations and Cisco’s close partnership, Cisco sets more benchmarks with the Sandybridge processor as part of Intel’s Romley platform.



2012 Intel® Xeon® Processor Families




















In 2012, Intel introduced several new product families to complement their top-of-the line Intel Xeon processor E7 family (with its highest reliability and scalability targeted for the high end enterprise and mission critical markets). Intel launched their mainstream leadership two-socket product, the Intel Xeon processor E5-2600 product family on March 6, which introduced new technologies and features to bring additional flexibility and energy efficiency to the market. The Intel Xeon processor E5-4600 product family will bring density- and cost-optimizations to four-socket servers, and the Intel Xeon processor E5-2400 product family will add an entry two-socket solution for customers looking to move up from one-socket servers (scheduled for release in Q2).


The Intel Xeon processor E3-1200v2 product family will introduce the next generation silicon technology in a high-density, economical one socket processor (when it is announced along with the Intel Xeon processor E5-4600/2400 product families on May 14, 2012).


		
In the figure, Xs indicates the number of sockets in each processor.





Product Selection by Workloads




















Typically, when deciding what server to recommend for a given application, one of the first decisions will be the need to determine “the size of server to deploy”. Should it be a single processor system, or dual processor (dual socket), or a larger symmetric multi processor (SMP) server? Typically, Cisco and Intel see definite trends, based on the type of workload, as shown in the figure. More demanding workloads, in terms of simultaneous execution threads and memory requirements, should run on 4 socket SMP or larger systems, shown in blue in the chart. The blue represents the 7000/E7 sequence Xeon processor servers, and orange represents the 5000 sequence servers. For the smaller workloads and applications, such as collaboration through UC on UCS, and web infrastructure, the Xeon 5000 series servers are an excellent performance per watt choice. It is worth noting that with higher instances of the workload and larger numbers of users (shown left to right in the chart), Intel recommends the Xeon E7 based UCS offerings.


		
S=Socket and shows the number of processors with which a server can be populated.




The figure matches workload “affinity” to type of platform. It is not intended to imply relative %s of workloads by system type.


For example, Mission Critical Business Processing workloads have a higher % incidence (affinity) to EX (Expandable Server) class servers than they do on EP (Efficient Performance) servers. While IT Infrastructure workloads are deployed on EX class servers they have a much higher incidence on EP servers (that is, IT Infrastructure workloads make up a larger % of total EP workloads than the % of total EX workloads.


This internal Intel assessment is based in part on 2009 IDC Server Workloads Forecast and Analysis Study (by Matt Eastwood).


Workload Sizing

Actual usage of 4-socket for volume 2-socket servers depends on many variables, but the main performance-oriented vectors are the application type, size of workload, number of simultaneously connected users and targeted server utilization. The size and number of users. There is wide variation, so IT shops devote planning resources to characterize the workload and user profiles to identify when 2-socket or 4-socket makes a good candidate. Then actual hardware performance test are run to determine the best solution for broad deployment.


But quantitative thresholds at which 4S systems may become a good candidate are useful for high-level discussions with customers. Here are some highly simplistic guidelines for the identification of situations where a 4S system may be a good candidate for consideration. These are for discussion purposes only. These are not intended as short cuts to comprehensive server sizing effort:


	Databases >50GB (assuming heavy duty transactions and multiple simultaneous users)

	Database with more than 200 users

	Virtualization of large VMs (>8GB/VM) or targeted highest level of VM per server consolidation goals (the average is ~10-20 on 2skt servers, although 2skt servers are capable of more)

	Targeted server utilization >70% (at such high utilization, a 4S system provides greater overhead for peak loads, 2S systems running at 70% baseline may be tapped out if the load peaks much above this

	Business Intelligence >250GB (assumes similar to a database but with far fewer users)

	SAP Business Suite >800 users

	Multi-tier consolidation. Example: consolidate multiple SAP applications off of multiple, older distributed servers onto a single 4S server (eliminating the cost and complexity associated with scaling out)




Intel® Xeon® Processor E7-8800/4800/2800 Product Families




















When looking to deploy larger workloads, larger numbers of users, and mission critical applications, the Xeon E7 family on Intel’s industry leading 32nm processor are your best choice. The Xeon E7 family offers up 10 cores with 2 threads per core. This provides a whopping 30MB of shared last level cache to enable the deployment of more virtual machines, and larger application consolidation, all on a single CPU. Memory capacity is expanded to 2 Terabytes of DDR3 memory. Like the Xeon 7000 Series family, the E7 family offers 20 RAS features for mission critical applications. This enables you to remove legacy, expensive, power-hungry RISC architecture deployments within your datacenter. The same Advanced Technology capabilities are again supported with Trusted Execution Technology and AES hardware encryption.



Intel® Xeon E7 Advanced Technologies

















So let’s take a moment to focus on the Advanced Technologies supported by Intel Xeon E7 processors, and discuss why there is a major advantage for Cisco UCS using Intel processor-based servers.


In the figure, we’ve grouped technology enhancements into 5 common challenges that IT typically faces in the datacenter. These are Security, Resilience, Service Delivery, Performance, and Power Consumption. From a security perspective, Intel enables performance without compromise using hardware accelerated AES encryption/decryption. For example, with cloud computing, enabling cloud to cloud data movement and communication with data encrypted and protected by AES, Intel TXT ensures that only the correct signed image runs on the CPU. In addition, VM migrations happen only when supported by system policies. Also, only signed software present at boot time runs on the UCS server, and software integrity has not been compromised by malware or rootkits. To support mission critical deployments, the Advanced RAS features answer all customer concerns regarding equivalence to RISC and power processor-based offerings. Finally, with Intel’s Xeon offerings, you can expect the same full platform-wide virtualization support and power management capabilities.


Additional Details

Features of Advanced Technologies include the following:


	Service Delivery improves the performance of service delivery, which is increasingly accomplished by virtualized machines. Improved performance technologies enable the HW to intelligently adapt and optimize for the workload characteristics at any given moment.

	Power technologies reduce the often significant cost of idle and active power of the server at the server, rack, and entire data center level.

	We’ve talked about AES-NI earlier. Trusted Execution verifies the integrity of the software running on an Intel Xeon based system, and ensures that code is the intended and signed image. This protects against hackers comprising a system, and mistaken updates or patches.

	We’ve also discussed how RAS ensures the highest resilience of a mission critical deployment.

	Intel virtualization enables near native performance with a virtualized system and supports VM migration to enable IT to easily update systems, and move loads to rebalance a datacenter.

	Xeon with Hyper Threading improves core efficiency and performance by supporting two threads per core. Turbo boost utilizes thermal headroom to increase clock frequency on the core to deliver higher performance for those threads running on that core.

	Finally, with datacenter power consumption comprising a major OPEX item, Intel offers processor level power management, system level power management, and the Intel Data Center Manager to provide deployment-wide power management.



Intel continually looks to offer new features and advanced technologies to anticipate, meet and at time exceed IT needs.


Security

Security is a growing concern for businesses of all sizes:


	AES-NI speeds up encryption algorithms by performing the work in hardware, instead of software. Generally speaking, any time you can do something in hardware instead of software, you’re going to see a performance boost. In this case, HW encryption/decryption on Xeon 5600 with AESNI can be up to 7x that on Xeon 5500. This significant jump allows IT departments to deploy encryption of data that was not feasible before due to performance concerns.

	Intel TXT measures the launch environment, whether it’s the virtualization hypervisor or a bare-metal operating system. Essentially, it can tell whether something inappropriate (from a hack or a mistaken update file) has been inserted into the system boot stack.



Reliability

Reliability features support the most demanding Mission Critical workloads in the data center. Xeon offers a multitude of Mission Critical class reliability features in Xeon, in 3 subcategories:


	Protecting data integrity: Xeon provides advanced circuitry to keep temperatures from getting too high–and causing data errors. It also provides parity and error correcting code (ECC) to detect and correct errors, by flagging bad data locations to make sure the bad data is not used by applications.

	Increasing Availability: It’s not always possible to prevent errors, so when they happen you need a mechanism for handling them. When data errors or failing components are detected, the Xeon processor E7 product family has features like Machine Check Architecture recovery―once only available on Itanium, RISC and mainframe systems. This feature enables built-in fail-over and automatic reconfiguration capabilities that allow the system to continue working.

	Minimizing Planned Downtime: Xeon products (especially the Xeon E7 family) work with the OS and system software to identify failing components in advance of their actual failure. This capability allows failing components to be replaced during planned maintenance cycles, and allows IT to extend the time between these cycles (for example error logging, memory board hot add/remove, operating system CPU online/off-lining).



Virtualization Technologies

When servers are deployed using virtualization, a portion of the raw physical performance is lost due to the overhead associated with running the virtualization software itself:


	Intel’s virtualization technologies in the processor, chipset and I/O aim to minimize this overhead so that the virtual performance is as nearly equal to native physical performance.



Power Efficiency

Data center power cost can be substantial. The total power cost over the life of some servers can exceed their initial hardware acquisition cost. Intel technologies aim directly at improving that statistic:


	Intel Intelligent Power Technology has integrated power gates which allow individual idling cores to reduce to near-zero power independence from other operating cores, reducing power consumption.

	And Intel Intelligent Node Manager and Data Center Manager combine to solve power and cooling issues for lower TCO. They enable managing power and cooling resources at the system, rack, row and the data center level via active monitoring and dynamically capping server power.




RAS Features


This topic explores the Reliability, Availability, and Serviceability (RAS) characteristics of the Xeon Processor.



Intel® Xeon® E7 Family RAS Philosophy




















Intel’s Xeon E7 Family Reliability, Availability and Serviceability (RAS) philosophy includes:


	Self monitoring: Actively monitors key interconnects, data stores, data paths, and subsystems.

	Self healing: Acts automatically, based on configurable thresholds or known error conditions. Provides information to minimize future problems.




E7 Family Portfolio of RAS Support




















The figure summarizes the E7 Family portfolio of RAS support. It shows which features provide protection for data (Integrity), and increased availability or minimized planned downtime (to Improve IT Efficiency).


We will go through these features in this lesson.



Categories of System Errors



Hard Errors: Permanent physical failure at the hardware level:















	Usually recovered through failover to redundant components

	Ultimately require replacement of the failed hardware



Soft Errors: Transient errors caused by alpha particles affecting the state of circuits:















	Usually corrected through error detection and correction codes (ECC)

	Advanced recovery mechanisms offer protection from uncorrectable errors



	RAS Assures Availability Across Error Categories







In general, there are two types of errors; hard errors and soft errors:


	Hard errors: Hard errors pertain to physical failures at a hardware level and will most likely require a redundant component for failover. The hardware level failure will require the component to be replaced.

	Soft errors: Soft errors are caused by a transient state affecting the state of the circuits. Many of these errors can be corrected by ECC methods. In the case of software errors that are more serious, more advanced recovery methods may predict software errors that would normally render the state uncorrectable, and thereby avoid the error entirely.




Enhanced Double Device Data Correction (DDDC+1)



	Able to fix both single and double device memory hard-errors and still correct an additional single bit error:

			No performance penalty for mapped out devices.






	Can improve system uptime and reduce DIMM replacement rates lowering overall service costs:

			Enables IT shops to avoid unscheduled maintenance, saving time and money





















	Eliminates need to immediately replace failed DRAM providing IT with greater control and flexibility.







Memory RAS Features

Extensive RAS features are integrated to detect and correct errors throughout the memory subsystem. Double Device Data Correction (DDDC), designed to protect errors in DRAM devices, is one of these integrated features.


DRAM Protection

Error Correcting Code (ECC) mechanisms are implemented to detect and fix errors in attached memory components. Both Single Device Data Correction (SDDC) and Double Device Data Correction (DDDC) are supported. SDDC is strong enough to correct multi-bit errors in a single DRAM device, to map out a failed device, and to continue correcting single-bit errors after a device is mapped out. DDDC is even stronger. It can correct multi-bit errors in two DRAM devices, map out two failed devices, and continue correcting single-bit errors after the devices are mapped out. These mechanisms can improve system uptime and reduce DIMM replacement rates. There is no performance penalty for mapping out the devices.



Protecting Data Integrity and Availability
















	Reduces Circuit-level Errors, Detects and Corrects Data Errors Across the System, Limits the Impact of Errors







The figure summarizes the Integrity and Availability features we are discussing.


Memory Scrubbing (Patrol and Demand)

The protections described in the figure are activated only when a memory location is read. However, errors can occur to data or instructions in memory locations that are not accessed. If these errors accumulate, they can result in multi-bit errors that cannot be corrected and could result in data corruption and even system failure. Memory scrubbing in the Intel Itanium processor 9300 series employs an integrated hardware engine to find and correct memory errors before they accumulate. Scrubbing is performed periodically and automatically on all populated memory locations (Patrol Scrubbing). In addition, when errors are discovered for data in transit, the corrected data is rewritten back to the appropriate memory location (Demand Scrubbing).


Memory Thermal Protection (Memory Thermal Throttling)

Overheating of memory components can cause or accelerate component failure. Intel E7 supports two mechanisms for throttling commands issued to the memory channels to protect against overheating. Closed loop thermal throttling (CLTT) is triggered by a thermal sensor in the DIMM that sends a signal to the memory controller. Open loop thermal throttling (OLTT) is triggered when the rate of memory commands per DIMM exceed a configurable limit for a configurable time window. Alternately, the firmware can be configured to increase system fan speed in response to these same triggers.


Memory Channel Protection

A Cyclic Redundancy Check (CRC) mechanism is used to detect errors in the memory channels. When an error is detected, a series of progressively stronger corrective actions are triggered: 1) the transaction is retried, several times if necessary, which corrects most soft errors; 2) the memory channel is physically reset (reinitialized), which corrects most persistent errors; 3) if the problem persists, the affected lane on the memory channel is mapped out. This corrects hard errors without degrading performance.


Memory Migration and DIMM Sparing (DIMM spares)

An algorithm in system firmware continuously monitors memory errors. If it determines that a memory component is failing, a hardware engine can copy the contents of the failing component to another location. This process can be completely transparent to the OS. Two mechanisms are supported. With DIMM sparing, the contents of the failing DIMM are copied to a spare DIMM on the same memory channel. With memory migration, the contents are copied to the memory of any other memory controller on the system. DIMM sparing requires less memory overhead (just a single spare DIMM per memory channel). However, memory migration enables hot-swap capabilities for an entire memory card and can help to support hot-swap functionality for processors.


Memory Mirroring (essentially RAID1-memory)

The Intel Itanium processor 9300 series can be configured to automatically maintain a backup copy of main memory. If a failure is detected, the correct data can be accessed from the backup. Since the probability of simultaneous errors in parallel memory locations on two different memory DIMMs is extremely small, this provides exceptionally strong protection against memory errors. However, it does require that the system be configured with twice the memory capacity to support the backup. If memory capacity is an issue, memory mirroring can be configured only for selected memory controllers.


Memory Channel Hot Plug

Memory channels can be placed in an electrically idle state. This enables IT personnel to logically reallocate memory resources among running partitions. It also enables them to physically add or remove memory riser cards without bringing down the system. With these capabilities, memory upgrades, faulty memory card replacements, and resource management can all be performed without downtime.



Inbuilt Redundancy, Failover, and Self-Healing



Socket Redundancy and Failover:


	Dynamic OS

	Assisted Processor Socket Migration*




















The figure describes inbuilt redundancy, failover, and self-healing.


Intel Scalable Memory Interconnect (SMI) Failover

Intel (SMI) Failover can detect channel transaction errors, affect automatic hardware recovery, and retry the transactions afterward. Upon detection of an error, SMI maps the failed data lane to a spare lane. Even though a wire has failed, this remapping allows the system to keep running, without compromising fault detection, until it is eligible for repair. Intel SMI provides an additional lane for both southbound and northbound links. Persistent CRC errors on the channel trigger this mapping, and using the spare lane, enables failover without compromising CRC protections.


Intel SMI Clock Failover

This process directs forwarded clocks to the clock failover lane in the case of a forwarded clock failure. As with lane failover, this allows for uninterrupted operation until the system can be repaired (refer to the explanation of QPI clock failover below).


Intel Packet Retry

This process restarts a cycle when SMI detects a transient failure on the link. When the CPU detects a CRC error, SMI retries the request at the link level. Intel SMI supports retry on both northbound and southbound transient errors. Note that repeated CRC errors can activate viral mode.


DDDC+1 and SDDC+1

This feature is discussed on the previous slide.


Fine-Grained Memory Mirroring

This is a method for keeping a duplicate (secondary or mirrored) copy of the contents of select memory that serves as a backup if the primary memory fails. The Intel Xeon processor E7 Family supports more flexible memory mirroring configurations than previous generations, and allows the mirroring of only a portion of memory, leaving the rest of memory un-mirrored. The benefit to IT is more cost-effective mirroring that focuses on the critical portion of memory, instead of mirroring the entire memory space. Failover to the mirrored memory does not require a reboot, and is transparent to the OS and applications.


Memory Sparing

Memory sparing allows a failing DIMM or rank to dynamically failover to a spare DIMM or rank behind the same memory controller. When the firmware detects that a DIMM or rank has crossed a failure threshold, it initiates the copying of the failing memory to the spare. There is no OS involvement in this process. If the memory is in lockstep, the operation occurs at the channel pair level. DIMM and rank sparing is not compatible with mirroring or migration.


Memory Migration

This process moves the memory contents of a failing DIMM to a spare DIMM, and reconfigures the caches to use the updated location so that the system can coherently use the copied content. This is necessary when a memory node fails or the memory node ceases to be accessible. The act of migrating the memory does not affect the OS or the applications using the memory. Typically, this operation is transparent to the OS. In some cases, OS assistance improves performance, and OS-assisted memory migration is also available.


QPI Self-Healing

In this process, Intel QPI connects each processor to any other processors in the system and to the I/O Hub. QPI Self-Healing reduces the width of a QPI link in response to persistent errors. This dynamic width reduction allows the system to continue operation in a degraded mode until repairs can be made. When detecting persistent errors, a full-width port reduces to a half-width port. If necessary, a half-width port can reduce further to a quarter-width port. IT can set the error threshold at which QPI will enter self-healing mode.


QPI Clock Failover

This process directs forwarded clocks to one of the two dual-use lanes. Lanes 9 and 10 normally function as data lanes, but, in the event of a failure, one of these data lanes is used as the clock lane.


QPI Packet Retry

Automatically retransmits packets containing errors. This supports recovery from transient errors on QPI links. Persistent failures will enter half-width mode.



Identify Failing Components Before They Fail



	Most hardware errors are detected and corrected internally without any interruption in availability:

			e.g. parity checking and error correcting code






	Corrected Machine Check Interrupt (CMCI) signals the OS with information about corrected errors.

	The tools in the OS perform Predictive Failure Analysis to isolate failing components for replacement.




















Corrected errors in socket will be signaled using corrected machine checks (CMCI):


	Architected CMCI support

	Core and uncore (memory, memory controller and QPI interface of socket) corrected errors are signaled using the CMCI mechanism

	Enables software value add through predictive failure solutions



CMCI requires OS and (possibly) application support. It sends a report to the OS about hardware errors that the RAS features of the Intel Xeon processor E7 Family have corrected. Although these errors have no effect on the running system, reporting corrected errors to the BIOS or OS allows predictive failure analysis to anticipate and avoid future problems.



Scalable Memory Subsystem Error Detection and Correction



Demand and Patrol Scrubbing:


	Proactively searches for memory errors

	If an error is detected, data is written back corrected or contained if uncorrectable



Explicit Write-Back:


	Proactively checks for errors as data is written from last level cache

	If an error is detected, data is written back corrected or contained if uncorrectable



Single DRAM Device Data Correction Plus 1 Bit:


	Failover from single DRAM device error

	Single bit error correction continues after DRAM failover




















The figure describes error detection and correction.



Intel QuickPath Interconnect (QPI) Self-Healing



	Intel QPI Self-Healing maintains system availability in the event of persistent interconnect errors

	On detecting persistent errors the QPI port automatically reduces to half the current width and keeps operating at a reduced level

	The system administrator sets the threshold at which to go into self-healing mode




















On detecting persistent errors, a full-width port reduces to a half-width port. A half-width port would go to quarter-width. The link will only step down once from its original width.


The user sets the threshold at which the QPI port goes into self-healing mode.



Intel Scalable Memory Interconnect (SMI) Lane Failover



	Intel SMI allows the memory interconnect to automatically failover and recover from partial link failures maintaining availability and performance.

	Intel SMI provides an additional interconnect lane in each direction (memory write and read).

	If a single lane failure is detected, the failed lane is automatically mapped out by the CPU and the spare lane is enabled.




















Intel® SMI provides an additional lane for both southbound and northbound links. Intel® SMI lane failover is performed as follows:


	If a single lane fails, the failed lane is mapped out by the CPU during training, and is initiated by persistent CRC errors.

	The spare lane allows failover to occur without reducing CRC coverage.

	The southbound spare lane in MB must be enabled prior to training via SMB.

	If the southbound spare lane is not enabled, then CRC coverage is reduced from 22 CRC bits to 10 CRC bits on detecting a link failure.

	The northbound spare lane in MB is always enabled.



The spare lane allows failover to occur without reducing performance or CRC error detection coverage.



Machine Check Architecture Recovery



Previously seen only in RISC, Mainframe, and Itanium-based Systems







	Allows Recovery from Otherwise Fatal System Errors















Mainframe-Like Features

This figure describes MCA-recovery and shows an example of the silicon and OS working together to keep the system up and running.


Machine Check Architecture recovery is a mechanism where the silicon works with the operating system to allow a server to recover from uncorrectable memory errors which would have otherwise caused a system crash in prior generations. This capability has been available on RISC, Mainframe, and Itanium systems for some time, but this is the first time it has been implemented in a Xeon-based system.


In this first implementation, MCA-r allows the OS to recover when uncorrectable errors are discovered in memory during either an explicit write-back operation from cache, or by a patrol scrub which examines every server memory location daily. Uncorrectable errors are typically multi-bit errors that cannot be corrected by error correcting code (ECC). It should be noted that the occurrence of these errors is rare.


When an uncorrectable error is detected, the silicon interrupts the OS and passes it the address of the memory error. The OS then determines whether this memory location is vital to the continued operation of the system. If not, the OS marks the defective memory location so that it will not be used again, then it resets the error condition, and the system keeps running. In cases where the memory location is being used for a critical kernel operation or application, the system or application will not be able to continue and will be shut down by the OS as before.


		
While MCA-r is a big step forward in XEON RAS capabilities, this generation does not yet equal the full recovery capabilities of Itanium or some RISC systems. For most customers, the addition of this capability will be significant, even though it is a first step. This presentation does not go into the detailed differences in implementations, but the presenter should acknowledge that there are differences if asked and offer a follow up discussion to go into more detail.





Machine Check Architecture Recovery




















The Machine Check Architecture (MCA) recovery allows higher-level software, such as the hypervisor, OS, or MCA recovery-aware applications, to recover from some data errors that cannot be corrected at the hardware level. Memory Patrol Scrub or Last Level Cache Write Back detects these errors. MCA recovery reports the location of the error to the software stack, which then takes the appropriate action. For example, the OS might abort the task owner in response to an error, to allow the system to continue running.



E7 Fine-Grained Memory Mirroring



	Provides protection against uncorrectable memory errors that would otherwise result in a platform failure

	Xeon E7 provides more flexible memory mirroring configurations

	Memory mirroring of just a portion of memory; leaves the rest of memory un-mirrored




















A pair of mirrored DIMMs forms a redundant group. In a mirror configuration, one pair of memory DIMMs is designated as the primary image and the other is designated as the secondary image. For memory writes, the write request is issued to both sets of DIMMs. For memory reads, the read request is issued to the primary DIMM. In the case of a detected Correctable Error, the primary image will toggle and the read will be issued to the “new” primary image. In the case of a detected Uncorrectable error, the definition of the primary image will “hard fail” to the other image. In this case, the “failed” image will never become the primary image until the failed DIMMs have been replaced and the image re-built.


Memory Mirroring reduces the available memory by one half. Memory Mirroring and Memory RAID cannot be configured at the same time.



Memory Sparing



	Data from a failing DIMM pair or rank pair is copied to a spare DIMM or rank pair

	The failed component is mapped out

	The OS is not involved

	Cannot be configured concurrently with memory mirroring




















The Intel Xeon processor 7500 supports DIMM sparing and rank sparing. Sparing is confined to each memory controller (Mbox); that means that no sparing can be performed between the two memory controllers on a socket. In case of DIMM sparing, the spare DIMM must be a super set of all the other DIMMs (with regard to number of ranks, and capacity).


In spare rank mode, the capacity of the spare rank must be greater than that of any other rank on the channel.


In DIMM sparing mode, contents of the entire failing DIMM are transferred to the spare DIMM.


In rank sparing mode, contents of the rank defined by the {Failing DIMM, Failing Rank} tuple are transferred to the spare rank defined by the {Spare DIMM, Spare rank} tuple.


		
The source of this information is the Intel Xeon Processor 7500 Series Datasheet, Volume 2, March 2010, published by Intel.





Intel Performance Optimizations


This topic explores the characteristics of the Xeon Processor that enhance performance.



Intel® Xeon® 2S Platform Comparison




















The figure shows the increased performance features and functions from the Nehalem Xeon series to the current Sandy Bridge processor.



Previous Generation I/O Intel® Integrated I/O




















Intel has not only improved the processor, but also implemented innovations throughout the system, starting with the biggest lead in I/O:


	Click 1: The figure shows (in concept) how data moves through the system. Historically, you’ll see that data started from a network adapter, moved up to a separate component on the mother board (called the I/O hub), and is then sent on to the processor. From there, the data heads to main system memory, then back to the cache, and then it works its way back to the network adapter. The revolution in the data center in deploying virtualization has spurred the broad adoption of 10 Gb Ethernet.

	Click 2: This removed one bottleneck from the system and enabled higher network bandwidth. Intel continues to be the leading provider of 10Gb Ethernet solutions, including the current launching of the Intel® X540 Controller–the industries 1st 10GBASE-T controller–for low-cost, low-power LAN on Motherboard solutions, so we know as well as anyone that the entire system has to be in balance. As the network adapters were upgraded, this increased pressure on other areas of the system. That’s why we are introducing Intel Integrated I/O with the Xeon processor E5 Family.

	Click 3: Intel integrated I/O is a suite of features that offer dramatic improvements in the I/O system by removing the IO hub and moving the I/O controller directly onto the processor die. This reduces latency on data traffic by up to 30% to get data where it needs to be, faster than ever before. Along with the integrated I/O, critical storage features were added, such as non-transparent bridging, hardware RAID support, and asynchronous DRAM refresh to the processor, to improve performance across the data center.

	Click 4: Intel was the first processor vendor to support PCI Express 3.0–the PCI-SIG’s latest specification. This updated specification doubles bandwidth per port by speeding up the lanes and by making architectural improvements that significantly reduce the ~20% overhead on PCI Express 2.0 traffic. In addition to integrating the I/O interface and turbocharging it with PCI Express 3.0, Intel has also included a new technology–called Intel Data Direct I/O–which lets Intel Ethernet controllers talk directly with the processor cache.

	Click 5: To start, look at the flow of data through the previous generation I/O system, from a time when I/O was slow and cache was a small and scarce resource. First, data flows up to the IO hub, through the processor, into the main system memory, then the processors cache and cores for processing, and then back to system memory, before repeating the trek back to the network adapter. These trips through memory add more time for data to get where it needs to be, but also means that you need to keep your main system memory active.

	Click 6: Intel Data Direct I/O re-architected this data flow so that the processor cache is now the primary destination for network traffic. This means shorter trips between Intel Ethernet adapters and the processor core, which can more than double the I/O capabilities of the Xeon E5 Family, depending on usage. This approach also helps keep system power costs down, by eliminating unnecessary accesses to memory.

	Click 7: All of these capabilities combine to create Intel Integrated I/O, the breakthrough I/O innovation that gets you to your data faster and helps you scale to meet the growing data demands of your users.




Virtual Machine Device Queues (VMDq)




















Improved SW-Switching Performance

VMDq is a networking hardware feature on Intel Server Adapters that provides acceleration by assigning packets to various virtual machines (VMs) in a virtualized server. Received packets are sorted into queues for the appropriate VM and are then handed up to the virtual machine monitor (VMM) switch, thereby reducing the number of memory copies the system has to make to get packets to the VMs. Normally, a single processor core of the hypervisor must be interrupted to receive the packet and pass the packet to the VMM responsible for the VM receiving the packet. This provides for only a single core to process all incoming packets. VMDq provides the packets to be delivered directly to the queue of the VM so that only the processor of the VMM of that VM needs to be interrupted to deliver the packet, and the process provides parallel delivery of packets, versus serial delivery, and thus offers an increased scalability to the VM switching environment.


VMDq also handles transmission of packets from the various VMs on the host server to ensure timely and fair delivery to the network. This reduces the significant I/O penalty created by overhead associated with the added layer of VMM software for sharing ports and switching data between multiple VMs. In brief, VMDq provides acceleration with multiple queues that sort and group packets for greater server and VM I/O efficiency.



Single Root I/O Virtualization and Sharing




















SR-IOV

The Direct Assignment method of virtualization provides very fast I/O. However, it prevents the sharing of the I/O device. The SR-IOV specification provides a mechanism by which a Single Root Function (for example, a single Ethernet Port) can appear as multiple separate physical devices.


An SR-IOV capable device can be configured (usually by the Hypervisor) to appear in the PCI Configuration space as multiple functions–each with its own configuration space, complete with Base Address Registers (BARs).


The SR-IOV capable device provides a configurable number of independent Virtual Functions, each with its own PCI Configuration space. The Hypervisor assigns one or more Virtual Functions to a virtual machine. Memory Translation technologies such as those in Intel VT-d provide hardware assisted techniques to allow direct DMA transfers.


SR-IOV paired with VMDq allows for a packet to be delivered via direct memory access (DMA’d) to the VM itself for processing so that a processor core does not have to be impaired with delivery of packets to the VMs, thus relieving the hypervisor virtual switch from packet delivery.


The SR-IOV specification details how the PCI Configuration information is to appear. This information is different from that on standard PCI devices. The Hypervisor must know how to access and read this information in order to provide access to them from a VM. Refer to the PCI SIG SR-IOV specification for details.



Virtualization Technology for Directed I/O



	With support from the processor, chipset, BIOS, and enabling software, Intel VT improves traditional software-based virtualization.

	Taking advantage of offloading workloads to system hardware, these integrated features enable virtualization software to provide more streamlined software stacks and "near native" performance characteristics.




















The virtualization of I/O resources is an important step toward enabling a significant set of emerging usage models in the data center, the enterprise, and the home. VT-d support on Intel platforms improves isolation of I/O resources for greater reliability, security, and availability.


Specifically, VT-d supports the remapping of I/O DMA transfers and device-generated interrupts. The architecture of VT-d provides the flexibility to support multiple usage models that may run un-modified, special-purpose, or “virtualization aware” guest OSs. The VT-d hardware capabilities for I/O virtualization complement the existing Intel® VT capability to virtualize processor and memory resources. Together, this roadmap of VT technologies offers a complete solution to provide full hardware support for the virtualization of Intel platforms.


Ongoing and future developments within the virtualization hardware and software communities will build upon VT-d to ensure that the requirements for sharing, security, performance, and scalability are being met. I/O devices will become more aware of the existence of VT-d to ensure efficient caching and consistency mechanisms to enhance their performance. Given the protection provided by VT-d, future I/O devices will emerge that are sharable among multiple guest OSs. With VT-d, software developers can develop and evolve their architectures to provide fully protected sharing of I/O resources that are highly available, provide high performance, and scale to increasing I/O demands.


		
The source of this information is the Intel® Technology Journal Volume 10, Issue 03, Published August 10, 2006.





Intel Security Features


This topic explores the characteristics of the Xeon Processor that protect servers and applications.



Intel® Technologies: Server Security
















Isolate


Intel® VT & Intel® TXT protects VM isolation and provides a more secure platform launch















Enforce


Intel® TXT establishes “trusted” status, foundation to control migration based on security policy















Encrypt


Intel® AES-NI delivers built-in encryption acceleration for better data protection






The foundation of enhanced cloud security is built around the ability to deliver secure multi-tenancy, better data protection, while ensuring compliance and controlled access. The terms Isolate, Enforce, and Encrypt provide the headings for this security discussion. Let’s look at each in turn:


	Isolate: A key premise for virtualized environments is that each virtual machine believes it is a real machine, and has control over its physical and logical resources―and is “protected” from other systems/VMs. But the reality is that VMs are existing on shared resources, and that software layers are responsible for arbitrating access to these shared resources, and for protecting the contents of one VM from another. With emerging attacks such as virtual rootkits and other VM escape methods, and more scrutiny driven by the desire to move more critical workloads to the cloud, software-only approaches fall short. Intel VT and Intel TXT provide additional hardening to isolate workloads and system execution from launch through runtime, and for helping to reduce overall attack surfaces of these shared environments.

	Enforce: The computing environment is difficult in a cloud implementation–because the resources are abstracted away and may exist anywhere. But when considering deployment of critical workloads to the cloud–or even to virtualized internal resources, one often MUST have more knowledge of the environment–due to policy or regulatory conditions. Intel TXT helps provide assurances of platform integrity through the enforcement of platform trust–so that a “known good” software environment is in control of the platform. With this knowledge, IT managers can establish and enforce policies so that critical workloads or sensitive data can only be deployed onto trusted platforms as a way to provide best protection of these apps.

	Encrypt: Whenever physical control of data is reduced (such as putting data on laptops or storing in cloud), encryption is essential as the last line of defense to protect from misuse. For the cloud and other shared infrastructures, one wants to have encryption in place to protect data as it is moved to the cloud, or between clouds and while in storage. Intel AES-NI provides performance benefits to make high-volume encryption faster and more efficient for these data transport and storage workloads. AES-NI also provides strengthening again side-channel attacks, which is an increasingly critical function in shared compute resources models.



Combined together, these features help to provide a more robust foundation to better address needs for security in data center and cloud deployments.



AES-NI



	Implements in the hardware some sub-steps of the AES algorithm

	This speeds up execution of the AES encryption/decryption algorithms and removes the performance penalty




















Encryption is frequently recommended as the best way to secure business-critical data, and AES is the most widely used standard when protecting network traffic, personal data, and corporate IT infrastructures.


With recent advancements in cloud computing, where personal or business-critical information leaves the traditional IT environment, a more widely usable and secure encryption standard such as AES and acceleration mechanisms like Intel® AES-NI are essential.


AES is a widely-deployed encryption standard when protecting network traffic, personal data, and corporate IT infrastructures; and Intel® AES-NI can be used to accelerate the AES encryption. With such robust, affordable, and flexible options, Intel® AES-NI can help your business stay ahead of growing threats.


The Intel® Advanced Encryption Standard New Instructions (Intel® AES-NI) set is a set of seven new instructions in the Intel® Xeon® processor 5600 series (formerly codenamed Westmere-EP). Four instructions accelerate encryption and decryption. Two instructions improve key generation and matrix manipulation. The seventh aids in carry-less multiplication. By implementing some complex and costly sub-steps of the AES algorithm in hardware, Intel AES-NI accelerates execution of the AES-based encryption.


The result is faster, more secure encryption, which makes the use of encryption more feasible than previously.


		
The source of this information is Secure the Enterprise with Intel® AES-NI: White Paper at www.intel.com.





Trusted Execution Technology (TXT)




















Intel® TXT provides an infrastructure rooted in the processor that enables an accurate comparison of all the critical elements of the launch environment against a known good source.


To do this, it first allows creation of the known good profile by establishing a cryptographically unique identifier for each approved launch-enabled component. It then provides hardware-based enforcement mechanisms to detect the launch of any code that does not match the approved code.


Intel® TXT’s hardware-based approach provides the foundation on which a trusted platform solution can be built to better protect against software-based attacks.



Cisco UCS BIOS Best Practice Settings for Desktop Virtualization


This topic discusses the Cisco UCS BIOS Best Practice settings for desktop virtualization.



Cisco UCS Intel Directed I/O BIOS Settings
















	Settings for Intel Directed I/O  BIOS from Cisco CVD VXI 2.6:

			See student notes below for complete discussion on setting details












The VT For Directed IO setting determines the processor’s use of Intel Virtualization Technology for Directed I/O (VT-d). This can be one of the following:


	disabled: The processor does not use virtualization technology.

	enabled: The processor uses virtualization technology.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



		
This option must be enabled if you want to change any of the other Intel Directed I/O BIOS settings.




The Interrupt Remap setting determines whether the processor supports Intel VT-d Interrupt Remapping. This can be one of the following:

	disabled: The processor does not support remapping.

	enabled: The processor uses VT-d Interrupt Remapping as required.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Coherency Support setting determines whether the processor supports Intel VT-d Coherency. This can be one of the following:


	disabled: The processor does not support coherency.

	enabled: The processor uses VT-d Coherency as required.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The ATS Support setting determines whether the processor supports Intel VT-d Address Translation Services (ATS). This can be one of the following:


	disabled: The processor does not support ATS.

	enabled: The processor uses VT-d ATS as required.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Pass Through DMA Support setting determines whether the processor supports Intel VT-d Pass-through DMA. This can be one of the following:


	disabled: The processor does not support pass-through DMA.

	enabled: The processor uses VT-d Pass-through DMA as required.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.




Cisco UCS Intel Processor BIOS Settings
















	Settings for Intel Processor BIOS Cisco CVD VXI 2.6:

			See student notes below for complete discussion on setting details













The Turbo Boost setting determines whether the processor uses Intel Turbo Boost Technology, which allows the processor to automatically increase its frequency if it is running below power, temperature, or voltage specifications. This can be one of the following:


	disabled: The processor does not increase its frequency automatically.

	enabled: The processor utilizes Turbo Boost Technology if required.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Enhanced Intel Speedstep setting determines whether the processor uses Enhanced Intel SpeedStep Technology, which allows the system to dynamically adjust processor voltage and core frequency. This technology can result in decreased average power consumption and decreased average heat production. This can be one of the following:


	disabled: The processor never dynamically adjusts its voltage or frequency.

	enabled: The processor utilizes Enhanced Intel SpeedStep Technology and enables all supported processor sleep states to further conserve power.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



We recommend that you contact your operating system vendor to make sure the operating system supports this feature.


The Hyper Threading setting determines whether the processor uses Enhanced Intel SpeedStep Technology, which allows the system to dynamically adjust processor voltage and core frequency. This technology can result in decreased average power consumption and decreased average heat production. This can be one of the following:


	disabled: The processor never dynamically adjusts its voltage or frequency.

	enabled: The processor utilizes Enhanced Intel SpeedStep Technology and enables all supported processor sleep states to further conserve power.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



We recommend that you contact your operating system vendor to make sure the operating system supports this feature.


The Execute Disable Bit setting classifies memory areas on the server to specify where application code can execute. As a result of this classification, the processor disables code execution if a malicious worm attempts to insert code in the buffer. This setting helps to prevent damage, worm propagation, and certain classes of malicious buffer overflow attacks. This can be one of the following:


	disabled: The processor does not classify memory areas.

	enabled: The processor classifies memory areas.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



We recommend that you contact your operating system vendor to make sure the operating system supports this feature.


The Virtualization Technology setting determines whether the processor uses Intel Virtualization Technology, which allows a platform to run multiple operating systems and applications in independent partitions. This can be one of the following:


	disabled: The processor does not permit virtualization.

	enabled: The processor allows multiple operating systems in independent partitions.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



		
If you change this option, you must power cycle the server before the setting takes effect.




The Direct Cache Access setting allows processors to increase I/O performance by placing data from I/O devices directly into the processor cache. This setting helps to reduce cache misses. This can be one of the following:


	disabled: Data from I/O devices is not placed directly into the processor cache.

	enabled: Data from I/O devices is placed directly into the processor cache.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Processor C State setting determines whether the system can enter a power savings mode during idle periods. This can be one of the following:


	disabled: The system remains in high performance state even when idle.

	enabled: The system can reduce power to system components such as the DIMMs and CPUs.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



We recommend that you contact your operating system vendor to make sure the operating system supports this feature.


The Processor C1E setting allows the processor to transition to its minimum frequency upon entering C1. This setting does not take effect until after you have rebooted the server. This can be one of the following:


	disabled: The CPU continues to run at its maximum frequency in C1 state.

	enabled: The CPU transitions to its minimum frequency. This option saves the maximum amount of power in C1 state.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Processor C3 Report setting determines whether the processor sends the C3 report to the operating system. This can be one of the following:


	disabled: The processor does not send the C3 report.

	acpi-c2: The processor sends the C3 report using the ACPI C2 format.

	acpi-c3: The processor sends the C3 report using the ACPI C3 format.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



On the B440 server, the BIOS Setup menu uses enabled and disabled for these options. If you specify acpi-c2 or acpi-c2, the server sets the BIOS value for that option to enabled.


The Processor C6 Report setting determines whether the processor sends the C6 report to the operating system. This can be one of the following:


	disabled: The processor does not send the C6 report.

	enabled: The processor sends the C6 report.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Processor C7 Report setting determines whether the processor sends the C7 report to the operating system. This can be one of the following:


	disabled: The processor does not send the C7 report.

	enabled: The processor sends the C7 report.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The CPU Performance Field setting sets the CPU performance profile for the server. This can be one of the following:


	enterprise: All prefetchers and data reuse are disabled.

	high-throughput: All prefetchers are enabled, and data reuse is disabled.

	hpc: All prefetchers and data reuse are enabled. This setting is also known as high performance computing.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Max Variable MTRR Settings setting allows you to select the number of MTRR variables. This can be one of the following:


	auto-max: The BIOS uses the default value for the processor.

	8: The BIOS uses the number specified for the variable MTRR.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.




Cisco UCS Intel RAS Memory BIOS Settings
















	Settings for Intel RAS Memory BIOS options:

			See student notes below for complete discussion on setting details













The Memory RAS Config setting determines how memory reliability, availability, and serviceability (RAS) is configured for the server. This can be one of the following:


	maximum performance: System performance is optimized.

	mirroring: System reliability is optimized by using half the system memory as backup.

	lockstep: If the DIMM pairs in the server have an identical type, size, and organization and are populated across the SMI channels, you can enable lockstep mode to minimize memory access latency and provide better performance. Lockstep is enabled by default for B440 servers.

	sparing: System reliability is enhanced with a degree of memory redundancy while making more memory available to the operating system than mirroring.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The NUMA setting determines whether the BIOS supports NUMA. This can be one of the following:


	disabled: The BIOS does not support NUMA.

	enabled: The BIOS includes the ACPI tables that are required for NUMA-aware operating systems. If you enable this option, the system must disable Inter-Socket Memory interleaving on some platforms.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The Mirroring Mode setting determines whether memory mirroring enhances system reliability by keeping two identical data images in memory.


This option is only available if you choose the mirroring option for Memory RAS Config. It can be one of the following:


	inter-socket: Memory is mirrored between two Integrated Memory Controllers (IMCs) across CPU sockets.

	intra-socket: One IMC is mirrored with another IMC in the same socket.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



Sparing optimizes reliability by holding memory in reserve so that it can be used in case other DIMMs fail. This option provides some memory redundancy, but does not provide as much redundancy as mirroring. The available sparing modes depend on the current memory population.


This option is only available if you choose sparing option for Memory RAS Config. It can be one of the following:


	dimm-sparing: One DIMM is held in reserve. If a DIMM fails, the contents of a failing DIMM are transferred to the spare DIMM.

	rank-sparing: A spare rank of DIMMs is held in reserve. If a rank of DIMMs fails, the contents of the failing rank are transferred to the spare rank.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.



The LV DDR setting determines whether the system prioritizes low voltage or high frequency memory operations. This can be one of the following:


	power-saving-mode: The system prioritizes low voltage memory operations over high frequency memory operations. This mode may lower memory frequency in order to keep the voltage low.

	performance-mode: The system prioritizes high frequency operations over low voltage operations.

	Platform Default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.




Summary


This topic summarizes the key points that were discussed in this lesson.



Summary



	During this lesson we have:

			Reviewed the Intel Xeon Processor Family

	Examined Intel features for Performance Optimization, Security, and System Reliability

	Provided detailed information on setting Cisco UCS BIOS settings for Intel advanced processor features












